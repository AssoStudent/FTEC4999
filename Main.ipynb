{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Statistical Learning Part 2 - Experiment Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 Environment Setup ##\n",
    "\n",
    "In thise section, we are importing the libraries, datasets and associative python programme. In addition, setup the experiment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Packages and Libraries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyter\n",
    "#!pip install numpy\n",
    "#!pip install torch torchvision \n",
    "#!pip install matplotlib\n",
    "#!pip install pandas\n",
    "#!pip install tensorflow\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install torch_xla\n",
    "#!pip install torch-neuron --extra-index-url=https://pip.repos.neuron.amazonaws.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Python Programme**\n",
    "\n",
    "Executing this, importing the global functions and classes directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Set up Experiment Environment ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilize GPU**\n",
    "\n",
    "For local environments, please utilize the GPU to speed up the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utililze TPU**\n",
    "\n",
    "To shortern the training time, we highly recommend that experiments should be done in Google Colab. Enable the following code in the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume that you are on the Google Colab platform.\n",
    "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "def to_device(data, device):\n",
    "    data.to(device)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.2 Datasets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.1 Importing Datasets from Packages ####\n",
    "This will load the dataset automatically downloaded from the package. Remember the datasets will be stored in the folder \"Datasets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Download Dataset\n",
    "MNIST_train_dataset = MNIST(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_test_dataset = MNIST(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.2 Importing Datasets from Downloaded Files ####\n",
    "This will load the dataset downloaded in the local directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.3 Optimizer and Loss functions ###\n",
    "Here, we define some customized optimizer classes and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Classes, Functions and Algorithms ##\n",
    "We define all necessary functions for learning here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#     Helping Functions    #\n",
    "############################\n",
    "def convert_to_list(input_list):\n",
    "    if not isinstance(input_list, list):\n",
    "        input_list = [input_list]\n",
    "    return input_list\n",
    "\n",
    "# Graph Plotting Functions\n",
    "def plot_cost_history(cost_history_list=[], save=True):\n",
    "    cost_history_list = convert_to_list(cost_history_list)\n",
    "    for i, cost_history in enumerate(cost_history_list):\n",
    "        plt.plot(cost_history, label=f\"Cost History {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Culminative Send Cost History\")\n",
    "    if len(cost_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_culminative_send_cost_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_history(time_history_list=[], save=True):\n",
    "    time_history_list = convert_to_list(time_history_list)\n",
    "    for i, time_history in enumerate(time_history_list):\n",
    "        plt.plot(time_history, label=f\"Time History {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Culminative Time Used\")\n",
    "    plt.title(\"Time History\")\n",
    "    if len(time_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_time_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(train_loss_history_list=[], test_loss_history_list=[], save=True):\n",
    "    train_loss_history_list = convert_to_list(train_loss_history_list)\n",
    "    test_loss_history_list = convert_to_list(test_loss_history_list)\n",
    "    for i, train_loss_history in enumerate(train_loss_history_list):\n",
    "        plt.plot(train_loss_history, label=f\"Train Loss History {i+1}\")\n",
    "    for i, test_loss_history in enumerate(test_loss_history_list):\n",
    "        plt.plot(test_loss_history, label=f\"Test Loss History {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss History\")\n",
    "    if len(train_loss_history_list) > 1 or len(test_loss_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_loss_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history(train_accuracy_history_list=[], test_accuracy_history_list=[], save=True):\n",
    "    train_accuracy_history_list = convert_to_list(train_accuracy_history_list)\n",
    "    test_accuracy_history_list = convert_to_list(test_accuracy_history_list)\n",
    "    for i, train_accuracy_history in enumerate(train_accuracy_history_list):\n",
    "        plt.plot(train_accuracy_history, label=f\"Train Accuracy History {i+1}\")\n",
    "    for i, test_accuracy_history in enumerate(test_accuracy_history_list):\n",
    "        plt.plot(test_accuracy_history, label=f\"Test Accuracy History {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy History\")\n",
    "    if len(train_accuracy_history_list) > 1 or len(test_accuracy_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_accuracy_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history(train_error_history_list=[], test_error_history_list=[], save=True):\n",
    "    train_error_history_list = convert_to_list(train_error_history_list)\n",
    "    test_error_history_list = convert_to_list(test_error_history_list)\n",
    "    for i, train_error_history in enumerate(train_error_history_list):\n",
    "        plt.plot(train_error_history, label=f\"Train Error History {i+1}\")\n",
    "    for i, test_error_history in enumerate(test_error_history_list):\n",
    "        plt.plot(test_error_history, label=f\"Test Error History {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Error History\")\n",
    "    if len(train_error_history_list) > 1 or len(test_error_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_error_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Error Rate Calculation\n",
    "def get_accuracy(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "def get_error(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        return torch.tensor(torch.sum(predictions != labels).item() / len(predictions))\n",
    "\n",
    "############################\n",
    "#   Neural Network Model   #\n",
    "############################\n",
    "class MNIST_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 7 * 7, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "def iterate_CNN_model(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            images, labels = batch\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "def evaluate_CNN_model(model, dataloader, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, labels = batch\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "    return loss_average, accuracy_average, error_average\n",
    "\n",
    "############################\n",
    "#      Client Devices      #\n",
    "############################\n",
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, client_id, model, optimizer, dataset, batch_size, iterate_func):\n",
    "        self.id = client_id\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DeviceDataLoader(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        self.iterate_func = iterate_func\n",
    "\n",
    "    def load_weights(self, weights):\n",
    "        self.model.load_state_dict(weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.id\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def save_local_history(self, num_epochs, loss_history, accuracy_history, error_history, time_history, value=train_start_time):\n",
    "        filename = \"{}_client_{}_with_local_epochs_{}_local_loss_accuracy_error_history_{}\".format(current_dataset_name, self.id, num_epochs, value)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, loss_history=loss_history, accuracy_history=accuracy_history, error_history=error_history, time_history=time_history)\n",
    "\n",
    "    def train(self, num_epochs, show_history=False):\n",
    "        if show_history:\n",
    "            print(f\"!-- Client {self.id} start iterations. ---!\")\n",
    "        loss_history, accuracy_history, error_history, time_history = self.iterate_func(self.model, self.dataloader, num_epochs, self.optimizer)\n",
    "        if show_history:\n",
    "            plot_time_history(time_history)\n",
    "            plot_loss_history(loss_history)\n",
    "            plot_accuracy_history(accuracy_history)\n",
    "            plot_error_history(error_history)\n",
    "            print(f\"!-- Client {self.id} finish iterations. ---!\")\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Distribute the training datasets to clients, remember it returns an array of datasets\n",
    "def split_datasets_for_clients_random(dataset, num_clients=1):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_clients = total_sample_size // num_clients\n",
    "    client_datasets = random_split(dataset, [min(i + samples_per_clients, total_sample_size) - i for i in range(0, total_sample_size, samples_per_clients)])\n",
    "    return client_datasets\n",
    "\n",
    "# Be careful where the label placed and remember to modify the sample[1] if necessary\n",
    "def split_datasets_for_clients_label(dataset, num_clients=1, num_classes=10):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_clients = total_sample_size // num_clients\n",
    "    labels_per_clients = num_classes // num_clients\n",
    "    filtered_datasets = []\n",
    "    for label_start in range(1, num_classes, labels_per_clients):\n",
    "        label_end = min(label_start + labels_per_clients, num_classes)\n",
    "        filtered_dataset = [sample for sample in dataset if sample[1] >= label_start and sample[1] < label_end]\n",
    "        filtered_datasets.append(filtered_dataset)\n",
    "    client_datasets = []\n",
    "    for filtered_dataset in filtered_datasets:\n",
    "        client_dataset = random_split(filtered_dataset, [samples_per_clients] * num_clients)\n",
    "        client_datasets.append(client_dataset)\n",
    "    return client_datasets\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, model_list, optimizer_list, dataset_list, batch_size_list, iterate_func_list):\n",
    "    client_device = [None] * num_clients\n",
    "    for client_id in range(num_clients):\n",
    "        client_device[client_id] = ClientDevice(client_id, model_list[client_id], optimizer_list[client_id], dataset_list[client_id], batch_size_list[client_id], iterate_func_list[client_id])\n",
    "    return client_device\n",
    "\n",
    "############################\n",
    "#    Training Algorithm    #\n",
    "############################\n",
    "def train_CNN_model(model, train_dataloader, test_dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, show_history=True, save_result=True):\n",
    "    train_loss_history, train_accuracy_history, train_error_history, time_history = iterate_CNN_model(model, train_dataloader, num_epochs, optimizer, loss_func)\n",
    "\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "    if test_dataloader is not None:\n",
    "        for epoch in range(num_epochs):\n",
    "            test_loss, test_accuracy, test_error = evaluate_CNN_model(model, test_dataloader)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.apeend(test_error)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_train_CNN_model_with_num_epochs_{}_{}\".format(current_dataset_name, num_epochs, train_start_time)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def federated_averaging(client_weights_total):\n",
    "    total_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / total_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / total_clients\n",
    "\n",
    "    return aggregate_weights\n",
    "\n",
    "def iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, global_epochs, local_epochs, aggregate_func=federated_averaging,show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        client_weights_total = []\n",
    "        for client in client_list:\n",
    "            client.load_weights(global_weights)\n",
    "            client_weights = client.train(num_epochs=local_epochs)\n",
    "            client_weights_total.append(client_weights)\n",
    "            send_cost += sum(value.numel() for value in client_weights.values())\n",
    "        global_weights.update(aggregate_func(client_weights_total))\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_CNN_model(global_model, train_dataloader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_CNN_model(global_model, test_dataloader)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_federated_learning_model(global_model, train_dataloader, test_dataloader, global_epochs, local_epochs, client_list, aggregate_func=federated_averaging, show_history=True, save_result=True):\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, global_epochs, local_epochs, aggregate_func)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_federated_learning_with_global_epochs_{}_local_epochs_{}_num_clients_{}_{}\".format(current_dataset_name, global_epochs, local_epochs, len(client_list), train_start_time)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def expirement_federated_learning_model(train_dataset, test_dataset, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    global_epochs_list_size = len(global_epochs_list)\n",
    "    local_epochs_list_size = len(local_epochs_list)\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    aggregate_func_list_size = len(batch_size_list)\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = local_epochs_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = num_clients_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = aggregate_func_list\n",
    "    else:\n",
    "        iteration_list = global_epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for local_epochs is {local_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for aggregate function is {aggregate_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for global_epochs is {global_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[n]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "        \n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        client_model_list = [to_device(modelClass(), device)] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = split_datasets_for_clients_random(train_dataset, num_clients)\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list)\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, global_epochs, local_epochs, client_list, aggregate_func)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Experiment ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Choose Dataset ###\n",
    "\n",
    "In this section, execute a cell only to choose a dataset you want do experiment with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.05\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) * (1 - split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 784\n",
    "model_type_preset = MNIST_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Federated Learning Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare global epochs\n",
    "\n",
    "2: compare local epochs\n",
    "\n",
    "3: compare number of clients\n",
    "\n",
    "4: compare learning rate\n",
    "\n",
    "5: compare batch size\n",
    "\n",
    "Others: compare global epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_CNN_model\n",
    "\n",
    "global_epochs_list = 5\n",
    "local_epochs_list = [1, 2, 3]\n",
    "num_clients_list = 2\n",
    "learning_rate_list = 0.5\n",
    "batch_size_list = batch_size_preset\n",
    "aggregate_func_list = federated_averaging\n",
    "\n",
    "compare_id = 2\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "\n",
    "expirement_federated_learning_model(train_dataset, test_dataset, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.1 Loading Data ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear and Initialize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cost_history_total = []\n",
    "data_time_history_total = []\n",
    "data_train_loss_history_total = []\n",
    "data_train_accuracy_history_total = []\n",
    "data_train_error_history_total = []\n",
    "data_test_loss_history_total = []\n",
    "data_test_accuracy_history_total = []\n",
    "data_test_error_history_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "load_dataset_name = \"MNIST\"\n",
    "load_global_epochs = 2\n",
    "load_local_epochs = 1\n",
    "load_num_clients = 2\n",
    "load_train_start_time = \"2023-12-04 19.48.18\"\n",
    "\n",
    "# Load the file\n",
    "filename_load = \"{}_federated_learning_with_global_epochs_{}_local_epochs_{}_num_clients_{}_{}\".format(load_dataset_name, load_global_epochs, load_local_epochs, load_num_clients, load_train_start_time)\n",
    "load_result = np.load(filename_load)\n",
    "print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "# Load the attributes from the file\n",
    "data_cost_history = load_result['cost_history']\n",
    "data_time_history = load_result['time_history']\n",
    "data_train_loss_history = load_result['train_loss_history']\n",
    "data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "data_train_error_history = load_result['train_error_history']\n",
    "data_test_loss_history = load_result['test_loss_history']\n",
    "data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "print(\"=======Content of the File=======\")\n",
    "print(load_result.files)\n",
    "\n",
    "print(\"=======VISUALIZATION RESULT=======\")\n",
    "plot_time_history([data_time_history])\n",
    "plot_loss_history([data_train_loss_history], [data_test_loss_history])\n",
    "plot_accuracy_history([data_train_accuracy_history], [data_test_accuracy_history])\n",
    "plot_error_history([data_train_error_history], [data_test_error_history])\n",
    "\n",
    "print(\"=======STATUS RESULT=======\")\n",
    "print(\"Cost History: \", data_cost_history)\n",
    "print(\"Time History: \", data_time_history)\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"Train Loss History: \", data_train_loss_history)\n",
    "print(\"Train Accuracy History: \", data_train_accuracy_history)\n",
    "print(\"Train Error History: \", data_train_error_history)\n",
    "\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"Test Loss History: \", data_test_loss_history)\n",
    "print(\"Test Accuracy History: \", data_test_accuracy_history)\n",
    "print(\"Test Error History: \", data_test_error_history)\n",
    "\n",
    "# Append the data\n",
    "data_cost_history_total.append(data_cost_history)\n",
    "data_time_history_total.append(data_time_history)\n",
    "data_train_loss_history_total.append(data_train_loss_history)\n",
    "data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "data_train_error_history_total.append(data_train_error_history)\n",
    "data_test_loss_history_total.append(data_test_loss_history)\n",
    "data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Using Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "# load_loop_max correspond to number of files you want to load\n",
    "load_dataset_name_list = [\"MNIST\"]\n",
    "load_global_epochs_list = [2]\n",
    "load_local_epochs_list = [1]\n",
    "load_num_clients_list = [2]\n",
    "load_train_start_time_list = [\"2023-12-04 19.48.18\"]\n",
    "load_loop_max = 1\n",
    "\n",
    "for n in range(load_loop_max):\n",
    "    # Load the file\n",
    "    filename_load = \"{}_federated_learning_with_global_epochs_{}_local_epochs_{}_num_clients_{}_{}\".format(load_dataset_name_list[n], load_global_epochs_list[n], load_local_epochs_list[n], load_num_clients_list[n], load_train_start_time_list[n])\n",
    "    load_result = np.load(filename_load)\n",
    "    print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "    # Load the attributes from the file\n",
    "    data_cost_history = load_result['cost_history']\n",
    "    data_time_history = load_result['time_history']\n",
    "    data_train_loss_history = load_result['train_loss_history']\n",
    "    data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "    data_train_error_history = load_result['train_error_history']\n",
    "    data_test_loss_history = load_result['test_loss_history']\n",
    "    data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "    data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "    # Append the data\n",
    "    data_cost_history_total.append(data_cost_history)\n",
    "    data_time_history_total.append(data_time_history)\n",
    "    data_train_loss_history_total.append(data_train_loss_history)\n",
    "    data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "    data_train_error_history_total.append(data_train_error_history)\n",
    "    data_test_loss_history_total.append(data_test_loss_history)\n",
    "    data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "    data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize All Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_history(data_cost_history_total)\n",
    "plot_time_history(data_time_history_total)\n",
    "plot_loss_history(data_train_loss_history_total, data_test_loss_history_total)\n",
    "plot_accuracy_history(data_train_accuracy_history_total, data_test_accuracy_history_total)\n",
    "plot_error_history(data_train_error_history_total, data_test_error_history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2 Analysing ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
