{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Statistical Learning Part 2 - Experiment Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 Environment Setup ##\n",
    "\n",
    "In this section, libraries, datasets and associative python programme are imported, as well as the setup of the experiment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Packages and Libraries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyter\n",
    "#!pip install numpy\n",
    "#!pip install torch torchvision \n",
    "#!pip install matplotlib\n",
    "#!pip install pandas\n",
    "#!pip install tensorflow\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install torch_xla\n",
    "#!pip install torch-neuron --extra-index-url=https://pip.repos.neuron.amazonaws.com/\n",
    "#!pip install pytorch torchvision cudatoolkit=9.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import random\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Python Programme**\n",
    "\n",
    "Here is the section for importing external python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Set up Experiment Environment ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilize GPU**\n",
    "\n",
    "For local environments, please utilize the GPU to speed up the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utililze TPU**\n",
    "\n",
    "To shorten the training time, we highly recommend that experiments should be done in Google Colab. Enable the following code in the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume that you are on the Google Colab platform.\n",
    "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "def to_device(data, device):\n",
    "    data.to(device)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.2 Datasets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.0 Preprocessing Functions ####\n",
    "Here are the functions for preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    mean = torch.mean(tensor)\n",
    "    std = torch.std(tensor)\n",
    "    normalized_tensor = (tensor - mean) / std\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.1 Importing Datasets from Packages ####\n",
    "This will load the dataset automatically downloaded from the package. Remember the datasets will be stored in the folder \"Datasets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Download Dataset\n",
    "MNIST_train_dataset = MNIST(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_test_dataset = MNIST(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the dataset\n",
    "print(\"=== Raw Data Samples from the MNIST Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_train_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"=== Raw Data Samples from the MNIST Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_test_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Download Dataset\n",
    "CIFAR10_train_dataset = CIFAR10(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR10_test_dataset = CIFAR10(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the CIFAR10 train dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_train_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the raw data from the CIFAR10 test dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_test_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.2 Importing Datasets from Downloaded Files ####\n",
    "This will load the dataset downloaded in the local directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**\n",
    "\n",
    "The source of this dataset comes from https://www.kaggle.com/c/GiveMeSomeCredit\n",
    "\n",
    "Note: This dataset is borrowed from the datasets used in the competitive task in FTEC2101 Optimization Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is Give Me Some Credit Dataset.\n",
      "Number of samples in the train dataset: 16657\n",
      "Number of samples in the test dataset: 34870\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset Structure\n",
    "class Give_Me_Some_Credit_Dataset_Class(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for _, row in data_frame.iterrows():\n",
    "            features = row.iloc[:-1].values.astype(np.float32)\n",
    "            label = row.iloc[-1]\n",
    "            data.append(features)\n",
    "            targets.append(float(int(label)))\n",
    "        \n",
    "        data = torch.tensor(data)\n",
    "        targets = torch.tensor(targets)\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Give_Me_Some_Credit_train_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-train.csv', train=True, transform=normalize_tensor)\n",
    "Give_Me_Some_Credit_test_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-test.csv', train=False, transform=normalize_tensor)\n",
    "\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**\n",
    "\n",
    "Note: This dataset is best suited for binary classification. The training dataset contains 400000 objects. Each object is described by 2001 columns. The first column contains the label value, all other columns contain numerical features. The validation dataset contains 100000 objects. The structure is identical to the training dataset.\n",
    "\n",
    "Warning: The loading time for this dataset is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[337], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m features, label\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m Epsilon_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEpsilonDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Datasets/epsilon/epsilon_normalized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m Epsilon_test_dataset \u001b[38;5;241m=\u001b[39m EpsilonDataset(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Datasets/epsilon/epsilon_normalized.t\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Epsilon_train_dataset\n",
      "Cell \u001b[1;32mIn[337], line 8\u001b[0m, in \u001b[0;36mEpsilonDataset.__init__\u001b[1;34m(self, root, train, transform, target_transform)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;241m=\u001b[39m target_transform\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[337], line 25\u001b[0m, in \u001b[0;36mEpsilonDataset._load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_frame\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 25\u001b[0m     label, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m     27\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "Cell \u001b[1;32mIn[337], line 11\u001b[0m, in \u001b[0;36mEpsilonDataset.process_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_line\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[1;32m---> 11\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     label, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(line[\u001b[38;5;241m0\u001b[39m]), line[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     13\u001b[0m     value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Define Dataset Structure\n",
    "class EpsilonDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(line[0]), line[1:]\n",
    "        value = torch.zeros(line[1:].size())\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx) - 1] = float(val)\n",
    "        return label, value\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root, nrows=20000)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(self.root, 'r') as fp:\n",
    "            for line in fp:\n",
    "                label, value = self.process_line(line.strip(\"\\n\"))\n",
    "                data.append(value)\n",
    "                targets.append(label)\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Epsilon_train_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized', train=True, transform=None)\n",
    "Epsilon_test_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized.t', train=False, transform=None)\n",
    "\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Classes, Functions and Algorithms ##\n",
    "All common and helping functions for machine learning tasks are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#     Helping Functions    #\n",
    "############################\n",
    "# Random Seed Function\n",
    "# To ensure a same training result under the random process, you might need to set the random seed via this function.\n",
    "def set_random_seed(custom_random_seed):\n",
    "    torch.manual_seed(custom_random_seed)\n",
    "    random.seed(custom_random_seed)\n",
    "    np.random.seed(custom_random_seed)\n",
    "\n",
    "# Convert anything into a list if input is not a list\n",
    "def convert_to_list(input_list):\n",
    "    if not isinstance(input_list, list):\n",
    "        input_list = [input_list]\n",
    "    return input_list\n",
    "\n",
    "# Graph Plotting Functions\n",
    "def plot_cost_history(cost_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Cost\", title_name=\"Culminative Send Cost History\"):\n",
    "    cost_history_list = convert_to_list(cost_history_list)\n",
    "    for i, cost_history in enumerate(cost_history_list):\n",
    "        plt.plot(cost_history, label=f\"Cost History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(cost_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_culminative_send_cost_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_history(time_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Culminative Time Used\", title_name=\"Time History\"):\n",
    "    time_history_list = convert_to_list(time_history_list)\n",
    "    for i, time_history in enumerate(time_history_list):\n",
    "        plt.plot(time_history, label=f\"Time History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(time_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_time_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(train_loss_history_list=[], test_loss_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Loss\", title_name=\"Loss History\"):\n",
    "    train_loss_history_list = convert_to_list(train_loss_history_list)\n",
    "    test_loss_history_list = convert_to_list(test_loss_history_list)\n",
    "    for i, train_loss_history in enumerate(train_loss_history_list):\n",
    "        plt.plot(train_loss_history, label=f\"Train Loss History {i+1}\")\n",
    "    for i, test_loss_history in enumerate(test_loss_history_list):\n",
    "        plt.plot(test_loss_history, label=f\"Test Loss History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_loss_history_list) + len(test_loss_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_loss_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history(train_accuracy_history_list=[], test_accuracy_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Accuracy\", title_name=\"Accuracy History\"):\n",
    "    train_accuracy_history_list = convert_to_list(train_accuracy_history_list)\n",
    "    test_accuracy_history_list = convert_to_list(test_accuracy_history_list)\n",
    "    for i, train_accuracy_history in enumerate(train_accuracy_history_list):\n",
    "        plt.plot(train_accuracy_history, label=f\"Train Accuracy History {i+1}\")\n",
    "    for i, test_accuracy_history in enumerate(test_accuracy_history_list):\n",
    "        plt.plot(test_accuracy_history, label=f\"Test Accuracy History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_accuracy_history_list) + len(test_accuracy_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_accuracy_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history(train_error_history_list=[], test_error_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Error\", title_name=\"Error History\"):\n",
    "    train_error_history_list = convert_to_list(train_error_history_list)\n",
    "    test_error_history_list = convert_to_list(test_error_history_list)\n",
    "    for i, train_error_history in enumerate(train_error_history_list):\n",
    "        plt.plot(train_error_history, label=f\"Train Error History {i+1}\")\n",
    "    for i, test_error_history in enumerate(test_error_history_list):\n",
    "        plt.plot(test_error_history, label=f\"Test Error History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_error_history_list) + len(test_error_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_error_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Error Rate Calculation\n",
    "def get_accuracy(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "def get_error(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions != labels).item() / len(predictions))\n",
    "\n",
    "############################\n",
    "#  Custom Loss Functions   #\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "#     Custom Optimizer     #\n",
    "############################\n",
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)\n",
    "\n",
    "############################\n",
    "#   Neural Network Model   #\n",
    "############################\n",
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=Linear_Model_in_features, out_features=Linear_Model_out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "class MNIST_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 7 * 7, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "class CIFAR10_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_stack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "\n",
    "            torch.nn.Flatten(), \n",
    "            torch.nn.Linear(256*4*4, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation_stack(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "############################\n",
    "#    Iterate Algorithm     #\n",
    "############################\n",
    "def iterate_model(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "def evaluate_model(model, dataloader, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "    return loss_average, accuracy_average, error_average\n",
    "\n",
    "############################\n",
    "#      Client Devices      #\n",
    "############################\n",
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, client_id, model, optimizer, dataset, batch_size, iterate_func, loss_func, accuracy_func=get_accuracy, error_func=get_error):\n",
    "        self.id = client_id\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DeviceDataLoader(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        self.iterate_func = iterate_func\n",
    "        self.loss_func = loss_func\n",
    "        self.accuracy_func = accuracy_func\n",
    "        self.error_func = error_func\n",
    "\n",
    "    def load_weights(self, weights):\n",
    "        self.model.load_state_dict(weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.id\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def save_local_history(self, num_epochs, loss_history, accuracy_history, error_history, time_history, value=train_start_time):\n",
    "        filename = \"{}_client_{}_with_local_epochs_{}_local_loss_accuracy_error_history_{}.npy\".format(current_dataset_name, self.id, num_epochs, value)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, loss_history=loss_history, accuracy_history=accuracy_history, error_history=error_history, time_history=time_history)\n",
    "\n",
    "    def train(self, num_epochs, show_history=False):\n",
    "        if show_history:\n",
    "            print(f\"!-- Client {self.id} start iterations. ---!\")\n",
    "        loss_history, accuracy_history, error_history, time_history = self.iterate_func(self.model, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func)\n",
    "        if show_history:\n",
    "            plot_time_history(time_history)\n",
    "            plot_loss_history(loss_history)\n",
    "            plot_accuracy_history(accuracy_history)\n",
    "            plot_error_history(error_history)\n",
    "            print(f\"!-- Client {self.id} finish iterations. ---!\")\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Distribute the training datasets to clients, remember it returns an array of datasets\n",
    "def split_datasets_for_clients_random(dataset, num_clients=1):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_clients = total_sample_size // num_clients\n",
    "    client_datasets = random_split(dataset, [min(i + samples_per_clients, total_sample_size) - i for i in range(0, total_sample_size, samples_per_clients)])\n",
    "    return client_datasets\n",
    "\n",
    "# Be careful where the label placed and remember to modify the sample[1] if necessary\n",
    "# Not correct\n",
    "def split_datasets_by_label(dataset, num_labels=10):\n",
    "    sorted_by_value = [0] * num_labels\n",
    "    for i in range(num_labels):\n",
    "        sorted_by_value[i] =(dataset.data[np.where(np.array(dataset.targets) == i)])\n",
    "        np.random.shuffle(sorted_by_value[i])\n",
    "    return sorted_by_value\n",
    "\n",
    "def split_datasets_for_clients_label(dataset, num_clients=1, num_labels=10):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_client = total_sample_size // num_clients\n",
    "    remaining_samples = total_sample_size % num_clients\n",
    "\n",
    "    # Determine labels for each client\n",
    "    labels_per_client = num_labels // num_clients\n",
    "    client_labels = [list(range(i * labels_per_client, (i + 1) * labels_per_client + 1)) for i in range(num_clients)]\n",
    "    remaining_labels = list(range(num_clients * labels_per_client, num_labels))\n",
    "    for client_id in range(num_clients):\n",
    "        client_labels[client_id].extend(remaining_labels[:client_id])\n",
    "\n",
    "    # Filter samples for each client\n",
    "    client_datasets = []\n",
    "    for client_id in range(num_clients):\n",
    "        labels = client_labels[client_id]\n",
    "        filtered_dataset = [sample for sample in dataset if dataset.targets in labels]\n",
    "\n",
    "        # Adjust the length of the dataset for the last client to include remaining samples\n",
    "        client_samples = samples_per_client + (remaining_samples if client_id == num_clients - 1 else 0)\n",
    "\n",
    "        client_dataset = random_split(filtered_dataset, [client_samples] * num_clients)\n",
    "        client_datasets.append(client_dataset)\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, model_list, optimizer_list, dataset_list, batch_size_list, iterate_func_list, loss_func_list, accuracy_func_list, error_func_list):\n",
    "    client_device = [None] * num_clients\n",
    "    for client_id in range(num_clients):\n",
    "        client_device[client_id] = ClientDevice(client_id, model_list[client_id], optimizer_list[client_id], dataset_list[client_id], batch_size_list[client_id], iterate_func_list[client_id], loss_func_list[client_id], accuracy_func_list[client_id], error_func_list[client_id])\n",
    "    return client_device\n",
    "\n",
    "#########################################\n",
    "#     Federated Learning Algorithms     #\n",
    "#########################################\n",
    "def federated_averaging(client_weights_total):\n",
    "    total_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / total_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / total_clients\n",
    "\n",
    "    return aggregate_weights\n",
    "\n",
    "def iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, global_epochs, local_epochs, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        client_weights_total = []\n",
    "        for client in client_list:\n",
    "            client.load_weights(global_weights)\n",
    "            client_weights = client.train(num_epochs=local_epochs)\n",
    "            client_weights_total.append(client_weights)\n",
    "            send_cost += sum(value.numel() for value in client_weights.values())\n",
    "        global_weights.update(aggregate_func(client_weights_total))\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_model(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#    Training Algorithm    #\n",
    "############################\n",
    "def train_neural_network_model(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True):\n",
    "    train_loss_history, train_accuracy_history, train_error_history, time_history = iterate_model(model, train_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "    if test_dataloader is not None:\n",
    "        for epoch in range(num_epochs):\n",
    "            test_loss, test_accuracy, test_error = evaluate_model(model=model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_train_CNN_with_num_epochs_{}_batch_size_{}_lr_{}_{}.npy\".format(current_dataset_name, num_epochs, batch_size, learning_rate, train_start_time)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "        torch.save(model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_federated_learning_model(global_model, train_dataloader, test_dataloader, global_epochs, local_epochs, client_list, batch_size, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True):\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}.npy\".format(current_dataset_name, \"FedAvg\", global_epochs, local_epochs, len(client_list), batch_size, train_start_time)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "        torch.save(global_model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#   Experiment Functions   #\n",
    "############################\n",
    "def experiment_neural_network_model(train_dataset, test_dataset, modelClass, optimizerClass, train_func, epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    epochs_list = convert_to_list(epochs_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "\n",
    "    local_epochs_list_size = len(epochs_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = error_func_list\n",
    "    else:\n",
    "        iteration_list = epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for num_epochs is {epochs_list[n]} ===')\n",
    "            num_epochs = epochs_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        \n",
    "        model = to_device(modelClass(), device)\n",
    "        print(model)\n",
    "        print(model.state_dict())\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        optimizer = optimizerClass(model.parameters(), learning_rate)\n",
    "\n",
    "        time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_func(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_federated_learning_model(train_dataset, test_dataset, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "\n",
    "    global_epochs_list_size = len(global_epochs_list)\n",
    "    local_epochs_list_size = len(local_epochs_list)\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    aggregate_func_list_size = len(aggregate_func_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = local_epochs_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = num_clients_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = aggregate_func_list\n",
    "    elif compare_id == 7:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 8:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 9:\n",
    "        iteration_list = error_func_list\n",
    "    else:\n",
    "        iteration_list = global_epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for local_epochs is {local_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for aggregate function is {aggregate_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 7:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 8:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 9:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for global_epochs is {global_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[n]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        \n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        client_model_list = [to_device(modelClass(), device)] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = split_datasets_for_clients_random(train_dataset, num_clients)\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, global_epochs, local_epochs, client_list, batch_size, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Experiment ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Choosing Dataset ###\n",
    "\n",
    "In this section, execute a cell only to choose a dataset you want do experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n",
      "Number of samples in the train dataset after random split: 73\n",
      "Number of samples in the test dataset after random split: 70\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.001231234\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.007\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 784\n",
    "model_type_preset = MNIST_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIFAR10_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR10_train_dataset\u001b[49m\n\u001b[0;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CIFAR10_test_dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Show Dataset Status\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CIFAR10_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.50\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.007\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 1024\n",
    "model_type_preset = CIFAR10_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is Give Me Some Credit Dataset.\n",
      "Number of samples in the train dataset: 16657\n",
      "Number of samples in the test dataset: 34870\n",
      "Number of samples in the train dataset after random split: 16657\n",
      "Number of samples in the test dataset after random split: 34870\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.01\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.01\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "Linear_Model_in_features = 7\n",
    "Linear_Model_out_features = 1\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Epsilon_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEpsilon_train_dataset\u001b[49m\n\u001b[0;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m Epsilon_test_dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Show Dataset Status\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Epsilon_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.50\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.007\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Neural Network Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare number of epochs\n",
    "\n",
    "2: compare learning rate\n",
    "\n",
    "3: compare batch size\n",
    "\n",
    "4: compare loss function\n",
    "\n",
    "5: compare accuracy function\n",
    "\n",
    "6: compare error function\n",
    "\n",
    "Others: compare number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is Give Me Some Credit Dataset.\n",
      "The current train start time is 2023-12-24 18.01.10.\n",
      "=== The training for learning_rate_list is 0.02 ===\n",
      "Linear_Model(\n",
      "  (linear): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n",
      "OrderedDict([('linear.weight', tensor([[-0.2337, -0.3289,  0.2761, -0.2793, -0.0277, -0.2983, -0.1621]])), ('linear.bias', tensor([-0.1122]))])\n",
      "Epoch [1/500], Average Loss: 0.2324859648942947, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 0.9690416000084952\n",
      "Epoch [2/500], Average Loss: 0.2036251425743103, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 1.9331431000027806\n",
      "Epoch [3/500], Average Loss: 0.2021695077419281, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 2.924773500068113\n",
      "Epoch [4/500], Average Loss: 0.2005174458026886, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 3.8831135000800714\n",
      "Epoch [5/500], Average Loss: 0.1989207416772842, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 4.826028499985114\n",
      "Epoch [6/500], Average Loss: 0.1987615823745728, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 5.745995400007814\n",
      "Epoch [7/500], Average Loss: 0.1975536048412323, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 6.652017100015655\n",
      "Epoch [8/500], Average Loss: 0.1972932964563370, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 7.574804700096138\n",
      "Epoch [9/500], Average Loss: 0.1973059326410294, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 8.515221400070004\n",
      "Epoch [10/500], Average Loss: 0.1963666081428528, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 9.4331727999961\n",
      "Epoch [11/500], Average Loss: 0.1960604786872864, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 10.341691300040111\n",
      "Epoch [12/500], Average Loss: 0.1962849497795105, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 11.243300500093028\n",
      "Epoch [13/500], Average Loss: 0.1953048557043076, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 12.163414300070144\n",
      "Epoch [14/500], Average Loss: 0.1955365687608719, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 13.076709900051355\n",
      "Epoch [15/500], Average Loss: 0.1952386349439621, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 13.981361599988304\n",
      "Epoch [16/500], Average Loss: 0.1950254440307617, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 14.889497200027108\n",
      "Epoch [17/500], Average Loss: 0.1954464614391327, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 15.801164800068364\n",
      "Epoch [18/500], Average Loss: 0.1955353915691376, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 16.723708000034094\n",
      "Epoch [19/500], Average Loss: 0.1952203512191772, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 17.69230500003323\n",
      "Epoch [20/500], Average Loss: 0.1951613873243332, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 18.693788700038567\n",
      "Epoch [21/500], Average Loss: 0.1952659338712692, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 19.711894700070843\n",
      "Epoch [22/500], Average Loss: 0.1952965557575226, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 20.68910750001669\n",
      "Epoch [23/500], Average Loss: 0.1952609866857529, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 21.615530700073577\n",
      "Epoch [24/500], Average Loss: 0.1951835602521896, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 22.592847699997947\n",
      "Epoch [25/500], Average Loss: 0.1946458071470261, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 23.53249470004812\n",
      "Epoch [26/500], Average Loss: 0.1948066204786301, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 24.517439700081013\n",
      "Epoch [27/500], Average Loss: 0.1952765285968781, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 25.49121310003102\n",
      "Epoch [28/500], Average Loss: 0.1951249688863754, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 26.41045550000854\n",
      "Epoch [29/500], Average Loss: 0.1951436102390289, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 27.346801900072023\n",
      "Epoch [30/500], Average Loss: 0.1951851546764374, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 28.283952100086026\n",
      "Epoch [31/500], Average Loss: 0.1952523291110992, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 29.220112600014545\n",
      "Epoch [32/500], Average Loss: 0.1949714124202728, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 30.149215399986133\n",
      "Epoch [33/500], Average Loss: 0.1953021883964539, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 31.06836540007498\n",
      "Epoch [34/500], Average Loss: 0.1946071088314056, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 31.988280500052497\n",
      "Epoch [35/500], Average Loss: 0.1951248794794083, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 32.915659700054675\n",
      "Epoch [36/500], Average Loss: 0.1952120363712311, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 33.84097910008859\n",
      "Epoch [37/500], Average Loss: 0.1949498057365417, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 34.767251100041904\n",
      "Epoch [38/500], Average Loss: 0.1948592811822891, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 35.724623600021005\n",
      "Epoch [39/500], Average Loss: 0.1953819096088409, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 36.66010480001569\n",
      "Epoch [40/500], Average Loss: 0.1948165893554688, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 37.61732349998783\n",
      "Epoch [41/500], Average Loss: 0.1946054846048355, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 38.55346460000146\n",
      "Epoch [42/500], Average Loss: 0.1954797208309174, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 39.54016169998795\n",
      "Epoch [43/500], Average Loss: 0.1952362358570099, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 40.47788700007368\n",
      "Epoch [44/500], Average Loss: 0.1953345239162445, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 41.42482190008741\n",
      "Epoch [45/500], Average Loss: 0.1952320784330368, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 42.39048770000227\n",
      "Epoch [46/500], Average Loss: 0.1951756030321121, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 43.39011879998725\n",
      "Epoch [47/500], Average Loss: 0.1949826478958130, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 44.35419310000725\n",
      "Epoch [48/500], Average Loss: 0.1946171671152115, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 45.380759000079706\n",
      "Epoch [49/500], Average Loss: 0.1953027546405792, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 46.47570340009406\n",
      "Epoch [50/500], Average Loss: 0.1955317407846451, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 47.47351380006876\n",
      "Epoch [51/500], Average Loss: 0.1954549849033356, Average Accuracy: 0.0000000000000000, Average Error: 1.0000000000000000, Culminative Time Used: 48.46860680007376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[426], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m train_start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe current train start time is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mexperiment_neural_network_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[422], line 586\u001b[0m, in \u001b[0;36mexperiment_neural_network_model\u001b[1;34m(train_dataset, test_dataset, modelClass, optimizerClass, train_func, epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id, show_history, save_result)\u001b[0m\n\u001b[0;32m    582\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), device)\n\u001b[0;32m    584\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizerClass(model\u001b[38;5;241m.\u001b[39mparameters(), learning_rate)\n\u001b[1;32m--> 586\u001b[0m time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m time_history_total\u001b[38;5;241m.\u001b[39mappend(time_history)\n\u001b[0;32m    589\u001b[0m train_loss_history_total\u001b[38;5;241m.\u001b[39mappend(train_loss_history)\n",
      "Cell \u001b[1;32mIn[422], line 430\u001b[0m, in \u001b[0;36mtrain_neural_network_model\u001b[1;34m(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func, accuracy_func, error_func, show_history, save_result)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_neural_network_model\u001b[39m(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy, accuracy_func\u001b[38;5;241m=\u001b[39mget_accuracy, error_func\u001b[38;5;241m=\u001b[39mget_error, show_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 430\u001b[0m     train_loss_history, train_accuracy_history, train_error_history, time_history \u001b[38;5;241m=\u001b[39m \u001b[43miterate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     test_loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    433\u001b[0m     test_accuracy_history \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[422], line 222\u001b[0m, in \u001b[0;36miterate_model\u001b[1;34m(model, dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history)\u001b[0m\n\u001b[0;32m    220\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    221\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 222\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mDeviceDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "num_epochs_list = 5\n",
    "learning_rate_list = [0.02, 0.04]\n",
    "batch_size_list = 128\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "num_epochs_list = 25\n",
    "learning_rate_list = 0.02\n",
    "batch_size_list = [10, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3 Federated Learning Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare global epochs\n",
    "\n",
    "2: compare local epochs\n",
    "\n",
    "3: compare number of clients\n",
    "\n",
    "4: compare learning rate\n",
    "\n",
    "5: compare batch size\n",
    "\n",
    "6: compare the aggregate weight algorithms\n",
    "\n",
    "7: compare loss function\n",
    "\n",
    "8: compare accuracy function\n",
    "\n",
    "9: compare error function\n",
    "\n",
    "Others: compare global epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = [2, 3, 4, 5]\n",
    "num_clients_list = 10\n",
    "learning_rate_list = 0.02\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_federated_learning_model(train_dataset, test_dataset, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Number of Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model\n",
    "loss_func = loss_func_preset\n",
    "\n",
    "global_epochs_list = 20\n",
    "local_epochs_list = 2\n",
    "num_clients_list = [2, 2]\n",
    "learning_rate_list = 0.02\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_federated_learning_model(train_dataset, test_dataset, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model\n",
    "loss_func = loss_func_preset\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 3\n",
    "num_clients_list = 10\n",
    "learning_rate_list = [0.01, 0.1, 1.0]\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_federated_learning_model(train_dataset, test_dataset, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model\n",
    "loss_func = loss_func_preset\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 3\n",
    "num_clients_list = 10\n",
    "learning_rate_list = 0.02\n",
    "batch_size_list = [10, 128, 1000]\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 5\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_federated_learning_model(train_dataset, test_dataset, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.1 Loading Data ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear and Initialize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cost_history_total = []\n",
    "data_time_history_total = []\n",
    "data_train_loss_history_total = []\n",
    "data_train_accuracy_history_total = []\n",
    "data_train_error_history_total = []\n",
    "data_test_loss_history_total = []\n",
    "data_test_accuracy_history_total = []\n",
    "data_test_error_history_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "load_dataset_name = \"MNIST\"\n",
    "load_aggreagte_func = \"FedAvg\"\n",
    "load_global_epochs = 100\n",
    "load_local_epochs = 10\n",
    "load_num_clients = 10\n",
    "load_batch_size = 128\n",
    "load_train_start_time = \"2023-12-04 23.14.39\"\n",
    "data_append_load = True\n",
    "\n",
    "# Load the file\n",
    "filename_load = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}.npy\".format(load_dataset_name, load_aggreagte_func, load_global_epochs, load_local_epochs, load_num_clients, load_batch_size, load_train_start_time)\n",
    "load_result = np.load(filename_load)\n",
    "print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "# Load the attributes from the file\n",
    "data_cost_history = load_result['cost_history']\n",
    "data_time_history = load_result['time_history']\n",
    "data_train_loss_history = load_result['train_loss_history']\n",
    "data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "data_train_error_history = load_result['train_error_history']\n",
    "data_test_loss_history = load_result['test_loss_history']\n",
    "data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "print(\"=======Content of the File=======\")\n",
    "print(load_result.files)\n",
    "\n",
    "print(\"=======VISUALIZATION RESULT=======\")\n",
    "plot_cost_history([data_cost_history], save=False)\n",
    "plot_time_history([data_time_history], save=False)\n",
    "plot_loss_history([data_train_loss_history], [data_test_loss_history], save=False)\n",
    "plot_accuracy_history([data_train_accuracy_history], [data_test_accuracy_history], save=False)\n",
    "plot_error_history([data_train_error_history], [data_test_error_history], save=False)\n",
    "\n",
    "print(\"=======STATUS RESULT=======\")\n",
    "print(\"Cost History: \", data_cost_history)\n",
    "print(\"Time History: \", data_time_history)\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"Train Loss History: \", data_train_loss_history)\n",
    "print(\"Train Accuracy History: \", data_train_accuracy_history)\n",
    "print(\"Train Error History: \", data_train_error_history)\n",
    "\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"Test Loss History: \", data_test_loss_history)\n",
    "print(\"Test Accuracy History: \", data_test_accuracy_history)\n",
    "print(\"Test Error History: \", data_test_error_history)\n",
    "\n",
    "# Append the data\n",
    "if data_append_load:\n",
    "    data_cost_history_total.append(data_cost_history)\n",
    "    data_time_history_total.append(data_time_history)\n",
    "    data_train_loss_history_total.append(data_train_loss_history)\n",
    "    data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "    data_train_error_history_total.append(data_train_error_history)\n",
    "    data_test_loss_history_total.append(data_test_loss_history)\n",
    "    data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "    data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Using Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "# load_loop_max correspond to number of files you want to load\n",
    "load_dataset_name_list = [\"MNIST\"]\n",
    "load_aggreagte_func_list = [\"FedAvg\"]\n",
    "load_global_epochs_list = [2]\n",
    "load_local_epochs_list = [1]\n",
    "load_num_clients_list = [2]\n",
    "load_batch_size_list = [128]\n",
    "load_train_start_time_list = [\"2023-12-04 19.48.18\"]\n",
    "load_loop_max = 1\n",
    "data_append_load = True\n",
    "\n",
    "for n in range(load_loop_max):\n",
    "    # Load the file\n",
    "    filename_load = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}.npy\".format(load_dataset_name_list[n], load_aggreagte_func_list[n], load_global_epochs_list[n], load_local_epochs_list[n], load_num_clients_list[n], load_batch_size_list[n], load_train_start_time_list[n])\n",
    "    load_result = np.load(filename_load)\n",
    "    print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "    # Load the attributes from the file\n",
    "    data_cost_history = load_result['cost_history']\n",
    "    data_time_history = load_result['time_history']\n",
    "    data_train_loss_history = load_result['train_loss_history']\n",
    "    data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "    data_train_error_history = load_result['train_error_history']\n",
    "    data_test_loss_history = load_result['test_loss_history']\n",
    "    data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "    data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "    # Append the data\n",
    "    if data_append_load:\n",
    "        data_cost_history_total.append(data_cost_history)\n",
    "        data_time_history_total.append(data_time_history)\n",
    "        data_train_loss_history_total.append(data_train_loss_history)\n",
    "        data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "        data_train_error_history_total.append(data_train_error_history)\n",
    "        data_test_loss_history_total.append(data_test_loss_history)\n",
    "        data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "        data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize All Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_history(data_cost_history_total, save=False)\n",
    "plot_time_history(data_time_history_total, save=False)\n",
    "plot_loss_history(data_train_loss_history_total, data_test_loss_history_total, save=False)\n",
    "plot_accuracy_history(data_train_accuracy_history_total, data_test_accuracy_history_total, save=False)\n",
    "plot_error_history(data_train_error_history_total, data_test_error_history_total, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Local Updates Epochs Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_local_update_epochs_list = [1, 10, 3, 4, 5, 10]\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_loss_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_accuracy_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_error_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Number of Clients Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_clients_list = [1, 5, 10, 20]\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with number of clients = {plot_num_clients_list[i]}\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different number of clients\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_loss_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different number of clients\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_accuracy_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with number of clients = {plot_num_clients_list[i]}\")\n",
    "#for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "#    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different number of clients\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_error_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Batch Size Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_size_list = [1, 5, 10, 20]\n",
    "\n",
    "plot_different_time_history = convert_to_list(data_time_history_total)\n",
    "for i, plot_time_history in enumerate(plot_different_time_history):\n",
    "    plt.plot(plot_time_history, label=f\"Time History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Culminative Time Used\")\n",
    "plt.title(\"Time History\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_time_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with batch size = {plot_batch_size_list[i]}\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different batch size\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_loss_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different batch size\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_accuracy_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with batch size = {plot_batch_size_list[i]}\")\n",
    "#for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "#    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different batch size\")\n",
    "plt.legend()\n",
    "#plt.savefig(f'{current_dataset_name}_error_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2 Analysing ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Analysis**\n",
    "\n",
    "We analysis the variance of a particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance = statistics.variance(data_cost_history)\n",
    "time_variance = statistics.variance(data_time_history)\n",
    "train_loss_variance = statistics.variance(data_train_loss_history)\n",
    "train_accuracy_variance = statistics.variance(data_train_accuracy_history)\n",
    "train_error_variance = statistics.variance(data_train_error_history)\n",
    "test_loss_variance = statistics.variance(data_test_loss_history)\n",
    "test_accuracy_variance = statistics.variance(data_test_accuracy_history)\n",
    "test_error_variance = statistics.variance(data_test_error_history)\n",
    "print(\"=======VARIANCE RESULT=======\")\n",
    "print(\"Cost Variance: \", cost_variance)\n",
    "print(\"Time Variance: \", time_variance)\n",
    "print(\"Train Loss Variance: \", train_loss_variance)\n",
    "print(\"Train Accuracy Variance: \", train_accuracy_variance)\n",
    "print(\"Train Error Variance: \", train_error_variance)\n",
    "print(\"Test Loss Variance: \", test_loss_variance)\n",
    "print(\"Test Accuracy Variance: \", test_accuracy_variance)\n",
    "print(\"Test Error Variance: \", test_error_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[:len(data_cost_history) // 2])\n",
    "time_variance_subset = statistics.variance(data_time_history[:len(data_time_history) // 2])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[:len(data_train_loss_history) // 2])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[:len(data_train_accuracy_history) // 2])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[:len(data_train_error_history) // 2])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[:len(data_test_loss_history) // 2])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[:len(data_test_accuracy_history) // 2])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[:len(data_test_error_history) // 2])\n",
    "print(\"=======VARIANCE FIRST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[len(data_cost_history) // 2:])\n",
    "time_variance_subset = statistics.variance(data_time_history[len(data_time_history) // 2:])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[len(data_train_loss_history) // 2:])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[len(data_train_accuracy_history) // 2:])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[len(data_train_error_history) // 2:])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[len(data_test_loss_history) // 2:])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[len(data_test_accuracy_history) // 2:])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[len(data_test_error_history) // 2:])\n",
    "print(\"=======VARIANCE LAST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
