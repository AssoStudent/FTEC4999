{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Statistical Learning Part 2 - Experiment Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 Environment Setup ##\n",
    "\n",
    "In this section, libraries, datasets and associative python programme are imported, as well as the setup of the experiment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Packages and Libraries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyter\n",
    "#!pip install numpy\n",
    "#!pip install torch torchvision \n",
    "#!pip install matplotlib\n",
    "#!pip install pandas\n",
    "#!pip install tensorflow\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install torchvision\n",
    "#!pip install torch_xla\n",
    "#!pip install torch-neuron --extra-index-url=https://pip.repos.neuron.amazonaws.com/\n",
    "#!pip install pytorch torchvision cudatoolkit=9.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import random\n",
    "import string\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, ConcatDataset, Subset\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Python Programme**\n",
    "\n",
    "Here is the section for importing external python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Set up Experiment Environment ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilize GPU**\n",
    "\n",
    "For local environments, please utilize the GPU to speed up the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utililze TPU**\n",
    "\n",
    "To shorten the training time, we highly recommend that experiments should be done in Google Colab. Enable the following code in the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume that you are on the Google Colab platform.\n",
    "#!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "def to_device(data, device):\n",
    "    data.to(device)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.2 Datasets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.0 Preprocessing Functions ####\n",
    "Here are the functions for preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global current_dataset_name\n",
    "global current_method_name\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    mean = torch.mean(tensor)\n",
    "    std = torch.std(tensor)\n",
    "    normalized_tensor = (tensor - mean) / std\n",
    "    return normalized_tensor\n",
    "\n",
    "##############################\n",
    "# Text Dataset Preprocessing #\n",
    "##############################\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.1 Importing Datasets from Packages ####\n",
    "This will load the dataset automatically downloaded from the package. Remember the datasets will be stored in the folder \"Datasets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Download Dataset\n",
    "MNIST_train_dataset = MNIST(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_test_dataset = MNIST(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the dataset\n",
    "print(\"=== Raw Data Samples from the MNIST Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_train_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"=== Raw Data Samples from the MNIST Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_test_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Download Dataset\n",
    "CIFAR10_train_dataset = CIFAR10(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR10_test_dataset = CIFAR10(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the CIFAR10 train dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_train_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the raw data from the CIFAR10 test dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_test_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.2 Importing Datasets from Downloaded Files ####\n",
    "This will load the dataset downloaded in the local directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**\n",
    "\n",
    "The source of this dataset comes from https://www.kaggle.com/c/GiveMeSomeCredit\n",
    "\n",
    "Note: This dataset is borrowed from the datasets used in the competitive task in FTEC2101 Optimization Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Structure\n",
    "class Give_Me_Some_Credit_Dataset_Class(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for _, row in data_frame.iterrows():\n",
    "            features = row.iloc[:-1].values.astype(np.float32)\n",
    "            label = row.iloc[-1]\n",
    "            data.append(features)\n",
    "            targets.append(float(int(label)))\n",
    "        \n",
    "        data = torch.tensor(data)\n",
    "        targets = torch.tensor(targets)\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Give_Me_Some_Credit_train_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-train.csv', train=True, transform=normalize_tensor)\n",
    "Give_Me_Some_Credit_test_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-test.csv', train=False, transform=normalize_tensor)\n",
    "\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**\n",
    "\n",
    "Note: This dataset is best suited for binary classification. The training dataset contains 400000 objects. Each object is described by 2001 columns. The first column contains the label value, all other columns contain numerical features. The validation dataset contains 100000 objects. The structure is identical to the training dataset.\n",
    "\n",
    "Warning: The loading time for this dataset is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Structure\n",
    "class EpsilonDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(line[0]), line[1:]\n",
    "        value = torch.zeros(line[1:].size())\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx) - 1] = float(val)\n",
    "        return label, value\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root, nrows=20000)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(self.root, 'r') as fp:\n",
    "            for line in fp:\n",
    "                label, value = self.process_line(line.strip(\"\\n\"))\n",
    "                data.append(value)\n",
    "                targets.append(label)\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Epsilon_train_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized', train=True, transform=None)\n",
    "Epsilon_test_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized.t', train=False, transform=None)\n",
    "\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Languages Dataset**\n",
    "\n",
    "The dataset is downloaded from here: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('Datasets/Language_dataset/names/*.txt'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "languages_dataset_category_lines = {}\n",
    "languages_dataset_all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('Datasets/Language_dataset/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages_dataset_all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    languages_dataset_category_lines[category] = lines\n",
    "\n",
    "n_categories = len(languages_dataset_all_categories)\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root, nrows=20000)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(self.root, 'r') as fp:\n",
    "            for line in fp:\n",
    "                label, value = self.process_line(line.strip(\"\\n\"))\n",
    "                data.append(value)\n",
    "                targets.append(label)\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "Languages_train_dataset = LanguageDataset(root='./Datasets/epsilon/epsilon_normalized', train=True, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Classes, Functions and Algorithms ##\n",
    "All common and helping functions for machine learning tasks are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#     Helping Functions    #\n",
    "############################\n",
    "# Random Seed Function\n",
    "# To ensure a same training result under the random process, you might need to set the random seed via this function.\n",
    "def set_random_seed(custom_random_seed):\n",
    "    torch.manual_seed(custom_random_seed)\n",
    "    random.seed(custom_random_seed)\n",
    "    np.random.seed(custom_random_seed)\n",
    "\n",
    "# Convert anything into a list if input is not a list\n",
    "def convert_to_list(input_list):\n",
    "    if not isinstance(input_list, list):\n",
    "        input_list = [input_list]\n",
    "    return input_list\n",
    "\n",
    "# Graph Plotting Functions\n",
    "def plot_cost_history(cost_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Cost\", title_name=\"Culminative Send Cost History\"):\n",
    "    cost_history_list = convert_to_list(cost_history_list)\n",
    "    for i, cost_history in enumerate(cost_history_list):\n",
    "        plt.plot(cost_history, label=f\"Cost History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(cost_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_culminative_send_cost_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_history(time_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Culminative Time Used\", title_name=\"Time History\"):\n",
    "    time_history_list = convert_to_list(time_history_list)\n",
    "    for i, time_history in enumerate(time_history_list):\n",
    "        plt.plot(time_history, label=f\"Time History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(time_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_time_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(train_loss_history_list=[], test_loss_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Loss\", title_name=\"Loss History\"):\n",
    "    train_loss_history_list = convert_to_list(train_loss_history_list)\n",
    "    test_loss_history_list = convert_to_list(test_loss_history_list)\n",
    "    for i, train_loss_history in enumerate(train_loss_history_list):\n",
    "        plt.plot(train_loss_history, label=f\"Train Loss History {i+1}\")\n",
    "    for i, test_loss_history in enumerate(test_loss_history_list):\n",
    "        plt.plot(test_loss_history, label=f\"Test Loss History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_loss_history_list) + len(test_loss_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_loss_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history(train_accuracy_history_list=[], test_accuracy_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Accuracy\", title_name=\"Accuracy History\"):\n",
    "    train_accuracy_history_list = convert_to_list(train_accuracy_history_list)\n",
    "    test_accuracy_history_list = convert_to_list(test_accuracy_history_list)\n",
    "    for i, train_accuracy_history in enumerate(train_accuracy_history_list):\n",
    "        plt.plot(train_accuracy_history, label=f\"Train Accuracy History {i+1}\")\n",
    "    for i, test_accuracy_history in enumerate(test_accuracy_history_list):\n",
    "        plt.plot(test_accuracy_history, label=f\"Test Accuracy History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_accuracy_history_list) + len(test_accuracy_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_accuracy_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history(train_error_history_list=[], test_error_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Error\", title_name=\"Error History\"):\n",
    "    train_error_history_list = convert_to_list(train_error_history_list)\n",
    "    test_error_history_list = convert_to_list(test_error_history_list)\n",
    "    for i, train_error_history in enumerate(train_error_history_list):\n",
    "        plt.plot(train_error_history, label=f\"Train Error History {i+1}\")\n",
    "    for i, test_error_history in enumerate(test_error_history_list):\n",
    "        plt.plot(test_error_history, label=f\"Test Error History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_error_history_list) + len(test_error_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_error_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Error Rate Calculation\n",
    "def get_accuracy(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "def get_error(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions != labels).item() / len(predictions))\n",
    "\n",
    "def relative_rate_to_client_number(num_client, percentage = 1.00):\n",
    "    return round(num_client * percentage)\n",
    "\n",
    "############################\n",
    "#  Custom Loss Functions   #\n",
    "############################\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "#     Custom Optimizer     #\n",
    "############################\n",
    "\n",
    "\n",
    "############################\n",
    "#   Neural Network Model   #\n",
    "############################\n",
    "global Linear_Model_in_features\n",
    "global Linear_Model_out_features\n",
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=Linear_Model_in_features, out_features=Linear_Model_out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "class MNIST_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 7 * 7, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        if len(x.size()) == 1:  # Check if output has a single level of brackets\n",
    "            x = x.unsqueeze(0)  # Add a dimension at the beginning\n",
    "        return x\n",
    "\n",
    "class CIFAR10_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_stack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "\n",
    "            torch.nn.Flatten(), \n",
    "            torch.nn.Linear(256*4*4, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation_stack(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "class RNN_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_stack = torch.nn.Sequential(\n",
    "            torch.nn.RNN()\n",
    "        )\n",
    "\n",
    "############################\n",
    "#    Iterate Algorithm     #\n",
    "############################\n",
    "def evaluate_model_simple(model, dataloader, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "    return loss_average, accuracy_average, error_average\n",
    "\n",
    "def iterate_model_simple(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, test_dataloader=None, include_intial_history=False):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    if include_intial_history is True:\n",
    "        loss, accuracy, error = evaluate_model_simple(model=model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        loss_history.append(loss)\n",
    "        accuracy_history.append(accuracy)\n",
    "        error_history.append(error)\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "    if test_dataloader is not None:\n",
    "        return loss_history, accuracy_history, error_history, time_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "##################################################################\n",
    "#  Dataset Preprocessing Functions Before Splitting For Clients  #\n",
    "##################################################################\n",
    "# Acknowledge from https://github.com/adap/flower/blob/main/baselines/fedprox/fedprox/dataset_preparation.py\n",
    "# Balance: Trims the dataset so each class contains as many elements as the class that contained the least elements.\n",
    "def dataset_balance_classes(trainset, seed=42):\n",
    "    class_counts = np.bincount(trainset.targets)\n",
    "    smallest = np.min(class_counts)\n",
    "    idxs = trainset.targets.argsort()\n",
    "    tmp = [Subset(trainset, idxs[: int(smallest)])]\n",
    "    tmp_targets = [trainset.targets[idxs[: int(smallest)]]]\n",
    "    for count in np.cumsum(class_counts):\n",
    "        tmp.append(Subset(trainset, idxs[int(count) : int(count + smallest)]))\n",
    "        tmp_targets.append(trainset.targets[idxs[int(count) : int(count + smallest)]])\n",
    "    unshuffled = ConcatDataset(tmp)\n",
    "    unshuffled_targets = torch.cat(tmp_targets)\n",
    "    shuffled_idxs = torch.randperm(\n",
    "        len(unshuffled), generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    shuffled = Subset(unshuffled, shuffled_idxs)\n",
    "    shuffled.targets = unshuffled_targets[shuffled_idxs]\n",
    "    return shuffled\n",
    "\n",
    "def dataset_sort_by_class(trainset: Dataset):\n",
    "    class_counts = np.bincount(trainset.targets)\n",
    "    idxs = trainset.targets.argsort()  # sort targets in ascending order\n",
    "\n",
    "    tmp = []  # create subset of smallest class\n",
    "    tmp_targets = []  # same for targets\n",
    "\n",
    "    start = 0\n",
    "    for count in np.cumsum(class_counts):\n",
    "        tmp.append(\n",
    "            Subset(trainset, idxs[start : int(count + start)])\n",
    "        )  # add rest of classes\n",
    "        tmp_targets.append(trainset.targets[idxs[start : int(count + start)]])\n",
    "        start += count\n",
    "    sorted_dataset = ConcatDataset(tmp)  # concat dataset\n",
    "    sorted_dataset.targets = torch.cat(tmp_targets)  # concat targets\n",
    "    return sorted_dataset\n",
    "\n",
    "# Implemention follow Li et al 2020: https://arxiv.org/abs/1812.06127 with default values set accordingly.\n",
    "global custom_power_law_num_labels_per_partition\n",
    "global custom_power_law_min_data_per_partition\n",
    "global custom_power_law_mean\n",
    "global custom_power_law_sigma\n",
    "custom_power_law_num_labels_per_partition = 2\n",
    "custom_power_law_min_data_per_partition = 10\n",
    "custom_power_law_mean = 0.0\n",
    "custom_power_law_sigma = 2.0\n",
    "def dataset_power_law_split(sorted_trainset, num_partitions):\n",
    "    # Custom Parameters\n",
    "    num_labels_per_partition = custom_power_law_num_labels_per_partition\n",
    "    min_data_per_partition = custom_power_law_min_data_per_partition\n",
    "    mean = custom_power_law_mean\n",
    "    sigma = custom_power_law_sigma\n",
    "\n",
    "    targets = sorted_trainset.targets\n",
    "    full_idx = list(range(len(targets)))\n",
    "\n",
    "    class_counts = np.bincount(sorted_trainset.targets)\n",
    "    labels_cs = np.cumsum(class_counts)\n",
    "    labels_cs = [0] + labels_cs[:-1].tolist()\n",
    "\n",
    "    partitions_idx: List[List[int]] = []\n",
    "    num_classes = len(np.bincount(targets))\n",
    "    hist = np.zeros(num_classes, dtype=np.int32)\n",
    "\n",
    "    # assign min_data_per_partition\n",
    "    min_data_per_class = int(min_data_per_partition / num_labels_per_partition)\n",
    "    for u_id in range(num_partitions):\n",
    "        partitions_idx.append([])\n",
    "        for cls_idx in range(num_labels_per_partition):\n",
    "            # label for the u_id-th client\n",
    "            cls = (u_id + cls_idx) % num_classes\n",
    "            # record minimum data\n",
    "            indices = list(\n",
    "                full_idx[\n",
    "                    labels_cs[cls]\n",
    "                    + hist[cls] : labels_cs[cls]\n",
    "                    + hist[cls]\n",
    "                    + min_data_per_class\n",
    "                ]\n",
    "            )\n",
    "            partitions_idx[-1].extend(indices)\n",
    "            hist[cls] += min_data_per_class\n",
    "\n",
    "    # add remaining images following power-law\n",
    "    probs = np.random.lognormal(\n",
    "        mean,\n",
    "        sigma,\n",
    "        (num_classes, int(num_partitions / num_classes), num_labels_per_partition),\n",
    "    )\n",
    "    remaining_per_class = class_counts - hist\n",
    "    # obtain how many samples each partition should be assigned for each of the\n",
    "    # labels it contains\n",
    "    # pylint: disable=too-many-function-args\n",
    "    probs = (\n",
    "        remaining_per_class.reshape(-1, 1, 1)\n",
    "        * probs\n",
    "        / np.sum(probs, (1, 2), keepdims=True)\n",
    "    )\n",
    "\n",
    "    for u_id in range(num_partitions):\n",
    "        for cls_idx in range(num_labels_per_partition):\n",
    "            cls = (u_id + cls_idx) % num_classes\n",
    "            count = int(probs[cls, u_id // num_classes, cls_idx])\n",
    "\n",
    "            # add count of specific class to partition\n",
    "            indices = full_idx[\n",
    "                labels_cs[cls] + hist[cls] : labels_cs[cls] + hist[cls] + count\n",
    "            ]\n",
    "            partitions_idx[u_id].extend(indices)\n",
    "            hist[cls] += count\n",
    "\n",
    "    # construct subsets\n",
    "    partitions = [Subset(sorted_trainset, p) for p in partitions_idx]\n",
    "    return partitions\n",
    "\n",
    "# Distribute the training datasets to clients, remember it returns an array of datasets\n",
    "def split_datasets_for_clients_random(dataset, num_clients=1):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_clients = total_sample_size // num_clients\n",
    "    client_datasets = random_split(dataset, [min(i + samples_per_clients, total_sample_size) - i for i in range(0, total_sample_size, samples_per_clients)])\n",
    "    return client_datasets\n",
    "\n",
    "def split_datasets_for_clients_label(dataset, num_clients=1, labels_per_client=10, sample_amount_decide_func=None):\n",
    "    # Automatically detect the maximum and minimum classes\n",
    "    classes = torch.unique(torch.tensor(dataset.targets))\n",
    "\n",
    "    # Determine labels for each client\n",
    "    max_possible_class_label = classes.max().item()\n",
    "    min_possible_class_label = classes.min().item()\n",
    "    client_filtering_labels = [None] * num_clients\n",
    "    labels_counter = min_possible_class_label\n",
    "    for i in range(num_clients):\n",
    "        client_filtering_labels[i] = (list(range(max(labels_counter, min_possible_class_label), min(labels_counter + labels_per_client, max_possible_class_label))))\n",
    "        labels_counter = labels_counter + labels_per_client\n",
    "        if labels_counter >= max_possible_class_label:\n",
    "            labels_counter = min_possible_class_label\n",
    "\n",
    "    # Determine sample amount for each client\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_client, remaining_samples = sample_amount_decide_func(total_sample_size, num_clients)\n",
    "\n",
    "    # Assign samples to clients\n",
    "    client_datasets = [None] * num_clients\n",
    "    client_counter = 0\n",
    "    temp_dataset = [[None]] * num_clients\n",
    "    temp_dataset_flag = [False] * num_clients\n",
    "    for features, targets in dataset:\n",
    "        for client_counter in range(num_clients):\n",
    "            if targets in client_filtering_labels[client_counter]:\n",
    "                if len(temp_dataset[client_counter]) < samples_per_client:\n",
    "                    temp_dataset[client_counter].append(features)\n",
    "                if len(temp_dataset[client_counter]) >= samples_per_client and temp_dataset_flag[client_counter] == False:\n",
    "                    temp_dataset_flag[client_counter] = True\n",
    "                    client_datasets.append(temp_dataset[client_counter])\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "global custom_split_dataset_iid\n",
    "global custom_split_dataset_power_law\n",
    "global custom_split_dataset_balance\n",
    "global custom_split_dataset_seed\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "def split_datasets_for_clients_custom(dataset, num_clients=1):\n",
    "    # Custom Parameters\n",
    "    iid=custom_split_dataset_iid\n",
    "    power_law=custom_split_dataset_power_law\n",
    "    balance=custom_split_dataset_balance\n",
    "    seed=custom_split_dataset_seed\n",
    "\n",
    "    trainset = dataset\n",
    "    if balance:\n",
    "        trainset = dataset_balance_classes(trainset, seed)\n",
    "\n",
    "    partition_size = int(len(trainset) / num_clients)\n",
    "    lengths = [partition_size] * num_clients\n",
    "\n",
    "    if iid is True:\n",
    "        client_datasets = random_split(trainset, lengths, torch.Generator().manual_seed(seed))\n",
    "    else:\n",
    "        if power_law is True:\n",
    "            trainset_sorted = dataset_sort_by_class(trainset)\n",
    "            client_datasets = dataset_power_law_split(\n",
    "                trainset_sorted,\n",
    "                num_partitions=num_clients,\n",
    "            )\n",
    "        else:\n",
    "            shard_size = int(partition_size / 2)\n",
    "            idxs = trainset.targets.argsort()\n",
    "            sorted_data = Subset(trainset, idxs)\n",
    "            tmp = []\n",
    "            for idx in range(num_clients * 2):\n",
    "                tmp.append(\n",
    "                    Subset(\n",
    "                        sorted_data, np.arange(shard_size * idx, shard_size * (idx + 1))\n",
    "                    )\n",
    "                )\n",
    "            idxs_list = torch.randperm(\n",
    "                num_clients * 2, generator=torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            client_datasets = [\n",
    "                ConcatDataset((tmp[idxs_list[2 * i]], tmp[idxs_list[2 * i + 1]]))\n",
    "                for i in range(num_clients)\n",
    "            ]\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "############################\n",
    "#      Client Devices      #\n",
    "############################\n",
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, client_id, model, optimizer, dataset, batch_size, iterate_func, loss_func, accuracy_func=get_accuracy, error_func=get_error, straggler_bool=False, client_controls=[]):\n",
    "        self.id = client_id\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.iterate_func = iterate_func\n",
    "        self.loss_func = loss_func\n",
    "        self.accuracy_func = accuracy_func\n",
    "        self.error_func = error_func\n",
    "        self.straggler = straggler_bool\n",
    "        self.client_controls = client_controls\n",
    "\n",
    "    def load_weights(self, weights):\n",
    "        self.model.load_state_dict(weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def get_local_client_controls(self):\n",
    "        return self.client_controls\n",
    "\n",
    "    def update_client_controls(self, client_controls_update):\n",
    "        self.client_controls = client_controls_update\n",
    "\n",
    "    def get_client_id(self):\n",
    "        return self.id\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def is_straggler(self):\n",
    "        return self.straggler\n",
    "\n",
    "    def save_local_history(self, num_epochs, loss_history, accuracy_history, error_history, time_history, value=train_start_time):\n",
    "        filename = \"{}_client_{}_with_local_epochs_{}_local_loss_accuracy_error_history_{}.npy\".format(current_dataset_name, self.id, num_epochs, value)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, loss_history=loss_history, accuracy_history=accuracy_history, error_history=error_history, time_history=time_history)\n",
    "\n",
    "    def train(self, num_epochs, show_history=False):\n",
    "        if show_history:\n",
    "            print(f\"!-- Client {self.id} start iterations. ---!\")\n",
    "        loss_history, accuracy_history, error_history, time_history = self.iterate_func(self.model, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func)\n",
    "        if show_history:\n",
    "            plot_time_history(time_history)\n",
    "            plot_loss_history(loss_history)\n",
    "            plot_accuracy_history(accuracy_history)\n",
    "            plot_error_history(error_history)\n",
    "            print(f\"!-- Client {self.id} finish iterations. ---!\")\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def train_Scaffold(self, global_weights, server_controls, num_epochs, Scaffold_update_controls_use_gradient):\n",
    "        loss_history, accuracy_history, error_history, time_history, client_controls_update = iterate_Scaffold_client(self, global_weights, server_controls, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func, Scaffold_update_controls_use_gradient)\n",
    "        return self.model.state_dict(), client_controls_update\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, model_list, optimizer_list, dataset_list, batch_size_list, iterate_func_list, loss_func_list, accuracy_func_list, error_func_list):\n",
    "    client_device = [None] * num_clients\n",
    "    for client_id in range(num_clients):\n",
    "        client_device[client_id] = ClientDevice(client_id, model_list[client_id], optimizer_list[client_id], dataset_list[client_id], batch_size_list[client_id], iterate_func_list[client_id], loss_func_list[client_id], accuracy_func_list[client_id], error_func_list[client_id])\n",
    "    return client_device\n",
    "\n",
    "#########################################\n",
    "#     Federated Learning Algorithms     #\n",
    "#########################################\n",
    "def federated_averaging(client_weights_total):\n",
    "    subset_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / subset_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / subset_clients\n",
    "\n",
    "    return aggregate_weights\n",
    "\n",
    "def iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    # Initial Loss, Accuracy, Error\n",
    "    train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    train_error_history.append(train_error)\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_accuracy_history.append(test_accuracy)\n",
    "        test_error_history.append(test_error)\n",
    "\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        client_weights_total = []\n",
    "        random_client_list = random.sample(client_list, random_sample_client_number)\n",
    "        for client in random_client_list:\n",
    "            client.load_weights(global_weights)\n",
    "            if client.straggler is True:\n",
    "                client_weights = client.train(num_epochs=random.randint(1, local_epochs))\n",
    "            else:\n",
    "                client_weights = client.train(num_epochs=local_epochs)\n",
    "            client_weights_total.append(client_weights)\n",
    "            send_cost += sum(value.numel() for value in client_weights.values())\n",
    "        global_weights.update(aggregate_func(client_weights_total))\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "#################################\n",
    "#  FedProx Framework Algorithm  #\n",
    "#################################\n",
    "def iterate_model_FedProx(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            optimizer.zero_grad()\n",
    "            proximal_term = 0.0\n",
    "            for w, w_t in zip(model.parameters(), FedProx_global_model.parameters()):\n",
    "                proximal_term += torch.square((w - w_t).norm(2))\n",
    "            loss = loss_func(outputs, labels) + (FedProx_mu / 2) * proximal_term\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "##################################\n",
    "#  SCAFFOLD Framework Algorithm  #\n",
    "##################################\n",
    "# Inspired by https://github.com/ki-ljl/Scaffold-Federated-Learning/blob/main/ScaffoldOptimizer.py\n",
    "# c: server_controls, ci: client_controls\n",
    "class ScaffoldOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=None):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay is not None and weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super(ScaffoldOptimizer, self).__init__(params, defaults)\n",
    "                \n",
    "    def step(self, server_controls, client_controls, return_grad=False):\n",
    "        parameters_grad_list = []\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            for parameters, c, ci in zip(group['params'], server_controls.values(), client_controls.values()):\n",
    "                if parameters.grad is None and return_grad is True:\n",
    "                    parameters_grad_list.append[parameters.grad.data]\n",
    "                parameters_derivative = parameters.grad.data - ci.data + c.data\n",
    "                parameters.data = parameters.data - lr * parameters_derivative\n",
    "                if weight_decay is not None:\n",
    "                    parameters.data = weight_decay * parameters.data - lr * parameters_derivative\n",
    "                else:\n",
    "                    parameters.data = parameters.data - lr * parameters_derivative\n",
    "        if return_grad is True:\n",
    "            return parameters_grad_list\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_step_size(self):\n",
    "        print(self.param_groups['lr'])\n",
    "        return self.param_groups['lr']\n",
    "\n",
    "def federated_aggregation_Scaffold(client_weights_total, client_controls_total):\n",
    "    subset_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / subset_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / subset_clients\n",
    "\n",
    "    # Aggregate client controls\n",
    "    aggregate_server_controls = torch.stack(client_controls_total).mean().item()\n",
    "\n",
    "    return aggregate_weights, aggregate_server_controls\n",
    "\n",
    "def iterate_Scaffold_client(client, global_weights, server_controls, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, Scaffold_update_controls_use_gradient=True, show_history=True):\n",
    "    model = client.model\n",
    "    client_controls = client.client_controls\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            gradient = optimizer.step(server_controls, client_controls, Scaffold_update_controls_use_gradient)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    \n",
    "    if Scaffold_update_controls_use_gradient is True:\n",
    "        client_controls_update = gradient\n",
    "    else:\n",
    "        client_controls_update = client_controls - server_controls + (global_weights() - client.get_local_weights()) / (num_epochs * optimizer.get_step_size())\n",
    "\n",
    "    return loss_history, accuracy_history, error_history, client_controls_update\n",
    "\n",
    "def iterate_Scaffold_global(train_dataloader, test_dataloader, global_model, server_controls, client_list, random_sample_client_number, global_epochs, global_step_size, local_epochs, Scaffold_update_controls_use_gradient=False, aggregate_func=federated_aggregation_Scaffold, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    # Initial Loss, Accuracy, Error\n",
    "    train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    train_error_history.append(train_error)\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_accuracy_history.append(test_accuracy)\n",
    "        test_error_history.append(test_error)\n",
    "\n",
    "    num_clients = len(client_list)\n",
    "    server_controls = 0.00\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        client_weights_total = []\n",
    "        client_controls_total = []\n",
    "        random_client_list = random.sample(client_list, random_sample_client_number)\n",
    "        for client in random_client_list:\n",
    "            client.load_weights(global_weights)\n",
    "            if client.straggler is True:\n",
    "                client_weights, client_controls_update = client.train_Scaffold(global_model.parameters(), server_controls, random.randint(1, local_epochs), Scaffold_update_controls_use_gradient)\n",
    "            else:\n",
    "                client_weights, client_controls_update = client.train_Scaffold(global_model.parameters(), server_controls, local_epochs, Scaffold_update_controls_use_gradient)\n",
    "            delta_weights = client_weights - global_weights\n",
    "            delta_client_controls = client_controls_update - client.client_controls\n",
    "            client_weights_total.append(delta_weights)\n",
    "            client_controls_total.append(delta_client_controls)\n",
    "            send_cost += sum(value.numel() for value in delta_weights.values())\n",
    "            send_cost += sum(value.numel() for value in delta_client_controls.values())\n",
    "            client.update_client_controls(client_controls_update)\n",
    "        aggregated_weight, aggregated_client_controls = aggregate_func(client_weights_total, client_controls_total)\n",
    "        global_weights.update( global_weights + global_step_size * aggregated_weight )\n",
    "        server_controls = server_controls + random_sample_client_number / num_clients * aggregated_client_controls\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#    Training Algorithm    #\n",
    "############################\n",
    "def train_neural_network_model(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True):\n",
    "    if test_dataloader is not None:\n",
    "        train_loss_history, train_accuracy_history, train_error_history, time_history, test_loss_history, test_accuracy_history, test_error_history = iterate_model_simple(model, train_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, test_dataloader, True)\n",
    "    else:\n",
    "        train_loss_history, train_accuracy_history, train_error_history, time_history = iterate_model_simple(model, train_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, True)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_train_NN_with_num_epochs_{}_batch_size_{}_lr_{}_{}_{}.npy\".format(current_dataset_name, num_epochs, batch_size, learning_rate, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "        torch.save(model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True):\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}_{}.npy\".format(current_dataset_name, current_method_name, global_epochs, local_epochs, len(client_list), batch_size, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "        torch.save(global_model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_Scaffold_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, global_step_size, Scaffold_update_controls_use_gradient, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True):\n",
    "    server_controls = 0.00\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_Scaffold_global(train_dataloader, test_dataloader, global_model, server_controls, client_list, random_sample_client_number, global_epochs, global_step_size, local_epochs, Scaffold_update_controls_use_gradient, aggregate_func, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}_{}.npy\".format(current_dataset_name, current_method_name, global_epochs, local_epochs, len(client_list), batch_size, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history)\n",
    "        torch.save(global_model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#   Experiment Functions   #\n",
    "############################\n",
    "def experiment_neural_network_model(train_dataset, test_dataset, modelClass, optimizerClass, train_func, epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    epochs_list = convert_to_list(epochs_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "\n",
    "    local_epochs_list_size = len(epochs_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = error_func_list\n",
    "    else:\n",
    "        iteration_list = epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        experiment_id = experiment_id + 1\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            num_epochs = epochs_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for num_epochs is {epochs_list[n]} ===')\n",
    "            num_epochs = epochs_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        \n",
    "        model = modelClass()\n",
    "        print(model)\n",
    "        print(model.state_dict())\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = optimizerClass(model.parameters(), learning_rate)\n",
    "\n",
    "        time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_func(model, train_dataloader, test_dataloader, num_epochs, optimizer, learning_rate, batch_size, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "\n",
    "    global_epochs_list_size = len(global_epochs_list)\n",
    "    local_epochs_list_size = len(local_epochs_list)\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    random_sample_client_number_list_size = len(random_sample_client_number_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    aggregate_func_list_size = len(aggregate_func_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = local_epochs_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = num_clients_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list = random_sample_client_number_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 7:\n",
    "        iteration_list = aggregate_func_list\n",
    "    elif compare_id == 8:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 9:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 10:\n",
    "        iteration_list = error_func_list\n",
    "    else:\n",
    "        iteration_list = global_epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        experiment_id = experiment_id + 1\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for local_epochs is {local_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[n]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for random_sample_client_number is {random_sample_client_number_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 7:\n",
    "            print(f'=== The training for aggregate function is {aggregate_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 8:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 9:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "        elif compare_id == 10:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for global_epochs is {global_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[n]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "        \n",
    "        global_model = modelClass()\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [modelClass()] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "    straggler_list = convert_to_list(straggler_list)\n",
    "    mu_list = convert_to_list(mu_list)\n",
    "\n",
    "    global_epochs_list_size = len(global_epochs_list)\n",
    "    local_epochs_list_size = len(local_epochs_list)\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    random_sample_client_number_list_size = len(random_sample_client_number_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    straggler_list_size = convert_to_list(straggler_list)\n",
    "    aggregate_func_list_size = len(aggregate_func_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "    straggler_list_size = len(straggler_list)\n",
    "    mu_func_list_size = len(mu_list)\n",
    "\n",
    "    global FedProx_global_model\n",
    "    global FedProx_mu\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = local_epochs_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = num_clients_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list == random_sample_client_number_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 7:\n",
    "        iteration_list = aggregate_func_list\n",
    "    elif compare_id == 8:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 9:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 10:\n",
    "        iteration_list = error_func_list\n",
    "    elif compare_id == 11:\n",
    "        iteration_list = straggler_list\n",
    "    elif compare_id == 12:\n",
    "        iteration_list = mu_list\n",
    "    else:\n",
    "        iteration_list = global_epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        experiment_id = experiment_id + 1\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for local_epochs is {local_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[n]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for random_sample_client_number is {random_sample_client_number_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 7:\n",
    "            print(f'=== The training for aggregate function is {aggregate_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 8:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 9:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 10:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 11:\n",
    "            print(f'=== The training for straggler_list is {straggler_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[n]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 12:\n",
    "            print(f'=== The training for mu is {mu_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for global_epochs is {global_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[n]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        \n",
    "        global_model = modelClass()\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "        FedProx_global_model = global_model\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [modelClass()] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        for i in range(straggler):\n",
    "            client_list[i].straggler = True\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id = 0, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "    straggler_list = convert_to_list(straggler_list)\n",
    "    mu_list = convert_to_list(mu_list)\n",
    "\n",
    "    global_epochs_list_size = len(global_epochs_list)\n",
    "    local_epochs_list_size = len(local_epochs_list)\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    random_sample_client_number_list_size = len(random_sample_client_number_list)\n",
    "    learning_rate_list_size = len(learning_rate_list)\n",
    "    batch_size_list_size = len(batch_size_list)\n",
    "    straggler_list_size = convert_to_list(straggler_list)\n",
    "    aggregate_func_list_size = len(aggregate_func_list)\n",
    "    loss_func_list_size = len(loss_func_list)\n",
    "    accuracy_func_list_size = len(accuracy_func_list)\n",
    "    error_func_list_size = len(error_func_list)\n",
    "    straggler_list_size = len(straggler_list)\n",
    "    mu_func_list_size = len(mu_list)\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    if compare_id == 2:\n",
    "        iteration_list = local_epochs_list\n",
    "    elif compare_id == 3:\n",
    "        iteration_list = num_clients_list\n",
    "    elif compare_id == 4:\n",
    "        iteration_list == random_sample_client_number_list\n",
    "    elif compare_id == 5:\n",
    "        iteration_list = learning_rate_list\n",
    "    elif compare_id == 6:\n",
    "        iteration_list = batch_size_list\n",
    "    elif compare_id == 7:\n",
    "        iteration_list = aggregate_func_list\n",
    "    elif compare_id == 8:\n",
    "        iteration_list = loss_func_list\n",
    "    elif compare_id == 9:\n",
    "        iteration_list = accuracy_func_list\n",
    "    elif compare_id == 10:\n",
    "        iteration_list = error_func_list\n",
    "    elif compare_id == 11:\n",
    "        iteration_list = straggler_list\n",
    "    elif compare_id == 12:\n",
    "        iteration_list = mu_list\n",
    "    else:\n",
    "        iteration_list = global_epochs_list\n",
    "    \n",
    "    for n in range(len(iteration_list)):\n",
    "        experiment_id = experiment_id + 1\n",
    "        if compare_id == 2:\n",
    "            print(f'=== The training for local_epochs is {local_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 3:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[n]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 4:\n",
    "            print(f'=== The training for random_sample_client_number is {random_sample_client_number_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[n]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 5:\n",
    "            print(f'=== The training for learning_rate_list is {learning_rate_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[n]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 6:\n",
    "            print(f'=== The training for batch_size_list is {batch_size_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[n]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 7:\n",
    "            print(f'=== The training for aggregate function is {aggregate_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[n]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 8:\n",
    "            print(f'=== The training for loss function is {loss_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[n]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 9:\n",
    "            print(f'=== The training for accuracy function is {accuracy_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[n]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 10:\n",
    "            print(f'=== The training for error function is {error_func_list[n].__name__} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[n]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 11:\n",
    "            print(f'=== The training for straggler_list is {straggler_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[n]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        elif compare_id == 12:\n",
    "            print(f'=== The training for mu is {mu_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[0]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[n]\n",
    "        else:\n",
    "            print(f'=== The training for global_epochs is {global_epochs_list[n]} ===')\n",
    "            global_epochs = global_epochs_list[n]\n",
    "            local_epochs = local_epochs_list[0]\n",
    "            num_clients = num_clients_list[0]\n",
    "            random_sample_client_number = random_sample_client_number_list[0]\n",
    "            learning_rate = learning_rate_list[0]\n",
    "            batch_size = batch_size_list[0]\n",
    "            aggregate_func = aggregate_func_list[0]\n",
    "            loss_func = loss_func_list[0]\n",
    "            accuracy_func = accuracy_func_list[0]\n",
    "            error_func = error_func_list[0]\n",
    "            straggler = straggler_list[0]\n",
    "            FedProx_mu = mu_list[0]\n",
    "        \n",
    "        global_step_size = 0.03\n",
    "        Scaffold_update_controls_use_gradient = False\n",
    "\n",
    "        global_model = modelClass()\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [modelClass()] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        for i in range(straggler):\n",
    "            client_list[i].straggler = True\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_Scaffold_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, global_step_size, Scaffold_update_controls_use_gradient, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Experiment ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Choosing Dataset ###\n",
    "\n",
    "In this section, execute a cell only to choose a dataset you want do experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n",
      "Number of samples in the train dataset after random split: 60000\n",
      "Number of samples in the test dataset after random split: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.1\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.1\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 784\n",
    "model_type_preset = MNIST_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.50\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.007\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 1024\n",
    "model_type_preset = CIFAR10_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.01\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.01\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "Linear_Model_in_features = 7\n",
    "Linear_Model_out_features = 1\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.50\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.007\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Neural Network Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare number of epochs\n",
    "\n",
    "2: compare learning rate\n",
    "\n",
    "3: compare batch size\n",
    "\n",
    "4: compare loss function\n",
    "\n",
    "5: compare accuracy function\n",
    "\n",
    "6: compare error function\n",
    "\n",
    "Others: compare number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "#Linear_Model_in_features = 28\n",
    "#Linear_Model_out_features = 10\n",
    "\n",
    "num_epochs_list = 100\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "num_epochs_list = 25\n",
    "learning_rate_list = 0.02\n",
    "batch_size_list = [10, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3 FedAvg Federated Learning Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare global epochs\n",
    "\n",
    "2: compare local epochs\n",
    "\n",
    "3: compare number of clients\n",
    "\n",
    "4: compare random sample client number\n",
    "\n",
    "5: compare learning rate\n",
    "\n",
    "6: compare batch size\n",
    "\n",
    "7: compare the aggregate weight algorithms\n",
    "\n",
    "8: compare loss function\n",
    "\n",
    "9: compare accuracy function\n",
    "\n",
    "10: compare error function\n",
    "\n",
    "Others: compare global epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = [10]\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Number of Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 5\n",
    "local_epochs_list = 3\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Random Client Number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 5\n",
    "local_epochs_list = 3\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 5\n",
    "local_epochs_list = 3\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 5\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = torch.optim.Adam\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "compare_id = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4 FedProx Federated Learning Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare global epochs\n",
    "\n",
    "2: compare local epochs\n",
    "\n",
    "3: compare number of clients\n",
    "\n",
    "4: compare random sample client number\n",
    "\n",
    "5: compare learning rate\n",
    "\n",
    "6: compare batch size\n",
    "\n",
    "7: compare the aggregate weight algorithms\n",
    "\n",
    "8: compare loss function\n",
    "\n",
    "9: compare accuracy function\n",
    "\n",
    "10: compare error function\n",
    "\n",
    "11: compare straggler\n",
    "\n",
    "12: compare mu value\n",
    "\n",
    "Others: compare global epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "The current train start time is 2024-02-15 23.15.22.\n",
      "=== The training for mu is 0.0 ===\n",
      "MNIST_CNN_Model(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 1.8038e-01, -1.5922e-04,  1.9409e-01, -8.2619e-02, -3.7567e-02],\n",
      "          [ 2.1436e-02,  1.8818e-01,  1.9091e-01, -1.1706e-01,  7.9470e-02],\n",
      "          [ 1.4326e-01,  1.2877e-01, -5.5729e-02, -6.0793e-02,  1.1638e-01],\n",
      "          [-1.2195e-01,  1.2008e-01,  1.7013e-01,  1.7253e-01, -2.8120e-02],\n",
      "          [-6.1758e-02, -1.9153e-01, -1.8048e-01,  8.7374e-02, -6.1740e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.2291e-02,  1.4931e-01, -2.2393e-02, -1.0290e-01, -3.0850e-02],\n",
      "          [-1.0473e-01, -1.9686e-01,  5.0817e-02, -2.9061e-02,  1.5648e-01],\n",
      "          [-1.4514e-01,  7.6719e-02,  3.1436e-03, -1.9365e-01,  1.6701e-01],\n",
      "          [ 1.5487e-01,  9.7205e-02, -8.8352e-02,  5.1332e-02,  1.9980e-01],\n",
      "          [-1.1943e-01, -6.7858e-02, -1.3992e-02,  1.2078e-02, -1.8491e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3760e-02,  1.5937e-01,  1.4825e-02, -1.9901e-02, -1.9000e-01],\n",
      "          [-5.2361e-02, -1.8000e-02,  2.0236e-02,  1.0515e-01, -1.2207e-01],\n",
      "          [-1.6979e-01,  1.9066e-01, -1.1097e-01,  1.8430e-01,  4.2129e-02],\n",
      "          [-4.2260e-03, -8.3385e-03,  1.0475e-01, -1.5722e-01,  1.2505e-01],\n",
      "          [-1.1020e-01,  1.3754e-01, -1.9031e-01,  9.2390e-02, -1.3245e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.9221e-03, -8.3324e-02,  1.1348e-02,  5.8293e-02,  4.3332e-02],\n",
      "          [ 3.6231e-02, -1.9584e-01,  8.1789e-02, -1.3196e-01,  8.8469e-02],\n",
      "          [ 3.5345e-02, -1.8789e-01, -1.8676e-01, -1.2006e-02,  5.3774e-02],\n",
      "          [-1.6876e-01, -1.0913e-01,  1.3508e-01, -6.2807e-02, -9.6457e-02],\n",
      "          [-1.6214e-01, -9.6943e-02, -1.7455e-01, -3.7766e-02,  1.5206e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7786e-02, -9.7133e-02,  6.0283e-02,  8.5763e-02, -8.8038e-02],\n",
      "          [-4.7710e-02, -1.9013e-01, -6.8001e-02, -1.4931e-01,  3.9845e-02],\n",
      "          [ 1.5262e-01,  2.8846e-02,  4.5253e-03,  1.8927e-02, -1.0275e-01],\n",
      "          [-3.0789e-03, -1.4615e-01, -8.7090e-02,  6.0188e-02,  8.2825e-02],\n",
      "          [-6.6160e-02, -2.7762e-02, -4.6247e-02, -8.6928e-02,  7.3148e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3985e-01, -7.7228e-02,  1.0743e-01,  1.2736e-01,  1.6386e-02],\n",
      "          [ 2.0727e-02, -1.5609e-01, -1.7798e-01, -2.3411e-02,  1.3214e-01],\n",
      "          [-1.6762e-01, -2.6819e-02,  9.5826e-03, -9.9059e-02,  1.4701e-01],\n",
      "          [ 2.7255e-02,  1.2596e-01, -1.7301e-01,  1.4238e-01, -1.2869e-01],\n",
      "          [ 1.7594e-01,  6.4243e-02,  1.9005e-01,  8.5048e-02, -1.5486e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.8502e-01,  1.9511e-01, -6.9809e-03,  8.3416e-02,  2.0629e-02],\n",
      "          [ 1.9709e-02,  1.1568e-01, -1.4870e-01,  1.2924e-01,  3.1244e-02],\n",
      "          [-1.7245e-01, -1.7397e-01, -1.3037e-01,  1.9141e-01, -1.6643e-01],\n",
      "          [-7.4956e-02, -1.3235e-01, -1.6442e-01, -1.9216e-01, -5.4542e-02],\n",
      "          [ 1.2476e-01, -1.8598e-01, -3.0859e-02, -1.1549e-01, -1.1682e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1933e-01,  2.6910e-02, -9.0125e-04, -6.0605e-03, -1.7943e-01],\n",
      "          [ 1.1960e-01,  4.4360e-02,  1.4255e-01, -7.6048e-02, -1.6254e-02],\n",
      "          [-1.2754e-01,  1.6873e-01, -2.0551e-02,  5.7354e-02,  1.9283e-01],\n",
      "          [ 8.8940e-02, -9.2665e-02, -6.4856e-02, -1.7242e-01, -1.5298e-01],\n",
      "          [-3.9777e-02, -1.0607e-01, -1.6440e-01,  1.8730e-01,  6.1805e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2170e-02, -1.0559e-01,  7.4540e-02,  5.2136e-02, -1.4151e-01],\n",
      "          [ 1.1917e-01,  1.6641e-01,  1.5217e-01, -6.6632e-02,  1.7849e-02],\n",
      "          [-1.9152e-01,  5.8300e-02, -9.5230e-03, -1.9932e-01, -1.5167e-01],\n",
      "          [-6.0384e-02,  9.0220e-02, -7.7185e-02, -6.6968e-02,  1.6927e-01],\n",
      "          [ 9.6448e-02, -5.2825e-02,  8.4166e-02,  7.5430e-02, -3.8929e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.8980e-02,  1.0681e-01, -4.9178e-03, -3.4542e-02, -1.7396e-01],\n",
      "          [-8.8748e-02,  1.5742e-01, -1.8850e-02,  2.2132e-02,  1.4106e-01],\n",
      "          [ 1.2093e-01,  7.5225e-02,  7.5643e-02,  5.3191e-02,  1.7329e-01],\n",
      "          [-7.1741e-02,  1.6919e-02, -1.0397e-01, -3.9707e-02,  1.9572e-01],\n",
      "          [-4.0941e-02, -7.8183e-02,  1.8406e-04,  8.0704e-02,  3.3267e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4475e-01,  5.0174e-02,  6.0319e-02,  1.9026e-01, -5.0468e-02],\n",
      "          [-1.8871e-01,  7.7060e-02,  9.0986e-02,  3.4214e-03, -5.7784e-02],\n",
      "          [ 2.6937e-02, -8.4011e-02,  1.3706e-01,  1.8425e-02, -1.1482e-01],\n",
      "          [ 1.6936e-01,  1.4111e-01,  1.5373e-01,  1.3983e-01, -2.8065e-02],\n",
      "          [ 1.4188e-01, -3.4228e-02, -1.7818e-01,  1.0852e-01, -2.8174e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0185e-01, -2.2855e-02, -3.8457e-02, -1.2857e-01,  2.2808e-02],\n",
      "          [ 1.8552e-02,  1.4567e-04, -1.8078e-01,  1.6320e-02, -5.7013e-02],\n",
      "          [ 6.0322e-02,  9.0654e-02,  1.5041e-01,  1.9526e-01,  9.3254e-02],\n",
      "          [ 7.0918e-02,  1.6267e-01,  1.4498e-01, -8.0013e-02, -1.2664e-01],\n",
      "          [ 8.2570e-02, -4.7533e-02, -7.9232e-02, -1.8956e-01, -6.5584e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6703e-01, -1.3079e-01, -9.9330e-02,  1.6916e-01, -1.1708e-01],\n",
      "          [ 7.3192e-02, -1.8762e-01, -1.0874e-01,  7.8542e-02, -1.8988e-01],\n",
      "          [-1.4573e-01,  6.0509e-02,  1.1075e-01, -1.8213e-01, -8.3527e-02],\n",
      "          [ 1.0465e-01,  3.9323e-02, -7.0870e-02,  3.0796e-02, -1.6627e-01],\n",
      "          [ 1.5537e-02,  1.6077e-01,  1.1841e-01,  8.5358e-02, -9.2954e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8531e-01,  1.8978e-01, -4.8627e-03, -1.9003e-01,  1.8216e-01],\n",
      "          [ 1.2475e-01, -1.2591e-01, -6.0559e-02,  1.5789e-01,  1.6656e-02],\n",
      "          [-7.0547e-02,  1.4861e-01,  1.5404e-01, -1.7822e-01, -2.3419e-02],\n",
      "          [ 1.0728e-01, -1.2236e-02, -1.4534e-02, -2.2590e-02, -9.2131e-02],\n",
      "          [-1.4493e-01, -2.2632e-02,  8.3546e-03, -3.7107e-02,  1.1450e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8356e-02,  2.8698e-02, -8.6098e-02, -5.0492e-03,  1.3634e-01],\n",
      "          [ 1.7747e-01,  8.5184e-02,  1.1577e-01,  1.5489e-01, -9.4433e-02],\n",
      "          [-1.6997e-01,  4.6429e-02,  1.1921e-01, -1.2522e-01, -2.5668e-02],\n",
      "          [ 5.7279e-02, -7.2096e-02, -3.3064e-02,  7.9392e-02, -1.8841e-01],\n",
      "          [ 3.8641e-02,  1.8018e-01, -1.3873e-01,  3.2417e-02,  1.8028e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4483e-01, -3.1446e-02,  2.9602e-02,  1.7155e-01, -8.5803e-02],\n",
      "          [-8.9051e-02,  1.6623e-01,  8.5789e-02,  1.4838e-01,  6.5029e-03],\n",
      "          [-1.2797e-01,  9.1360e-03,  3.2054e-02, -9.6577e-02, -9.9068e-02],\n",
      "          [-1.5628e-01,  1.4751e-01,  1.1814e-01, -8.1168e-02,  6.6908e-02],\n",
      "          [-8.6841e-02,  1.9945e-02,  1.8706e-01, -1.1220e-01, -1.4003e-01]]]])), ('conv1.bias', tensor([ 0.0571,  0.1683, -0.1802, -0.1485,  0.1026, -0.0875, -0.1281, -0.0152,\n",
      "         0.0295,  0.0562,  0.0854, -0.1906, -0.1071, -0.0226, -0.1122,  0.0041])), ('conv2.weight', tensor([[[[-3.5910e-02,  3.0534e-02,  4.8636e-02, -4.3648e-02,  1.8160e-02],\n",
      "          [-2.3399e-02, -2.6147e-02,  3.5509e-02, -1.6275e-02, -2.1462e-02],\n",
      "          [ 2.1706e-02,  6.2501e-03,  4.3086e-02,  4.1169e-02,  3.2524e-02],\n",
      "          [ 4.4487e-02,  2.8482e-02, -1.4695e-02,  4.9859e-02,  2.3561e-02],\n",
      "          [-7.8071e-03,  4.6940e-02, -4.8784e-02,  1.2392e-02,  4.3127e-02]],\n",
      "\n",
      "         [[-4.1363e-02, -2.4184e-02, -6.4900e-03, -1.0742e-02,  1.9089e-02],\n",
      "          [-4.6796e-02, -4.0570e-02, -3.4782e-02, -7.3408e-03, -2.0310e-02],\n",
      "          [ 4.8511e-02, -6.0495e-03,  8.8055e-03,  4.9752e-02, -1.2743e-02],\n",
      "          [-4.7130e-02, -3.7505e-02, -4.1650e-02,  1.0166e-02,  4.9010e-02],\n",
      "          [ 1.1023e-02,  1.8167e-02,  2.8349e-02,  4.1359e-02,  4.2507e-02]],\n",
      "\n",
      "         [[ 2.9190e-02, -2.5950e-02,  1.8885e-02, -1.2423e-02, -4.8654e-02],\n",
      "          [ 1.3909e-02, -4.1620e-02,  3.1255e-02, -1.6362e-02, -2.7990e-02],\n",
      "          [-1.3403e-02,  3.7358e-02, -1.2245e-02, -7.5996e-03, -4.9293e-02],\n",
      "          [ 4.4852e-02,  2.8063e-02,  3.7054e-02, -5.1051e-03, -3.7782e-03],\n",
      "          [-4.0837e-02,  4.9018e-02, -1.8318e-02,  1.8625e-02, -2.1390e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5584e-02, -4.9567e-02,  1.1955e-02, -2.4140e-03, -1.1040e-02],\n",
      "          [-6.9251e-03, -3.2088e-02, -5.6775e-03,  4.3912e-03, -4.2148e-02],\n",
      "          [ 2.2275e-02,  2.8328e-02,  2.5029e-02,  2.4970e-02, -6.7503e-03],\n",
      "          [ 1.9416e-02,  4.2848e-02,  4.5556e-02, -4.5051e-02,  3.6863e-02],\n",
      "          [-4.2145e-02,  4.7216e-02, -4.8984e-02,  4.8822e-02,  9.7396e-03]],\n",
      "\n",
      "         [[-4.8898e-05, -3.1401e-02, -2.3636e-02, -2.3542e-02,  2.5198e-02],\n",
      "          [ 3.2531e-02, -4.4393e-02, -6.2231e-03, -3.3424e-02, -7.8270e-03],\n",
      "          [-2.4680e-02, -1.2227e-02,  4.8775e-03,  2.3304e-02,  4.2917e-02],\n",
      "          [ 3.0186e-02, -3.2998e-02,  4.1710e-02,  5.9085e-03,  1.5766e-02],\n",
      "          [ 1.4193e-02,  2.7758e-02,  3.8114e-02,  3.1112e-02,  2.1194e-02]],\n",
      "\n",
      "         [[ 1.3626e-02,  2.6300e-02, -4.3783e-02, -2.0036e-02,  4.0939e-03],\n",
      "          [-9.2207e-03, -3.0781e-02,  1.6806e-03,  1.6918e-03, -1.4517e-03],\n",
      "          [ 1.3393e-02,  2.9543e-02, -2.0353e-02,  3.8269e-02,  2.4504e-02],\n",
      "          [-2.7239e-02, -4.1333e-02,  4.9146e-02, -1.7929e-02,  4.2280e-02],\n",
      "          [-1.5550e-02, -3.1655e-02,  3.4171e-02, -2.5685e-02, -1.6859e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.6520e-02,  1.2203e-02,  2.2692e-02,  3.6146e-02, -1.2546e-02],\n",
      "          [-3.2490e-02, -4.8113e-02, -3.5041e-02,  1.1590e-02,  3.9679e-02],\n",
      "          [ 1.4455e-02,  2.9198e-02, -1.0000e-02,  4.2591e-02, -1.4382e-03],\n",
      "          [-3.5551e-02, -4.4629e-02,  2.7961e-02,  1.8047e-02,  4.4798e-02],\n",
      "          [-3.8576e-02,  3.5413e-02,  4.9318e-02, -4.3717e-02,  1.9583e-02]],\n",
      "\n",
      "         [[-4.8142e-02,  4.4003e-02,  2.7324e-02, -2.1615e-02, -2.7593e-02],\n",
      "          [ 2.0787e-03, -2.9347e-02, -3.2811e-02,  4.9563e-02,  4.3026e-03],\n",
      "          [ 3.6662e-02,  3.5690e-02, -1.7982e-02, -1.7081e-03,  2.2596e-02],\n",
      "          [ 1.2073e-02, -4.0230e-02, -3.7479e-02,  2.7985e-03, -1.4104e-03],\n",
      "          [ 9.4820e-03,  4.4519e-02,  1.0482e-02, -1.6457e-02,  7.4643e-03]],\n",
      "\n",
      "         [[ 4.7553e-02, -3.1244e-02, -4.0343e-02, -1.7342e-02,  4.3975e-02],\n",
      "          [-1.6804e-02,  3.7999e-02,  2.1861e-02,  4.0479e-02, -1.2646e-02],\n",
      "          [ 2.1610e-02, -4.6669e-02,  1.8193e-02,  1.1041e-02, -2.4150e-02],\n",
      "          [ 2.9337e-02,  3.5171e-02, -1.2427e-02, -3.7845e-02,  5.5889e-03],\n",
      "          [-3.4080e-02,  2.8656e-02, -3.5965e-02,  3.6782e-03, -3.1432e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5164e-02, -4.2815e-02, -4.9137e-02,  2.3983e-03,  2.4616e-02],\n",
      "          [ 4.8304e-02, -3.4338e-02, -1.7248e-03, -4.1598e-02, -1.7901e-03],\n",
      "          [-2.1834e-02,  2.3765e-02,  2.8141e-02, -1.1615e-03, -3.5512e-03],\n",
      "          [-1.9913e-02,  3.1429e-04, -3.0274e-02,  1.0775e-02,  4.4995e-03],\n",
      "          [ 3.5451e-02, -2.0233e-04,  1.7930e-02,  2.0637e-02, -4.2306e-02]],\n",
      "\n",
      "         [[-4.9288e-03, -1.3377e-02,  1.0945e-02, -1.2535e-02, -2.9737e-02],\n",
      "          [-2.8466e-02,  3.0455e-02, -2.5518e-02, -3.9255e-02, -9.7704e-03],\n",
      "          [ 3.4230e-02, -1.4330e-03,  1.9445e-02, -1.9742e-02,  3.0470e-03],\n",
      "          [ 3.4359e-02,  3.0559e-02,  3.6132e-03, -6.5622e-03,  2.9114e-02],\n",
      "          [-2.0752e-02,  2.6409e-02, -3.1149e-02, -8.7566e-03,  4.5916e-02]],\n",
      "\n",
      "         [[ 1.9320e-02, -4.9773e-03,  3.2033e-02, -3.6485e-02, -2.6123e-02],\n",
      "          [-1.2773e-03, -1.0780e-03,  1.5874e-04,  4.0308e-02,  9.1846e-03],\n",
      "          [-3.6303e-02,  1.1885e-03,  3.9800e-02,  4.6014e-02,  7.1036e-03],\n",
      "          [-3.3659e-02, -9.2617e-03, -4.1900e-02,  3.9717e-02, -3.1352e-02],\n",
      "          [ 2.9627e-02, -4.5238e-02,  2.0898e-02,  1.7751e-02,  3.1466e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0476e-02,  4.5256e-02, -3.4173e-02,  2.4581e-02, -3.8561e-02],\n",
      "          [ 8.6842e-03, -3.0662e-02, -4.9171e-02,  4.7831e-02,  8.4894e-03],\n",
      "          [ 9.8544e-03, -3.2376e-02, -3.2440e-02,  3.9724e-02,  3.5415e-02],\n",
      "          [ 2.0317e-02,  2.0392e-02,  4.1565e-02,  1.2029e-02, -2.2930e-02],\n",
      "          [-1.0501e-02,  2.0281e-02,  2.7387e-02,  4.7721e-02,  1.1938e-02]],\n",
      "\n",
      "         [[-3.9038e-02,  3.0162e-03, -1.9409e-02,  4.8599e-02, -5.1925e-03],\n",
      "          [-4.9693e-02,  1.3077e-02, -2.6614e-02, -4.4160e-02,  2.1095e-02],\n",
      "          [-3.0228e-02, -3.1614e-02,  1.6498e-02,  2.7060e-02, -1.9851e-02],\n",
      "          [-2.1307e-02,  1.8410e-02,  3.8008e-02,  3.6264e-02, -3.2761e-02],\n",
      "          [ 4.7979e-02,  3.1791e-02, -1.6455e-02, -3.3873e-02, -4.0026e-02]],\n",
      "\n",
      "         [[ 1.0284e-02,  4.9539e-02, -4.0281e-02,  2.5716e-02, -3.6881e-02],\n",
      "          [-3.2112e-02,  1.0094e-02, -4.5133e-02,  3.1505e-03, -6.0540e-03],\n",
      "          [ 1.1536e-02, -2.7781e-02, -5.6724e-03,  1.7613e-02,  2.6456e-02],\n",
      "          [-3.1380e-02, -3.2349e-02,  3.7168e-02,  4.0656e-02,  3.4732e-02],\n",
      "          [-2.0174e-02,  3.3583e-02,  3.5990e-02, -6.1598e-03, -1.4452e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6745e-02,  3.1463e-02, -6.4538e-03,  8.2267e-03, -3.5072e-02],\n",
      "          [ 9.5714e-03,  1.1719e-02, -2.7597e-02,  1.8436e-02,  4.7483e-02],\n",
      "          [ 4.5321e-04,  1.3200e-02,  1.5837e-02,  4.8928e-02,  6.2757e-03],\n",
      "          [ 2.7138e-02,  3.1086e-02,  3.8701e-02,  3.4018e-02,  1.3227e-02],\n",
      "          [ 3.8055e-02,  3.0285e-02,  2.6070e-03,  9.7043e-03,  2.0191e-02]],\n",
      "\n",
      "         [[ 2.1649e-02,  6.3981e-03,  3.9656e-02, -3.4939e-02,  3.2076e-02],\n",
      "          [-3.9885e-02,  1.5942e-02, -3.5870e-02,  4.5812e-02, -2.0884e-03],\n",
      "          [-2.8895e-02, -5.7137e-03, -3.0946e-02, -3.3992e-02,  2.1603e-02],\n",
      "          [ 3.3511e-02,  4.4728e-02, -4.7527e-02,  3.9003e-04, -7.5775e-03],\n",
      "          [-8.3549e-03,  3.7225e-02,  4.9005e-02,  1.9585e-02,  1.6723e-02]],\n",
      "\n",
      "         [[-2.2404e-02,  3.6865e-02,  5.7465e-03,  3.2881e-02,  3.6123e-02],\n",
      "          [ 2.5866e-02, -4.9946e-02,  2.2169e-03, -1.1824e-02, -1.5307e-02],\n",
      "          [ 2.1588e-02, -3.9647e-02,  1.0152e-02,  3.2584e-02,  5.4870e-03],\n",
      "          [-3.8009e-03, -2.7720e-02,  3.4533e-02, -3.6415e-02,  2.0190e-02],\n",
      "          [-1.3579e-02, -3.2710e-02,  2.7274e-02,  4.8974e-02, -3.1952e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.9588e-02, -1.0678e-02,  4.6608e-02, -2.4549e-02,  4.0075e-02],\n",
      "          [-2.0603e-02, -6.2977e-03,  3.7393e-02, -3.7083e-02, -4.4869e-02],\n",
      "          [-3.6721e-03, -4.7762e-02, -3.3374e-02, -3.6220e-02,  2.6959e-02],\n",
      "          [-3.4861e-02,  1.7154e-02,  2.3511e-02, -9.3801e-03, -1.1064e-02],\n",
      "          [ 2.3493e-02,  5.1859e-03, -1.0094e-02, -1.0001e-02,  2.7764e-02]],\n",
      "\n",
      "         [[ 2.7496e-02, -3.3828e-02,  1.6563e-02, -7.2028e-03, -2.5286e-02],\n",
      "          [-4.0493e-02, -1.8839e-02, -1.3337e-02, -2.6824e-02,  1.2406e-02],\n",
      "          [ 3.8021e-02,  4.3466e-02,  4.6586e-02, -3.2990e-02,  1.5528e-03],\n",
      "          [ 4.5808e-02, -4.8065e-02, -1.4073e-02, -1.6290e-03,  5.2715e-03],\n",
      "          [-2.7335e-02,  1.3338e-02, -4.5104e-02,  4.7311e-02,  1.1643e-02]],\n",
      "\n",
      "         [[ 4.5685e-02, -1.2339e-03,  5.2018e-03, -3.1121e-02,  1.9708e-02],\n",
      "          [ 3.2620e-02,  1.3321e-02, -2.0785e-02, -4.4258e-02,  2.2484e-02],\n",
      "          [-2.7047e-03,  3.8741e-02, -1.5867e-02,  4.5658e-02,  1.7944e-02],\n",
      "          [-2.3262e-02,  1.0042e-02, -2.7618e-02,  3.0282e-02, -4.3067e-02],\n",
      "          [ 4.5361e-02,  3.7149e-03,  2.4372e-02,  5.7605e-03,  4.5757e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5020e-02, -1.8533e-02,  5.9446e-03, -4.4075e-03,  4.7078e-02],\n",
      "          [-1.4386e-02,  2.9081e-02,  5.4435e-04, -3.7012e-02,  2.3502e-02],\n",
      "          [-3.4027e-02, -3.4019e-02, -1.5672e-02,  4.8236e-02, -4.7763e-02],\n",
      "          [ 3.0132e-02, -1.6198e-02, -3.1444e-02,  3.3584e-02,  6.5053e-03],\n",
      "          [ 1.8101e-02,  1.7897e-02, -2.9689e-02,  1.1297e-02,  2.3124e-02]],\n",
      "\n",
      "         [[-5.9589e-04, -7.4524e-03,  4.0795e-02,  2.5535e-02, -4.7958e-02],\n",
      "          [ 4.9417e-02,  4.1293e-03,  2.9349e-02,  1.4530e-02, -4.8310e-02],\n",
      "          [-3.5439e-02, -2.6237e-02, -3.4175e-02, -1.3028e-02,  3.2009e-02],\n",
      "          [ 1.7075e-03,  4.0459e-02, -3.7557e-02, -1.5834e-02, -4.7162e-02],\n",
      "          [ 2.6639e-02,  4.7322e-02, -3.1110e-02,  4.3242e-02, -3.6073e-02]],\n",
      "\n",
      "         [[-4.8296e-02, -1.0620e-02, -4.7372e-02,  1.6253e-02, -4.6212e-02],\n",
      "          [-3.6800e-03,  9.4037e-03,  3.8726e-02,  4.8097e-02,  8.7726e-03],\n",
      "          [-4.7324e-02,  3.7108e-02,  2.6133e-02, -2.2647e-02, -2.6327e-02],\n",
      "          [-4.0081e-02,  3.4197e-02,  1.8286e-02, -5.8737e-03, -1.3004e-02],\n",
      "          [-6.1306e-03, -3.9689e-02,  6.0936e-04,  3.6116e-02, -1.7185e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.0950e-02,  1.3269e-02, -1.3944e-02,  3.5328e-02,  6.7014e-03],\n",
      "          [ 1.4534e-02, -1.2860e-02, -1.4282e-02, -9.7899e-03, -3.8202e-02],\n",
      "          [-4.3999e-02,  2.3108e-02, -5.3192e-03,  3.0510e-02,  3.3612e-02],\n",
      "          [ 9.8641e-03,  9.9008e-03, -4.5622e-02,  3.7135e-02, -4.7003e-02],\n",
      "          [-1.7369e-02,  3.7986e-02,  4.7619e-02,  3.2026e-02, -3.1876e-02]],\n",
      "\n",
      "         [[-1.9826e-05,  2.3097e-02,  2.2514e-02, -5.3807e-04, -2.4263e-02],\n",
      "          [-3.7785e-02,  2.9064e-02,  2.1086e-02,  4.8350e-02, -4.8269e-02],\n",
      "          [ 4.5578e-02, -3.6556e-02, -2.1679e-02, -3.3549e-02, -2.1489e-02],\n",
      "          [ 3.1331e-03, -2.9460e-02,  3.2581e-02,  7.7419e-03,  5.9035e-03],\n",
      "          [ 7.0077e-03,  2.5865e-02,  4.5059e-02, -4.4314e-02, -3.8688e-03]],\n",
      "\n",
      "         [[ 3.3800e-02, -1.1829e-02, -9.1119e-03,  3.4535e-02, -7.8703e-03],\n",
      "          [-4.7902e-02,  4.0857e-02, -4.8497e-02,  3.2940e-02,  1.6887e-02],\n",
      "          [ 1.2128e-02, -2.6371e-03,  2.6040e-03,  3.4353e-02, -7.5450e-03],\n",
      "          [ 2.0880e-02, -4.6186e-02, -6.4886e-03,  4.8194e-02, -8.4782e-03],\n",
      "          [-4.9131e-02, -2.5032e-02,  1.9920e-02, -3.7823e-02,  4.0723e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0198e-02,  1.8399e-02, -9.4222e-03, -3.6509e-02, -2.9659e-02],\n",
      "          [-4.9814e-02, -2.1917e-02,  1.1595e-02,  4.0089e-02, -1.0351e-02],\n",
      "          [-3.6653e-02,  2.2463e-02, -7.4201e-03,  2.8910e-02, -2.7402e-02],\n",
      "          [ 3.4699e-02, -1.9227e-02,  1.4057e-02, -3.4325e-02,  1.2836e-02],\n",
      "          [ 4.2494e-02, -3.9199e-02,  2.3837e-02,  3.2033e-02,  4.3504e-02]],\n",
      "\n",
      "         [[-4.2982e-02, -1.2479e-02,  8.7113e-03,  4.9614e-02, -3.5353e-02],\n",
      "          [-4.6442e-02, -3.0617e-02,  1.6135e-02,  7.3901e-03,  2.0435e-02],\n",
      "          [ 1.5351e-02, -3.1823e-02,  6.6224e-03,  2.6403e-03,  2.3940e-02],\n",
      "          [-4.5068e-02,  4.4879e-02, -2.3932e-02, -1.7160e-02,  3.9785e-02],\n",
      "          [-2.0017e-02, -2.6733e-02, -3.0626e-02, -1.4140e-02, -1.2528e-02]],\n",
      "\n",
      "         [[-4.0878e-02,  2.8498e-02, -4.0937e-02, -2.2134e-02,  1.1974e-04],\n",
      "          [-1.6744e-02, -2.7626e-02,  4.4396e-02,  2.3846e-03,  1.2294e-02],\n",
      "          [ 6.8816e-03, -4.4952e-02, -2.7649e-02, -2.9669e-02,  1.1250e-02],\n",
      "          [-4.8387e-02,  2.1519e-02, -5.2382e-03,  1.7249e-03,  4.1930e-02],\n",
      "          [ 4.5391e-02, -1.9622e-02,  8.7832e-04,  1.8772e-02,  2.5550e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1387e-02,  3.9130e-02, -4.5902e-02,  2.8616e-02, -6.3684e-03],\n",
      "          [-4.0054e-02, -2.3555e-02, -4.1744e-02,  3.9548e-02,  4.5613e-02],\n",
      "          [-3.6101e-02,  1.5824e-02,  1.5559e-02, -3.9977e-02, -3.6495e-02],\n",
      "          [-2.5557e-03,  3.9684e-02, -3.3736e-02,  4.0445e-02, -4.4383e-02],\n",
      "          [ 3.6060e-03,  4.2024e-02,  1.9785e-02, -5.9432e-03,  3.4273e-02]],\n",
      "\n",
      "         [[ 5.2757e-03, -1.3402e-02,  4.4926e-02,  9.2129e-03, -4.6584e-02],\n",
      "          [ 1.4226e-02,  4.0575e-02, -4.1019e-02,  3.4987e-02,  6.2422e-03],\n",
      "          [-2.2489e-02, -4.2624e-02, -1.0359e-02, -1.7256e-02,  4.8677e-02],\n",
      "          [ 3.2034e-02, -4.3040e-02,  2.5949e-02,  2.3884e-02, -1.8725e-02],\n",
      "          [ 3.6024e-02, -2.1067e-03,  2.5841e-02,  4.1496e-02,  4.5401e-02]],\n",
      "\n",
      "         [[ 2.6302e-02,  4.9744e-02, -2.7565e-02,  4.8251e-02,  1.2478e-02],\n",
      "          [ 3.9936e-03,  4.5892e-02,  3.1071e-02, -4.4876e-02,  2.7118e-02],\n",
      "          [-2.1878e-02,  2.7605e-02,  1.4165e-02, -7.3411e-05, -1.2132e-02],\n",
      "          [-2.3552e-02,  4.7699e-02, -4.2355e-02, -6.0314e-03,  4.2271e-02],\n",
      "          [-4.6654e-02, -3.7663e-02, -4.2939e-02, -2.0684e-02, -3.7077e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4223e-03,  4.2398e-02,  4.2677e-02,  4.2254e-02,  2.0135e-02],\n",
      "          [-1.5186e-02, -3.3164e-02, -1.9703e-02,  4.7351e-02,  4.1228e-02],\n",
      "          [ 3.3072e-02,  4.0964e-02, -2.4744e-02,  2.1835e-02,  4.9985e-02],\n",
      "          [ 4.7655e-02,  3.3927e-03, -1.5168e-02,  6.2365e-03,  3.3090e-02],\n",
      "          [ 3.3906e-02, -3.4558e-02,  3.5282e-02,  1.1963e-02,  1.4176e-02]],\n",
      "\n",
      "         [[ 1.7859e-02,  3.8475e-02,  1.3960e-02, -4.0932e-02, -8.9119e-03],\n",
      "          [ 1.5923e-02, -2.1136e-02,  3.7780e-02,  4.3982e-02,  2.3602e-02],\n",
      "          [ 2.8749e-02, -2.4865e-02,  4.0977e-02,  4.9998e-02, -3.9256e-02],\n",
      "          [ 1.1910e-02, -1.5245e-02,  3.5680e-02,  3.4956e-02,  1.1765e-02],\n",
      "          [ 7.4117e-03, -4.7198e-02, -4.5673e-02,  4.3768e-02,  1.8307e-02]],\n",
      "\n",
      "         [[-8.9537e-03, -2.5544e-02, -2.2207e-02, -3.8127e-02, -5.7766e-03],\n",
      "          [ 1.8305e-03, -2.2944e-02,  5.6058e-03, -3.0844e-02, -2.7487e-03],\n",
      "          [ 7.2067e-03, -3.0001e-03, -2.8760e-02,  2.3475e-02,  1.9869e-03],\n",
      "          [ 5.2300e-03, -4.1581e-02, -2.0647e-02, -3.6819e-02, -4.6374e-03],\n",
      "          [ 1.7202e-02, -2.5715e-03, -3.0466e-02, -3.1806e-02, -4.0610e-02]]]])), ('conv2.bias', tensor([-0.0316,  0.0361,  0.0268, -0.0148,  0.0006, -0.0338, -0.0076, -0.0015,\n",
      "         0.0468, -0.0492, -0.0230, -0.0254, -0.0465, -0.0404,  0.0374,  0.0384,\n",
      "         0.0227, -0.0368,  0.0120, -0.0191, -0.0091, -0.0440,  0.0272,  0.0036,\n",
      "        -0.0475, -0.0128, -0.0476,  0.0200, -0.0006, -0.0291, -0.0028,  0.0345])), ('fc1.weight', tensor([[ 0.0078, -0.0139,  0.0180,  ...,  0.0190,  0.0145,  0.0029],\n",
      "        [ 0.0165, -0.0241, -0.0149,  ...,  0.0143,  0.0111,  0.0093],\n",
      "        [ 0.0040, -0.0043,  0.0040,  ...,  0.0028, -0.0098, -0.0250],\n",
      "        ...,\n",
      "        [ 0.0067, -0.0149, -0.0155,  ..., -0.0229, -0.0131,  0.0088],\n",
      "        [-0.0006, -0.0135, -0.0059,  ..., -0.0146, -0.0058, -0.0153],\n",
      "        [-0.0164, -0.0048, -0.0193,  ...,  0.0195, -0.0237, -0.0250]])), ('fc1.bias', tensor([ 0.0088, -0.0198,  0.0129,  0.0030, -0.0173,  0.0072, -0.0222, -0.0179,\n",
      "        -0.0089, -0.0033]))])\n",
      "Establishing client devices...\n",
      "Training dataset has been distributed into 1000 pieces.\n",
      "Established 1000 client devices.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe current train start time is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m current_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFedProx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mexperiment_FedProx_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_distributing_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitera_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstraggler_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 1552\u001b[0m, in \u001b[0;36mexperiment_FedProx_model\u001b[1;34m(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id, show_history, save_result)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;66;03m# Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;66;03m#for i in range(len(client_list)):\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m \u001b[38;5;66;03m#    print(client_dataset_list[i])\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstablished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(client_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m client devices.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1552\u001b[0m cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_federated_learning_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1554\u001b[0m cost_history_total\u001b[38;5;241m.\u001b[39mappend(cost_history)\n\u001b[0;32m   1555\u001b[0m time_history_total\u001b[38;5;241m.\u001b[39mappend(time_history)\n",
      "Cell \u001b[1;32mIn[8], line 890\u001b[0m, in \u001b[0;36mtrain_federated_learning_model\u001b[1;34m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_federated_learning_model\u001b[39m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, aggregate_func\u001b[38;5;241m=\u001b[39mfederated_averaging, loss_func\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy, accuracy_func\u001b[38;5;241m=\u001b[39mget_accuracy, error_func\u001b[38;5;241m=\u001b[39mget_error, show_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 890\u001b[0m     cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43miterate_federated_learning_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;66;03m# Print learned parameters\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m global_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "Cell \u001b[1;32mIn[8], line 598\u001b[0m, in \u001b[0;36miterate_federated_learning_model\u001b[1;34m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history)\u001b[0m\n\u001b[0;32m    595\u001b[0m test_error_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Initial Loss, Accuracy, Error\u001b[39;00m\n\u001b[1;32m--> 598\u001b[0m train_loss, train_accuracy, train_error \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m train_loss_history\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    600\u001b[0m train_accuracy_history\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "Cell \u001b[1;32mIn[8], line 213\u001b[0m, in \u001b[0;36mevaluate_model_simple\u001b[1;34m(model, dataloader, loss_func, accuracy_func, error_func)\u001b[0m\n\u001b[0;32m    211\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 213\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[0;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\autograd\\profiler.py:631\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Number of Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = torch.optim.Adam\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = True\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 50\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [relative_rate_to_client_number(num_clients_list[0], 0.5)]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Straggler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Mu Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 20\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = []\n",
    "mu_list = [1.00]\n",
    "\n",
    "random_local_epoch = False\n",
    "compare_id = 11\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, learning_rate_list, batch_size_list, straggler_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, mu_list, random_local_epoch, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.5 Scaffold Federated Learning Experiments ###\n",
    "\n",
    "Note that, compare_id represents:\n",
    "\n",
    "1: compare global epochs\n",
    "\n",
    "2: compare local epochs\n",
    "\n",
    "3: compare number of clients\n",
    "\n",
    "4: compare random sample client number\n",
    "\n",
    "5: compare learning rate\n",
    "\n",
    "6: compare batch size\n",
    "\n",
    "7: compare the aggregate weight algorithms\n",
    "\n",
    "8: compare loss function\n",
    "\n",
    "9: compare accuracy function\n",
    "\n",
    "10: compare error function\n",
    "\n",
    "11: compare straggler\n",
    "\n",
    "12: compare mu value\n",
    "\n",
    "Others: compare global epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "The current train start time is 2024-02-15 23.20.09.\n",
      "=== The training for mu is 0.0 ===\n",
      "MNIST_CNN_Model(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "OrderedDict([('conv1.weight', tensor([[[[ 7.0056e-02, -3.6078e-02,  1.4718e-02,  3.7970e-02,  5.6893e-03],\n",
      "          [-1.2880e-02, -1.0208e-01,  1.7620e-01,  4.7947e-03,  7.2699e-02],\n",
      "          [-8.6508e-02, -3.5053e-02,  1.4207e-02, -4.3081e-02,  1.8967e-01],\n",
      "          [-1.0488e-01, -7.3419e-02, -6.0547e-02, -6.2782e-02, -7.1061e-02],\n",
      "          [-5.0021e-02, -1.3669e-01, -1.4520e-01,  1.0690e-01, -1.7700e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2600e-01,  3.2521e-02,  2.9996e-02, -3.8422e-02,  4.0186e-02],\n",
      "          [ 3.3763e-02,  1.4044e-01, -1.4322e-01,  1.6722e-01,  6.3892e-02],\n",
      "          [-5.5757e-02, -1.1654e-01, -7.0194e-02,  1.5725e-01, -1.5839e-01],\n",
      "          [-1.8172e-01, -2.5115e-02, -1.0503e-01,  1.1648e-01, -1.6502e-01],\n",
      "          [-1.0165e-01,  1.3428e-01, -1.8702e-01, -5.0852e-02,  1.5292e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.1327e-02, -1.9360e-01, -1.7726e-01,  1.3330e-01, -1.7723e-01],\n",
      "          [ 9.9721e-02, -1.7505e-01,  1.9377e-02, -1.7738e-01,  5.5376e-02],\n",
      "          [-1.7768e-01, -1.3444e-01, -1.1571e-01,  2.8315e-02, -1.2721e-01],\n",
      "          [-1.5111e-01, -4.5198e-02,  1.4797e-01, -3.4945e-02, -6.5891e-02],\n",
      "          [ 1.1853e-03,  1.0614e-01, -1.8282e-01, -1.3919e-01, -3.8653e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3649e-02, -1.9981e-01,  1.1206e-01, -9.8411e-02,  1.0314e-01],\n",
      "          [-1.0322e-01, -9.1037e-03,  1.0581e-01, -1.6460e-01,  1.6171e-02],\n",
      "          [ 1.7103e-01,  1.2718e-01, -1.4250e-01, -7.3437e-02,  1.2147e-01],\n",
      "          [-2.1608e-02,  9.9230e-02, -4.2001e-02, -5.6965e-02,  1.2438e-02],\n",
      "          [ 1.2200e-02, -3.8183e-02,  1.6300e-01, -1.6336e-01, -1.4880e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9094e-01,  5.3543e-02, -4.1717e-02,  1.0165e-01, -1.0960e-01],\n",
      "          [ 1.0219e-01,  1.7536e-01,  4.1290e-02, -1.3257e-01,  3.5980e-02],\n",
      "          [-1.7018e-02, -1.6623e-01, -1.2999e-01,  7.2628e-02, -4.7909e-02],\n",
      "          [-1.6473e-01, -1.9723e-01, -2.3732e-02,  6.5169e-03,  4.3907e-02],\n",
      "          [ 1.8144e-02, -8.3350e-02,  7.1666e-03,  5.3650e-02, -9.4695e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4012e-02,  8.5093e-02,  6.8745e-02,  4.0655e-03, -6.1001e-02],\n",
      "          [-1.0407e-01, -1.2393e-01,  2.1129e-02, -9.1329e-03,  8.7896e-02],\n",
      "          [ 3.5637e-02,  1.9728e-02, -1.2586e-01, -7.6235e-02,  6.9201e-02],\n",
      "          [-6.2220e-02,  5.0418e-02, -4.0099e-02, -1.9826e-01,  8.1063e-02],\n",
      "          [ 1.1405e-01,  1.3533e-01, -2.6696e-02, -1.8002e-01,  1.0709e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.7763e-02, -1.8378e-02, -1.8695e-01,  1.7333e-01, -8.1873e-02],\n",
      "          [ 1.1345e-01,  5.9492e-02,  1.6671e-01,  1.1282e-02,  4.1473e-02],\n",
      "          [-6.2920e-02,  9.0030e-02,  1.9752e-01,  1.3986e-01,  6.1292e-02],\n",
      "          [ 1.2180e-01,  6.5892e-02, -1.0742e-01, -2.6175e-02,  1.5657e-01],\n",
      "          [-1.0401e-02, -1.7426e-01, -9.3702e-02, -2.0710e-02,  8.8631e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5728e-01, -1.6468e-01, -1.9278e-01,  7.9824e-02, -2.3429e-03],\n",
      "          [-5.5533e-02, -8.3070e-02,  1.0777e-01, -1.4747e-01, -2.5953e-02],\n",
      "          [-5.6265e-03,  3.2140e-02, -1.4023e-01,  3.4294e-02,  4.3775e-02],\n",
      "          [ 1.5602e-01,  1.4670e-01, -1.2793e-01,  2.7478e-02, -7.1892e-02],\n",
      "          [ 1.8766e-01,  6.2192e-02,  1.6882e-01,  2.4081e-02, -5.0433e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0459e-01,  1.5224e-01,  4.7319e-02, -1.7343e-02,  8.6203e-02],\n",
      "          [-1.3690e-01, -1.5640e-01,  5.3713e-02, -1.8870e-01, -1.7037e-01],\n",
      "          [-3.8904e-02, -1.0243e-01, -4.5157e-03,  6.6634e-02, -1.2752e-02],\n",
      "          [-7.3449e-02,  1.3980e-01, -1.7420e-01,  8.5490e-02, -1.4730e-01],\n",
      "          [-1.2519e-01, -9.1270e-02, -4.2059e-02, -6.9502e-02,  1.4759e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4109e-01, -1.2956e-01, -2.4901e-02,  9.2122e-02, -1.2693e-01],\n",
      "          [-1.7685e-01, -1.7000e-01, -2.8084e-02, -1.3736e-01,  1.1325e-01],\n",
      "          [ 1.5702e-01, -1.3299e-01, -1.7885e-01, -3.1540e-02,  1.3907e-02],\n",
      "          [ 5.9907e-02,  1.2921e-01, -1.4732e-01, -1.1493e-01, -3.6655e-02],\n",
      "          [ 1.1486e-01, -1.6498e-01, -2.9524e-02, -1.5453e-01, -1.2960e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4529e-02, -6.3003e-02,  1.7224e-01, -2.1238e-02,  9.4467e-02],\n",
      "          [-1.7047e-01,  1.2957e-01, -2.7052e-02,  1.8947e-02,  1.9611e-01],\n",
      "          [-9.1663e-02, -1.8869e-01, -1.6488e-01,  1.7070e-01,  1.8050e-01],\n",
      "          [-1.5157e-01, -9.5100e-02, -1.3958e-01,  1.9843e-01, -4.8943e-02],\n",
      "          [-4.6315e-02,  5.5509e-02, -1.0859e-01,  1.0278e-01, -1.3063e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.4668e-02, -6.9736e-02, -7.2403e-02,  1.7676e-01,  1.1452e-01],\n",
      "          [ 6.7137e-02,  1.3365e-01,  9.2683e-02,  1.0883e-01, -8.3486e-02],\n",
      "          [-1.8836e-01, -7.9845e-02, -8.9959e-02,  1.7321e-01, -7.9212e-02],\n",
      "          [-1.8765e-01, -9.2941e-02, -1.0078e-01,  1.4289e-01,  1.3413e-01],\n",
      "          [ 1.7060e-02,  1.7276e-01, -2.0392e-04,  9.6822e-02,  6.1246e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9011e-02, -1.7132e-01, -1.5334e-01, -6.0346e-02, -8.3544e-02],\n",
      "          [-1.4414e-01, -1.7363e-01, -1.1672e-01, -9.3899e-02,  3.4313e-02],\n",
      "          [ 5.6079e-02, -1.9427e-01,  1.5407e-02,  1.0990e-01,  1.7031e-03],\n",
      "          [-9.8586e-05,  7.5007e-02,  1.0703e-02,  6.2217e-02,  1.1265e-01],\n",
      "          [-1.0736e-01, -1.7226e-01, -7.8727e-02, -8.8305e-02, -2.2749e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.7679e-02,  4.0902e-02, -1.7578e-01, -3.6789e-02, -4.3447e-02],\n",
      "          [-6.6417e-02,  1.1301e-01,  6.7057e-02,  7.7802e-02, -7.1439e-02],\n",
      "          [-1.1139e-01,  3.2168e-02, -1.6564e-01, -1.3877e-01, -4.8567e-02],\n",
      "          [ 1.7979e-01, -1.1776e-01,  1.3588e-01, -2.0298e-02, -4.3735e-02],\n",
      "          [-4.3328e-02, -1.5443e-01,  6.2857e-02,  4.3630e-02, -4.9028e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2663e-02, -1.2179e-01,  5.8897e-02, -6.5709e-02, -1.2164e-02],\n",
      "          [-1.2286e-01,  1.0235e-02,  1.8379e-01,  1.1945e-01, -1.4690e-01],\n",
      "          [ 1.9758e-01,  1.3307e-02, -1.9084e-01, -9.2287e-03,  1.3303e-01],\n",
      "          [ 1.5161e-01, -2.5910e-02, -1.9148e-01,  1.1389e-01, -6.8160e-02],\n",
      "          [-4.5876e-02, -1.4137e-01, -5.2871e-02, -1.0923e-01, -1.0931e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.7524e-02, -1.3354e-01, -4.4262e-02, -1.3763e-01,  2.1984e-02],\n",
      "          [-1.8052e-01, -1.0885e-01, -1.2716e-01, -1.7032e-01, -1.0316e-01],\n",
      "          [ 1.8102e-01,  2.4822e-02,  1.4948e-02,  1.9749e-01, -1.3880e-01],\n",
      "          [-1.2591e-01,  1.2027e-01, -3.8178e-02, -1.0512e-01,  1.1694e-01],\n",
      "          [ 9.1784e-02, -3.0149e-02, -1.0969e-01, -1.5154e-01, -1.9698e-01]]]])), ('conv1.bias', tensor([ 0.0699, -0.0685,  0.1364,  0.1565,  0.1077, -0.1877, -0.1929,  0.0007,\n",
      "         0.0277,  0.1526, -0.1659, -0.1374,  0.1976,  0.0974, -0.0717, -0.1873])), ('conv2.weight', tensor([[[[ 3.5099e-02,  2.6974e-03, -4.6617e-02,  3.4812e-02,  3.9636e-02],\n",
      "          [-4.6080e-02, -4.8258e-02, -3.3116e-03, -3.8481e-02,  2.2251e-02],\n",
      "          [ 3.4993e-02, -4.7354e-02,  2.7543e-02,  3.2344e-02,  2.1661e-02],\n",
      "          [-8.3540e-04, -6.5041e-03,  1.8795e-03,  2.2675e-02, -1.4207e-03],\n",
      "          [-4.5841e-02, -2.9385e-02,  2.8084e-02,  4.0577e-02, -3.7535e-02]],\n",
      "\n",
      "         [[ 1.2301e-02,  8.2907e-03,  1.5023e-02,  2.2897e-03, -3.6241e-02],\n",
      "          [-8.1259e-03,  1.7325e-02, -7.4316e-03,  3.9784e-02,  4.7947e-02],\n",
      "          [ 3.8926e-02, -2.5471e-02,  3.3292e-02,  6.5196e-03, -4.0916e-02],\n",
      "          [-2.8263e-02,  3.8666e-02, -2.5729e-02,  1.3229e-02, -3.5452e-03],\n",
      "          [-3.3450e-02,  9.5209e-03, -2.4259e-02,  5.9686e-03, -3.6056e-02]],\n",
      "\n",
      "         [[ 3.8105e-02, -4.8554e-02, -3.9048e-02, -2.8162e-02,  4.8726e-02],\n",
      "          [ 5.6575e-03,  3.9228e-02, -6.6161e-03,  3.8676e-02,  1.0243e-02],\n",
      "          [-1.1024e-02,  6.8644e-03, -2.3991e-02, -5.6546e-03, -3.0547e-02],\n",
      "          [ 4.9699e-02, -2.1976e-02,  2.4191e-02, -2.9483e-02,  2.3881e-02],\n",
      "          [-3.3627e-03, -4.0100e-02, -3.8467e-02, -4.0396e-02, -1.4006e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9024e-02,  2.2413e-02, -1.3177e-02, -5.7706e-03,  3.8688e-02],\n",
      "          [ 1.5548e-02,  2.2509e-02, -7.7484e-03, -1.8386e-02,  3.0246e-02],\n",
      "          [-9.0644e-03, -4.7256e-02,  1.0731e-02,  8.9388e-05,  5.2200e-03],\n",
      "          [-3.8888e-02, -1.6525e-02, -4.3142e-03, -2.7023e-02,  7.8297e-03],\n",
      "          [-4.8328e-02, -3.6370e-02,  2.1090e-02, -1.0048e-02,  2.3638e-02]],\n",
      "\n",
      "         [[-4.5774e-02,  2.3967e-02,  3.1998e-02, -1.2559e-02,  2.4218e-02],\n",
      "          [-2.6102e-03, -1.6735e-02, -3.3662e-02, -4.5301e-02,  1.4154e-02],\n",
      "          [ 3.0361e-02, -1.4311e-02, -4.4658e-02, -2.1917e-02,  2.9029e-02],\n",
      "          [ 2.6953e-02, -1.3243e-02, -3.6733e-02, -4.7475e-02, -1.4088e-02],\n",
      "          [-1.1537e-02,  2.4453e-02, -2.1123e-03, -2.5237e-02, -2.4139e-02]],\n",
      "\n",
      "         [[ 2.6595e-02, -7.6910e-03, -4.8870e-02, -3.3495e-02, -2.1292e-02],\n",
      "          [ 2.3543e-02, -2.4340e-02, -2.1819e-03, -4.5919e-02, -2.0973e-02],\n",
      "          [-4.0243e-02,  3.2767e-02,  6.5912e-03,  2.0840e-02,  1.9706e-02],\n",
      "          [-3.3271e-02,  1.7845e-02,  6.5321e-03, -2.1341e-02,  4.6354e-02],\n",
      "          [ 4.5407e-02, -3.9309e-02,  4.2229e-02, -9.0867e-03,  4.7704e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.8282e-02, -6.9298e-04,  1.8916e-02,  2.6173e-02,  9.2235e-03],\n",
      "          [ 2.6196e-02, -1.8543e-02, -8.4516e-03, -3.7935e-02, -4.4541e-02],\n",
      "          [-4.7656e-02,  2.0477e-02,  3.9968e-02, -4.2924e-02,  3.0411e-02],\n",
      "          [ 1.7516e-02, -4.2516e-02, -2.8098e-02, -4.3670e-02, -3.0858e-02],\n",
      "          [ 6.5878e-03, -2.0697e-02, -2.4507e-02,  4.2769e-02,  4.5971e-02]],\n",
      "\n",
      "         [[ 4.9555e-02,  2.5712e-03,  1.7634e-02, -1.2744e-02,  6.6585e-04],\n",
      "          [ 2.5821e-02,  2.3493e-02, -1.4398e-02, -3.4121e-02, -3.9433e-02],\n",
      "          [-4.8713e-02,  9.5254e-03,  2.7943e-02, -3.7581e-02,  1.1394e-02],\n",
      "          [-3.0152e-02, -7.5110e-03, -4.2829e-02,  4.5969e-03,  2.1639e-02],\n",
      "          [ 1.8391e-02,  1.9068e-02, -1.1473e-02,  9.3964e-03,  1.2733e-02]],\n",
      "\n",
      "         [[-4.1070e-02,  4.1255e-02, -3.1039e-02,  1.6810e-02,  3.3592e-02],\n",
      "          [ 2.5881e-02, -1.6280e-02,  1.4362e-02,  1.1721e-02,  1.6565e-02],\n",
      "          [ 3.8126e-02, -4.4557e-02, -1.6687e-02, -4.0498e-02,  1.2733e-02],\n",
      "          [-1.9260e-02,  3.7757e-02, -1.9237e-02, -1.1328e-02, -6.9810e-03],\n",
      "          [-1.5124e-02, -7.2351e-03,  4.3909e-02,  2.2275e-02,  3.0075e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3231e-02, -1.8790e-02, -1.8490e-03,  1.2282e-02, -1.1718e-02],\n",
      "          [-2.4480e-02, -1.0026e-02, -2.4133e-02,  1.9433e-03,  3.0228e-02],\n",
      "          [-4.0605e-02, -3.6759e-03, -6.5660e-04, -2.7152e-02,  1.4135e-02],\n",
      "          [ 3.3605e-02, -6.5204e-03, -1.2602e-02, -2.2170e-02, -9.6500e-05],\n",
      "          [-4.2346e-02, -3.1295e-02, -4.8195e-02,  3.9378e-02, -3.4613e-03]],\n",
      "\n",
      "         [[-8.5352e-03,  3.1297e-02, -5.6171e-04,  3.6984e-02,  1.2277e-02],\n",
      "          [ 3.6370e-02,  1.8875e-02,  3.4005e-02, -1.8120e-02, -9.0849e-05],\n",
      "          [-1.3774e-03, -2.8120e-02,  2.9735e-03,  3.0414e-02, -4.3344e-02],\n",
      "          [ 3.1244e-02, -4.5428e-02,  3.8746e-03,  2.5398e-02, -4.1028e-02],\n",
      "          [ 4.0438e-02, -1.2557e-02,  2.2265e-03, -1.9656e-02,  1.4791e-02]],\n",
      "\n",
      "         [[-2.2508e-02, -4.5681e-02, -4.7292e-02,  2.7697e-02, -2.3033e-02],\n",
      "          [-4.5789e-02,  5.5292e-04,  1.8697e-02,  4.3845e-02, -4.8966e-02],\n",
      "          [ 4.0306e-02,  1.7682e-02,  9.8004e-03,  2.4583e-02, -1.3003e-02],\n",
      "          [-4.2388e-02, -3.6715e-02, -5.1770e-04, -4.5059e-03,  4.8143e-02],\n",
      "          [-2.1408e-02,  3.4344e-03,  2.7359e-02, -2.6161e-02,  1.9718e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9968e-02, -3.8261e-02, -2.7757e-02,  3.8803e-03, -1.4033e-02],\n",
      "          [-2.9739e-02, -3.7403e-02, -1.4597e-02,  5.2276e-03, -6.3543e-04],\n",
      "          [ 2.5850e-02,  3.1592e-02,  9.3941e-03,  2.3119e-02,  2.4585e-02],\n",
      "          [-3.6681e-02, -6.7554e-03,  2.1613e-02, -1.5587e-02,  4.1848e-02],\n",
      "          [-2.6911e-03, -1.1266e-02,  2.3880e-02,  3.1477e-02, -3.8570e-02]],\n",
      "\n",
      "         [[-2.4914e-02, -2.5586e-02,  3.9399e-02,  4.9020e-02,  1.9233e-02],\n",
      "          [-6.2314e-03, -1.9418e-02,  2.4495e-02, -2.6739e-02,  1.9296e-02],\n",
      "          [ 9.5617e-03,  1.9977e-02, -4.8503e-02,  3.6659e-02, -6.2797e-03],\n",
      "          [ 4.7139e-02,  3.3306e-02, -4.8305e-02,  3.8617e-03,  5.5416e-03],\n",
      "          [ 4.9954e-02, -2.0339e-02,  3.0850e-02, -4.8057e-02,  1.5074e-03]],\n",
      "\n",
      "         [[-1.6532e-02, -3.9726e-02, -4.5451e-02, -1.0473e-02,  3.7830e-04],\n",
      "          [-1.3001e-02, -3.1883e-02, -1.4287e-02, -1.2243e-02, -3.2780e-02],\n",
      "          [ 2.5192e-02,  2.0818e-02,  2.8779e-02,  2.6532e-02,  3.2627e-02],\n",
      "          [ 1.1900e-02,  1.5553e-02,  2.4918e-02, -1.5765e-02,  3.7725e-02],\n",
      "          [ 2.0004e-02,  4.8859e-02, -3.7405e-02,  3.3376e-02,  3.2163e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7302e-02, -4.9720e-02, -3.4710e-02, -3.8336e-02,  2.5295e-02],\n",
      "          [-3.2810e-02,  1.4880e-03, -2.8151e-02,  3.2759e-02,  9.6389e-03],\n",
      "          [ 4.0259e-02,  5.9301e-03, -3.0714e-02, -2.2763e-02, -3.3246e-02],\n",
      "          [ 9.3432e-03, -4.8496e-03, -2.1950e-02, -4.4209e-02,  4.3739e-02],\n",
      "          [-1.2485e-02, -2.6425e-03,  2.7111e-02, -2.6308e-02,  2.5688e-02]],\n",
      "\n",
      "         [[ 2.2961e-02,  2.3676e-02, -2.4110e-04,  4.7704e-02, -3.4388e-02],\n",
      "          [-3.2579e-02,  2.6979e-02, -3.3787e-02,  2.3404e-02,  3.8321e-02],\n",
      "          [-4.5716e-03,  1.9112e-02, -4.7190e-02, -4.8310e-02,  1.9192e-02],\n",
      "          [-4.9563e-03, -4.2584e-02, -3.0701e-02, -1.2379e-02,  4.0579e-02],\n",
      "          [-3.2885e-03,  3.1062e-02,  2.4834e-02, -4.7019e-02,  5.6974e-03]],\n",
      "\n",
      "         [[-3.2752e-02,  3.0737e-02, -7.2334e-03, -3.8324e-02,  1.4051e-02],\n",
      "          [ 4.3115e-02, -3.2634e-02,  2.6559e-02, -4.4987e-03, -1.7267e-02],\n",
      "          [-4.8484e-02, -4.2421e-02, -2.9054e-02, -3.1918e-02,  2.0292e-03],\n",
      "          [ 5.5380e-03,  1.7996e-02,  7.3834e-03,  3.0506e-02, -3.4108e-02],\n",
      "          [-2.4724e-02,  3.2260e-02, -3.9888e-02,  1.3087e-02, -2.0018e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.5799e-02,  3.1193e-02, -2.8251e-02,  2.4869e-02,  5.0600e-03],\n",
      "          [ 5.9184e-03,  2.5731e-02,  4.3156e-02,  2.0520e-02,  2.5907e-02],\n",
      "          [ 2.6648e-02, -3.1505e-02,  4.9138e-02,  4.1487e-02, -4.9008e-02],\n",
      "          [-3.2838e-03,  3.0207e-02, -6.6211e-03, -1.5580e-02,  1.5489e-03],\n",
      "          [-1.6023e-02, -2.6835e-02, -4.7785e-02,  2.7001e-02, -4.6325e-02]],\n",
      "\n",
      "         [[ 9.4570e-03, -4.0475e-03, -2.2206e-02, -4.8437e-02, -8.4627e-03],\n",
      "          [-4.3878e-02,  4.9830e-02,  2.1275e-02, -2.8408e-02,  2.9130e-02],\n",
      "          [ 4.4656e-02,  3.9209e-02, -4.1512e-02,  2.9577e-02, -1.7699e-02],\n",
      "          [ 2.5456e-02, -3.6724e-02, -4.9627e-02,  7.7796e-03,  7.9563e-03],\n",
      "          [-2.8194e-02,  2.1392e-02, -3.3771e-03,  3.0476e-03, -1.7935e-02]],\n",
      "\n",
      "         [[ 4.0592e-03,  3.9285e-02,  4.5826e-02,  3.5852e-02,  1.8452e-02],\n",
      "          [-8.9766e-03,  4.9176e-02,  2.9841e-02, -3.1296e-02, -2.0897e-02],\n",
      "          [ 8.6007e-03,  4.4034e-02,  6.5342e-06, -3.0680e-02, -3.8989e-03],\n",
      "          [-4.4600e-02,  2.5054e-02,  7.6151e-03,  1.3286e-02,  3.9681e-03],\n",
      "          [ 3.1623e-02, -2.8329e-02, -4.2076e-02,  3.6123e-02, -1.9502e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1838e-02, -8.3371e-03,  2.3774e-02, -1.9917e-02, -3.9126e-02],\n",
      "          [-1.0233e-02,  2.9141e-02, -4.4808e-02, -4.6543e-02, -2.9451e-02],\n",
      "          [ 2.4147e-02, -3.9848e-02, -2.1257e-02, -2.2348e-02,  3.0927e-02],\n",
      "          [-4.3115e-02, -1.2700e-02, -6.1463e-03,  3.1651e-02,  2.7956e-02],\n",
      "          [-5.4720e-03,  1.3762e-02, -3.0011e-02,  3.6180e-02,  1.5854e-02]],\n",
      "\n",
      "         [[-1.8137e-02, -1.0839e-02, -1.6969e-02,  2.0943e-02, -3.8897e-02],\n",
      "          [ 4.6588e-02, -3.5676e-02, -4.1391e-03, -5.3140e-03,  4.1073e-02],\n",
      "          [ 3.7815e-02,  4.3933e-02, -1.8423e-02,  3.3622e-02, -3.7424e-02],\n",
      "          [ 4.5902e-02,  2.6417e-03,  4.4490e-02, -9.5712e-03, -3.4183e-02],\n",
      "          [-4.6662e-02, -4.3846e-02,  3.7332e-02,  4.4714e-02,  3.8421e-02]],\n",
      "\n",
      "         [[-6.9444e-03,  3.6856e-02, -2.6702e-02, -2.2929e-02,  4.2975e-02],\n",
      "          [-2.9185e-02, -1.9161e-02,  2.8061e-02,  3.5229e-02, -3.8895e-02],\n",
      "          [ 3.4660e-02, -5.6079e-03,  5.9902e-03,  3.8413e-02, -3.4227e-02],\n",
      "          [-4.4149e-02,  4.8986e-02, -3.8653e-02, -2.8822e-02,  2.6974e-02],\n",
      "          [-2.0455e-03,  3.9192e-02, -1.4349e-02, -5.3915e-03,  8.1203e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8354e-02, -4.0355e-02,  4.6643e-03,  2.3205e-02,  2.9261e-02],\n",
      "          [-3.5467e-03,  4.6227e-02, -4.3571e-02,  4.1653e-02,  1.7028e-02],\n",
      "          [-9.0321e-03,  3.9312e-02,  3.2738e-02,  2.8494e-02, -1.2751e-03],\n",
      "          [-1.6368e-03,  3.2202e-02,  3.7695e-02, -4.9047e-02, -1.5375e-02],\n",
      "          [-3.0184e-02,  1.9119e-02, -4.4590e-02,  3.8280e-02,  2.7167e-02]],\n",
      "\n",
      "         [[-1.1305e-02, -3.9092e-02,  2.8905e-02,  3.8895e-02, -4.6534e-02],\n",
      "          [ 1.7005e-02, -4.1824e-02, -2.6167e-02,  9.5973e-03, -2.7491e-02],\n",
      "          [-1.3801e-02,  1.6019e-02,  1.4880e-02,  1.4299e-02,  4.5152e-02],\n",
      "          [ 4.4917e-02, -1.6868e-02,  4.0573e-02, -2.4945e-02,  2.0172e-02],\n",
      "          [ 3.8969e-02, -4.1254e-02,  1.8816e-02,  2.0327e-02, -1.1496e-02]],\n",
      "\n",
      "         [[ 1.6980e-02, -2.3874e-02, -1.3714e-03,  1.6011e-03, -2.8422e-02],\n",
      "          [ 1.5890e-02,  2.6313e-02,  4.2091e-03, -3.5942e-02, -5.3189e-03],\n",
      "          [ 1.9293e-02, -2.4397e-02, -2.8689e-02,  3.8304e-03,  9.5570e-03],\n",
      "          [-2.3831e-02,  4.1903e-02,  3.8528e-02,  1.6681e-02, -3.4013e-02],\n",
      "          [ 9.6826e-03, -2.0049e-02, -3.5170e-02, -2.3336e-02,  3.3206e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.1748e-02,  4.9892e-02,  2.9504e-02, -2.1620e-02,  1.7736e-02],\n",
      "          [ 3.6321e-02,  2.9785e-02,  3.8147e-02,  7.4647e-03,  2.6577e-02],\n",
      "          [-1.3067e-03, -4.9222e-02, -2.2048e-03,  1.4567e-02,  1.9064e-03],\n",
      "          [-2.9260e-02,  1.2458e-03,  3.1746e-02, -2.6558e-02,  1.0508e-02],\n",
      "          [ 2.4987e-02, -1.9899e-02,  3.4189e-02, -5.0942e-03,  2.3688e-02]],\n",
      "\n",
      "         [[ 2.0285e-02,  4.7911e-02,  1.0297e-02,  4.3527e-02, -2.5663e-02],\n",
      "          [ 3.6102e-02,  4.0519e-02, -3.7870e-02, -2.2083e-02,  1.5127e-03],\n",
      "          [ 2.3722e-03,  2.8726e-03, -7.7210e-03, -2.5071e-02,  1.1450e-02],\n",
      "          [-3.6165e-02,  4.8677e-03,  4.9734e-02, -2.1993e-02,  4.4888e-02],\n",
      "          [ 3.2836e-02, -3.0639e-02, -2.4525e-02,  4.5331e-02,  8.2925e-03]],\n",
      "\n",
      "         [[ 1.1808e-02, -4.3052e-02,  3.7794e-02,  6.9374e-03,  1.6806e-02],\n",
      "          [ 4.6606e-02, -1.0899e-02,  1.8363e-02,  3.6222e-02,  2.6626e-02],\n",
      "          [-2.2191e-02,  2.3839e-02,  4.9285e-02,  4.6214e-02,  4.8629e-02],\n",
      "          [-3.2267e-02, -2.0274e-02,  6.7267e-03,  2.0504e-02, -3.6662e-02],\n",
      "          [ 4.3148e-02,  3.4985e-02,  2.7795e-02, -9.8791e-03,  3.9103e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5686e-02, -3.5174e-02,  4.5516e-02,  1.4409e-02,  2.3731e-02],\n",
      "          [-2.0835e-02, -3.5283e-02, -1.3203e-02,  3.5586e-02,  4.0519e-02],\n",
      "          [-3.8532e-02,  3.9195e-02,  1.1867e-02, -4.4938e-02,  2.7761e-02],\n",
      "          [-1.8983e-02, -2.5387e-04, -4.4357e-02,  4.8645e-02, -5.1608e-03],\n",
      "          [-3.3230e-02, -2.4813e-02,  4.2533e-02,  2.8468e-02,  2.7268e-03]],\n",
      "\n",
      "         [[ 3.5574e-02, -7.9314e-03,  3.1844e-02,  4.2920e-02,  3.5937e-03],\n",
      "          [ 2.8392e-02,  3.8444e-02,  1.3298e-02,  3.5686e-03,  2.9303e-02],\n",
      "          [ 4.6649e-02,  6.2938e-03, -4.0873e-02,  1.5404e-02,  4.5900e-03],\n",
      "          [ 1.2667e-02,  1.1986e-02,  1.8981e-02, -1.5976e-02,  2.0675e-02],\n",
      "          [ 3.8033e-02, -4.5481e-02, -3.2326e-02, -2.8256e-03,  1.9215e-02]],\n",
      "\n",
      "         [[-3.6744e-02,  1.7131e-02, -1.0400e-02,  4.7698e-02, -4.7362e-02],\n",
      "          [ 4.3251e-02, -1.1400e-02, -2.1905e-02,  4.6331e-02,  1.7817e-02],\n",
      "          [ 1.5242e-03, -4.9455e-02, -1.4075e-02,  4.7487e-02,  5.1977e-03],\n",
      "          [ 1.8653e-02,  4.0671e-02,  4.8129e-02,  8.9274e-03, -2.4760e-02],\n",
      "          [-2.4049e-02, -3.2980e-02, -1.7887e-02,  2.2099e-02,  5.0856e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3161e-02, -4.0683e-02, -4.4034e-02,  2.6981e-02,  3.0000e-02],\n",
      "          [ 5.0560e-03,  3.5745e-02, -4.6078e-02, -1.2458e-02, -3.5563e-02],\n",
      "          [ 8.1721e-03, -2.4929e-02, -2.6482e-02,  3.0368e-02,  1.2696e-02],\n",
      "          [ 8.3844e-03, -1.0245e-02, -1.8539e-03,  4.6659e-02,  3.0412e-02],\n",
      "          [-3.8362e-02,  1.3364e-02,  2.5024e-02, -4.6929e-02,  2.9827e-02]],\n",
      "\n",
      "         [[ 3.7851e-02, -2.1226e-02,  2.0026e-02,  1.0981e-02,  5.1383e-03],\n",
      "          [-4.1722e-03,  3.5669e-02, -4.1410e-02, -2.4764e-02,  1.1557e-02],\n",
      "          [ 3.1941e-02, -4.6413e-02, -4.5311e-02,  7.2564e-03, -3.7647e-02],\n",
      "          [ 4.3987e-02, -2.4494e-02, -1.3233e-02, -3.2562e-02, -2.0061e-02],\n",
      "          [-4.5885e-02,  1.9545e-02, -1.2775e-03,  4.4147e-02, -1.0762e-02]],\n",
      "\n",
      "         [[-2.8688e-02,  4.2628e-02,  1.2816e-02,  2.2791e-02, -3.0599e-03],\n",
      "          [-4.5466e-02,  2.4515e-02, -2.3380e-02, -1.8820e-02, -3.9754e-02],\n",
      "          [ 3.3209e-04,  2.7674e-02,  4.8108e-02, -4.6990e-02,  9.5350e-03],\n",
      "          [-3.9169e-02, -1.1173e-02,  4.0027e-02, -4.4216e-02,  1.6526e-03],\n",
      "          [ 2.8681e-02, -4.5559e-02, -1.4989e-02,  7.1106e-04, -3.1403e-02]]]])), ('conv2.bias', tensor([ 0.0479,  0.0165,  0.0138,  0.0056, -0.0461, -0.0159, -0.0259, -0.0338,\n",
      "         0.0216, -0.0349,  0.0008,  0.0364, -0.0185,  0.0069,  0.0184, -0.0037,\n",
      "         0.0107,  0.0452, -0.0438, -0.0459,  0.0378,  0.0310,  0.0090,  0.0164,\n",
      "         0.0078,  0.0185,  0.0416,  0.0443,  0.0118, -0.0162, -0.0112, -0.0470])), ('fc1.weight', tensor([[-0.0021,  0.0043,  0.0016,  ...,  0.0185,  0.0025, -0.0126],\n",
      "        [ 0.0122, -0.0015, -0.0108,  ...,  0.0182, -0.0022, -0.0241],\n",
      "        [-0.0219,  0.0202, -0.0069,  ..., -0.0216,  0.0091,  0.0109],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0136, -0.0077,  ..., -0.0052, -0.0191,  0.0221],\n",
      "        [ 0.0252, -0.0088, -0.0064,  ...,  0.0009, -0.0200, -0.0209],\n",
      "        [-0.0042, -0.0010, -0.0071,  ..., -0.0024, -0.0147, -0.0060]])), ('fc1.bias', tensor([-0.0223, -0.0065, -0.0008, -0.0187,  0.0028,  0.0142, -0.0127,  0.0136,\n",
      "         0.0141,  0.0172]))])\n",
      "Establishing client devices...\n",
      "Training dataset has been distributed into 10 pieces.\n",
      "Established 10 client devices.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe current train start time is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m current_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaffold\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mexperiment_Scaffold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_distributing_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitera_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstraggler_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 1839\u001b[0m, in \u001b[0;36mexperiment_Scaffold_model\u001b[1;34m(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id, show_history, save_result)\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;66;03m# Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\u001b[39;00m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;66;03m#for i in range(len(client_list)):\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m \u001b[38;5;66;03m#    print(client_dataset_list[i])\u001b[39;00m\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstablished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(client_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m client devices.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1839\u001b[0m cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_Scaffold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaffold_update_controls_use_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1841\u001b[0m cost_history_total\u001b[38;5;241m.\u001b[39mappend(cost_history)\n\u001b[0;32m   1842\u001b[0m time_history_total\u001b[38;5;241m.\u001b[39mappend(time_history)\n",
      "Cell \u001b[1;32mIn[20], line 916\u001b[0m, in \u001b[0;36mtrain_Scaffold_model\u001b[1;34m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, global_step_size, Scaffold_update_controls_use_gradient, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_Scaffold_model\u001b[39m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, batch_size, global_step_size, Scaffold_update_controls_use_gradient, aggregate_func\u001b[38;5;241m=\u001b[39mfederated_averaging, loss_func\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy, accuracy_func\u001b[38;5;241m=\u001b[39mget_accuracy, error_func\u001b[38;5;241m=\u001b[39mget_error, show_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    915\u001b[0m     server_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00\u001b[39m\n\u001b[1;32m--> 916\u001b[0m     cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43miterate_Scaffold_global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_controls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaffold_update_controls_use_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# Print learned parameters\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m global_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "Cell \u001b[1;32mIn[20], line 822\u001b[0m, in \u001b[0;36miterate_Scaffold_global\u001b[1;34m(train_dataloader, test_dataloader, global_model, server_controls, client_list, random_sample_client_number, global_epochs, global_step_size, local_epochs, Scaffold_update_controls_use_gradient, aggregate_func, loss_func, accuracy_func, error_func, show_history)\u001b[0m\n\u001b[0;32m    820\u001b[0m     client_weights, client_controls_update \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtrain_Scaffold(global_model\u001b[38;5;241m.\u001b[39mparameters(), server_controls, random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, local_epochs), Scaffold_update_controls_use_gradient)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m     client_weights, client_controls_update \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_Scaffold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_controls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaffold_update_controls_use_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m delta_weights \u001b[38;5;241m=\u001b[39m client_weights \u001b[38;5;241m-\u001b[39m global_weights\n\u001b[0;32m    824\u001b[0m delta_client_controls \u001b[38;5;241m=\u001b[39m client_controls_update \u001b[38;5;241m-\u001b[39m client\u001b[38;5;241m.\u001b[39mclient_controls\n",
      "Cell \u001b[1;32mIn[20], line 556\u001b[0m, in \u001b[0;36mClientDevice.train_Scaffold\u001b[1;34m(self, global_weights, server_controls, num_epochs, Scaffold_update_controls_use_gradient)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_Scaffold\u001b[39m(\u001b[38;5;28mself\u001b[39m, global_weights, server_controls, num_epochs, Scaffold_update_controls_use_gradient):\n\u001b[1;32m--> 556\u001b[0m     loss_history, accuracy_history, error_history, time_history, client_controls_update \u001b[38;5;241m=\u001b[39m \u001b[43miterate_Scaffold_client\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_controls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaffold_update_controls_use_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), client_controls_update\n",
      "Cell \u001b[1;32mIn[20], line 763\u001b[0m, in \u001b[0;36miterate_Scaffold_client\u001b[1;34m(client, global_weights, server_controls, dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, Scaffold_update_controls_use_gradient, show_history)\u001b[0m\n\u001b[0;32m    761\u001b[0m error \u001b[38;5;241m=\u001b[39m error_func(outputs, labels)\n\u001b[0;32m    762\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 763\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_controls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_controls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaffold_update_controls_use_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    765\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 707\u001b[0m, in \u001b[0;36mScaffoldOptimizer.step\u001b[1;34m(self, server_controls, client_controls, return_grad)\u001b[0m\n\u001b[0;32m    705\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    706\u001b[0m lr \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 707\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parameters, c, ci \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mserver_controls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m(), client_controls\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m return_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m         parameters_grad_list\u001b[38;5;241m.\u001b[39mappend[parameters\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 5\n",
    "local_epochs_list = 2\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.00, 1.00]\n",
    "\n",
    "compare_id = 12\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, compare_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.0 Loading Data ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear and Initialize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cost_history_total = []\n",
    "data_time_history_total = []\n",
    "data_train_loss_history_total = []\n",
    "data_train_accuracy_history_total = []\n",
    "data_train_error_history_total = []\n",
    "data_test_loss_history_total = []\n",
    "data_test_accuracy_history_total = []\n",
    "data_test_error_history_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "load_dataset_name = \"MNIST\"\n",
    "load_aggreagte_func = \"FedAvg\"\n",
    "load_global_epochs = 100\n",
    "load_local_epochs = 20\n",
    "load_num_clients = 1000\n",
    "load_batch_size = 128\n",
    "load_train_start_time = \"2024-02-04 13.59.58\"\n",
    "load_experiment_id = 0\n",
    "data_append_load = True\n",
    "\n",
    "# Load the file\n",
    "if load_experiment_id == 0:\n",
    "    filename_load = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}.npy\".format(load_dataset_name, load_aggreagte_func, load_global_epochs, load_local_epochs, load_num_clients, load_batch_size, load_train_start_time)\n",
    "else:\n",
    "    filename_load = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}_{}.npy\".format(load_dataset_name, load_aggreagte_func, load_global_epochs, load_local_epochs, load_num_clients, load_batch_size, load_train_start_time, load_experiment_id)\n",
    "load_result = np.load(filename_load)\n",
    "print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "# Load the attributes from the file\n",
    "data_cost_history = load_result['cost_history']\n",
    "data_time_history = load_result['time_history']\n",
    "data_train_loss_history = load_result['train_loss_history']\n",
    "data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "data_train_error_history = load_result['train_error_history']\n",
    "data_test_loss_history = load_result['test_loss_history']\n",
    "data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "print(\"=======Content of the File=======\")\n",
    "print(load_result.files)\n",
    "\n",
    "print(\"=======VISUALIZATION RESULT=======\")\n",
    "plot_cost_history([data_cost_history], save=False)\n",
    "plot_time_history([data_time_history], save=False)\n",
    "plot_loss_history([data_train_loss_history], [data_test_loss_history], save=False)\n",
    "plot_accuracy_history([data_train_accuracy_history], [data_test_accuracy_history], save=False)\n",
    "plot_error_history([data_train_error_history], [data_test_error_history], save=False)\n",
    "\n",
    "print(\"=======STATUS RESULT=======\")\n",
    "print(\"Cost History: \", data_cost_history)\n",
    "print(\"Time History: \", data_time_history)\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"Train Loss History: \", data_train_loss_history)\n",
    "print(\"Train Accuracy History: \", data_train_accuracy_history)\n",
    "print(\"Train Error History: \", data_train_error_history)\n",
    "\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"Test Loss History: \", data_test_loss_history)\n",
    "print(\"Test Accuracy History: \", data_test_accuracy_history)\n",
    "print(\"Test Error History: \", data_test_error_history)\n",
    "\n",
    "# Append the data\n",
    "if data_append_load:\n",
    "    data_cost_history_total.append(data_cost_history)\n",
    "    data_time_history_total.append(data_time_history)\n",
    "    data_train_loss_history_total.append(data_train_loss_history)\n",
    "    data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "    data_train_error_history_total.append(data_train_error_history)\n",
    "    data_test_loss_history_total.append(data_test_loss_history)\n",
    "    data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "    data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Using Specific Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the filename_load path manually here!!\n",
    "filename_load = \"MNIST_train_NN_with_num_epochs_100_batch_size_128_lr_0.03_2024-02-04 13.59.58_1.npy\"\n",
    "data_append_load = True\n",
    "\n",
    "# Load the file\n",
    "load_result = np.load(filename_load)\n",
    "print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "# Load the attributes from the file\n",
    "#data_cost_history = load_result['cost_history']\n",
    "data_time_history = load_result['time_history']\n",
    "data_train_loss_history = load_result['train_loss_history']\n",
    "data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "data_train_error_history = load_result['train_error_history']\n",
    "data_test_loss_history = load_result['test_loss_history']\n",
    "data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "print(\"=======Content of the File=======\")\n",
    "print(load_result.files)\n",
    "\n",
    "print(\"=======VISUALIZATION RESULT=======\")\n",
    "#plot_cost_history([data_cost_history], save=False)\n",
    "plot_time_history([data_time_history], save=False)\n",
    "plot_loss_history([data_train_loss_history], [data_test_loss_history], save=False)\n",
    "plot_accuracy_history([data_train_accuracy_history], [data_test_accuracy_history], save=False)\n",
    "plot_error_history([data_train_error_history], [data_test_error_history], save=False)\n",
    "\n",
    "print(\"=======STATUS RESULT=======\")\n",
    "#print(\"Cost History: \", data_cost_history)\n",
    "print(\"Time History: \", data_time_history)\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"Train Loss History: \", data_train_loss_history)\n",
    "print(\"Train Accuracy History: \", data_train_accuracy_history)\n",
    "print(\"Train Error History: \", data_train_error_history)\n",
    "\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"Test Loss History: \", data_test_loss_history)\n",
    "print(\"Test Accuracy History: \", data_test_accuracy_history)\n",
    "print(\"Test Error History: \", data_test_error_history)\n",
    "\n",
    "# Append the data\n",
    "if data_append_load:\n",
    "    #data_cost_history_total.append(data_cost_history)\n",
    "    data_time_history_total.append(data_time_history)\n",
    "    data_train_loss_history_total.append(data_train_loss_history)\n",
    "    data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "    data_train_error_history_total.append(data_train_error_history)\n",
    "    data_test_loss_history_total.append(data_test_loss_history)\n",
    "    data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "    data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Using Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables manually here!!\n",
    "# load_loop_max correspond to number of files you want to load\n",
    "load_dataset_name_list = [\"MNIST\"]\n",
    "load_aggreagte_func_list = [\"FedAvg\"]\n",
    "load_global_epochs_list = [2]\n",
    "load_local_epochs_list = [1]\n",
    "load_num_clients_list = [2]\n",
    "load_batch_size_list = [128]\n",
    "load_train_start_time_list = [\"2023-12-04 19.48.18\"]\n",
    "load_loop_max = 1\n",
    "data_append_load = True\n",
    "\n",
    "for n in range(load_loop_max):\n",
    "    # Load the file\n",
    "    filename_load = \"{}_{}_with_global_epochs_{}_local_epochs_{}_num_clients_{}_batch_size_{}_{}.npy\".format(load_dataset_name_list[n], load_aggreagte_func_list[n], load_global_epochs_list[n], load_local_epochs_list[n], load_num_clients_list[n], load_batch_size_list[n], load_train_start_time_list[n])\n",
    "    load_result = np.load(filename_load)\n",
    "    print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "    # Load the attributes from the file\n",
    "    data_cost_history = load_result['cost_history']\n",
    "    data_time_history = load_result['time_history']\n",
    "    data_train_loss_history = load_result['train_loss_history']\n",
    "    data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "    data_train_error_history = load_result['train_error_history']\n",
    "    data_test_loss_history = load_result['test_loss_history']\n",
    "    data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "    data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "    # Append the data\n",
    "    if data_append_load:\n",
    "        data_cost_history_total.append(data_cost_history)\n",
    "        data_time_history_total.append(data_time_history)\n",
    "        data_train_loss_history_total.append(data_train_loss_history)\n",
    "        data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "        data_train_error_history_total.append(data_train_error_history)\n",
    "        data_test_loss_history_total.append(data_test_loss_history)\n",
    "        data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "        data_test_error_history_total.append(data_test_error_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.1 Data Visualization ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize All Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_history(data_cost_history_total, save=False)\n",
    "plot_time_history(data_time_history_total, save=False)\n",
    "plot_loss_history(data_train_loss_history_total, data_test_loss_history_total, save=False)\n",
    "plot_accuracy_history(data_train_accuracy_history_total, data_test_accuracy_history_total, save=False)\n",
    "plot_error_history(data_train_error_history_total, data_test_error_history_total, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment in Centralized Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_list = [1, 2, 3, 4, 5, 10]\n",
    "\n",
    "plot_different_parameter_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_parameter_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_parameter_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_parameter_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History in Centralized Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_parameter_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_parameter_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_parameter_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History in Centralized Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_parameter_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_parameter_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History\")\n",
    "for i, plot_test_error_history in enumerate(plot_different_parameter_test_error_history):\n",
    "    plt.plot(plot_test_error_history, label=f\"Test Error History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History in Centralized Training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Local Updates Epochs Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_local_update_epochs_list = [1, 2, 3, 4, 5, 10]\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "#for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "#    plt.plot(plot_test_loss_history, label=f\"Test Loss History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_loss_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "#for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "#    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_accuracy_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "#for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "#    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with local epochs = {plot_local_update_epochs_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different local update epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_error_history_compare_local_update_epochs_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Number of Clients Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num_clients_list = [1, 5, 10, 20]\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with number of clients = {plot_num_clients_list[i]}\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different number of clients\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_loss_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different number of clients\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_accuracy_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with number of clients = {plot_num_clients_list[i]}\")\n",
    "for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with number of clients = {plot_num_clients_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different number of clients\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_error_history_compare_number_of_clients_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Batch Size Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_size_list = [1, 5, 10, 20]\n",
    "\n",
    "plot_different_time_history = convert_to_list(data_time_history_total)\n",
    "for i, plot_time_history in enumerate(plot_different_time_history):\n",
    "    plt.plot(plot_time_history, label=f\"Time History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Culminative Time Used\")\n",
    "plt.title(\"Time History\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_time_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with batch size = {plot_batch_size_list[i]}\")\n",
    "for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "    plt.plot(plot_test_loss_history, label=f\"Test Loss History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different batch size\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_loss_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different batch size\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_accuracy_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with batch size = {plot_batch_size_list[i]}\")\n",
    "for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with batch size = {plot_batch_size_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different batch size\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_error_history_compare_batch_size_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Mu Rate Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mu_list = [0.00, 1.00]\n",
    "\n",
    "plot_different_time_history = convert_to_list(data_time_history_total)\n",
    "for i, plot_time_history in enumerate(plot_different_time_history):\n",
    "    plt.plot(plot_time_history, label=f\"Time History with mu = {plot_mu_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Culminative Time Used\")\n",
    "plt.title(\"Time History\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_time_history_compare_mu_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_local_epoch_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "for i, plot_train_loss_history in enumerate(plot_different_local_epoch_train_loss_history):\n",
    "    plt.plot(plot_train_loss_history, label=f\"Train Loss History with mu = {plot_mu_list[i]}\")\n",
    "#for i, plot_test_loss_history in enumerate(plot_different_local_epoch_test_loss_history):\n",
    "#    plt.plot(plot_test_loss_history, label=f\"Test Loss History with mu = {plot_mu_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with different mu\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_loss_history_compare_mu_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_local_epoch_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "for i, plot_train_accuracy_history in enumerate(plot_different_local_epoch_train_accuracy_history):\n",
    "    plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with mu = {plot_mu_list[i]}\")\n",
    "#for i, plot_test_accuracy_history in enumerate(plot_different_local_epoch_test_accuracy_history):\n",
    "#    plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History with mu = {plot_mu_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with different mu\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_accuracy_history_compare_mu_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_local_epoch_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_local_epoch_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "for i, plot_train_error_history in enumerate(plot_different_local_epoch_train_error_history):\n",
    "    plt.plot(plot_train_error_history, label=f\"Train Error History with mu = {plot_mu_list[i]}\")\n",
    "#for i, plot_test_error_history in enumerate(plot_different_local_epoch_test_error_history):\n",
    "#    plt.plot(plot_test_error_history, label=f\"Test Accuracy History with mu = {plot_mu_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with different mu\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{current_dataset_name}_error_history_compare_mu_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Federated Learning Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Scale Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Graph Averaging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2 Analysing ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Analysis**\n",
    "\n",
    "We analysis the variance of a particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance = statistics.variance(data_cost_history)\n",
    "time_variance = statistics.variance(data_time_history)\n",
    "train_loss_variance = statistics.variance(data_train_loss_history)\n",
    "train_accuracy_variance = statistics.variance(data_train_accuracy_history)\n",
    "train_error_variance = statistics.variance(data_train_error_history)\n",
    "test_loss_variance = statistics.variance(data_test_loss_history)\n",
    "test_accuracy_variance = statistics.variance(data_test_accuracy_history)\n",
    "test_error_variance = statistics.variance(data_test_error_history)\n",
    "print(\"=======VARIANCE RESULT=======\")\n",
    "print(\"Cost Variance: \", cost_variance)\n",
    "print(\"Time Variance: \", time_variance)\n",
    "print(\"Train Loss Variance: \", train_loss_variance)\n",
    "print(\"Train Accuracy Variance: \", train_accuracy_variance)\n",
    "print(\"Train Error Variance: \", train_error_variance)\n",
    "print(\"Test Loss Variance: \", test_loss_variance)\n",
    "print(\"Test Accuracy Variance: \", test_accuracy_variance)\n",
    "print(\"Test Error Variance: \", test_error_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_variance_subset_min = 0\n",
    "analysis_variance_subset_max = len(data_cost_history) // 2\n",
    "\n",
    "cost_variance_subset = statistics.variance(data_cost_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "time_variance_subset = statistics.variance(data_time_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "print(f'=======VARIANCE RESULT IN SUBSET BETWEEN {analysis_variance_subset_min} and {analysis_variance_subset_max}=======')\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[:len(data_cost_history) // 2])\n",
    "time_variance_subset = statistics.variance(data_time_history[:len(data_time_history) // 2])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[:len(data_train_loss_history) // 2])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[:len(data_train_accuracy_history) // 2])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[:len(data_train_error_history) // 2])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[:len(data_test_loss_history) // 2])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[:len(data_test_accuracy_history) // 2])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[:len(data_test_error_history) // 2])\n",
    "print(\"=======VARIANCE FIRST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[len(data_cost_history) // 2:])\n",
    "time_variance_subset = statistics.variance(data_time_history[len(data_time_history) // 2:])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[len(data_train_loss_history) // 2:])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[len(data_train_accuracy_history) // 2:])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[len(data_train_error_history) // 2:])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[len(data_test_loss_history) // 2:])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[len(data_test_accuracy_history) // 2:])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[len(data_test_error_history) // 2:])\n",
    "print(\"=======VARIANCE LAST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
