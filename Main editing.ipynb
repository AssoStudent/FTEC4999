{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Statistical Learning Part 2 - Experiment Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 Environment Setup ##\n",
    "\n",
    "In this section, libraries, datasets and associative python programme are imported, as well as the setup of the experiment environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Packages and Libraries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jupyter in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (1.0.0)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (5.5.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (6.29.3)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (7.1.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (8.1.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (8.6.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (5.9.8)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (6.4)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (25.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (24.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (5.14.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyter) (8.22.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets->jupyter) (3.0.10)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets->jupyter) (4.0.10)\n",
      "Requirement already satisfied: pygments in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-console->jupyter) (2.17.2)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-console->jupyter) (3.0.43)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (5.10.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (2.1.5)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.10.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from notebook->jupyter) (2.25.4)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from notebook->jupyter) (0.2.4)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from notebook->jupyter) (2.13.0)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from notebook->jupyter) (4.1.5)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (306)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.0)\n",
      "Requirement already satisfied: overrides in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (7.7.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.10.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.13)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (0.5.3)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (1.7.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter) (4.3.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter) (2.2.4)\n",
      "Requirement already satisfied: tomli in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter) (2.0.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter) (0.27.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.9.24)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (4.21.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.31.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.14.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (3.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (23.2.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.34.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (0.18.0)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.4)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.0.7)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (0.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (6.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (3.3.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.1)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (1.13)\n",
      "Requirement already satisfied: isoduration in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (20.11.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.4)\n",
      "Requirement already satisfied: uri-template in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.9.0.20240316)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (0.17.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (3.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: packaging in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: namex in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: rich in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (2.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (0.17.1)\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (3.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch_xla in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com/\n",
      "Requirement already satisfied: torch-neuron in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (1.0.1522.0)\n",
      "Requirement already satisfied: torch-neuron-base in c:\\users\\s1155158397\\appdata\\roaming\\python\\python310\\site-packages (from torch-neuron) (1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pytorch'\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter\n",
    "!pip install numpy\n",
    "!pip install torch torchvision \n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install torch_xla\n",
    "!pip install torch-neuron --extra-index-url=https://pip.repos.neuron.amazonaws.com/\n",
    "!pip install pytorch torchvision cudatoolkit=9.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import random\n",
    "import string\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import os\n",
    "import csv\n",
    "#import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, ConcatDataset, Subset\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Python Programme**\n",
    "\n",
    "Here is the section for importing external python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Set up Experiment Environment ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilize GPU**\n",
    "\n",
    "For local environments, please utilize the GPU to speed up the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utililze TPU**\n",
    "\n",
    "To shorten the training time, we highly recommend that experiments should be done in Google Colab. Enable the following code in the Google Colab platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume that you are on the Google Colab platform.\n",
    "#!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "def to_device(data, device):\n",
    "    data.to(device)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.2 Datasets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.0 Preprocessing Functions ####\n",
    "Here are the functions for preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#       Normalization        #\n",
    "##############################\n",
    "def normalize_tensor(tensor):\n",
    "    mean = torch.mean(tensor)\n",
    "    std = torch.std(tensor)\n",
    "    normalized_tensor = (tensor - mean) / std\n",
    "    return normalized_tensor\n",
    "\n",
    "##############################\n",
    "#    Image Preprocessing     #\n",
    "##############################\n",
    "def load_image(filename):\n",
    "  im_pil = Image.open(filename)\n",
    "  im = np.array(im_pil).astype(np.float32) / 255\n",
    "  return im\n",
    "\n",
    "##############################\n",
    "# Text Dataset Preprocessing #\n",
    "##############################\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.1 Importing Datasets from Packages ####\n",
    "This will load the dataset automatically downloaded from the package. Remember the datasets will be stored in the folder \"Datasets\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./Datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./Datasets\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./Datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./Datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./Datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./Datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./Datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./Datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./Datasets\\MNIST\\raw\n",
      "\n",
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Download Dataset\n",
    "MNIST_train_dataset = MNIST(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_test_dataset = MNIST(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the dataset\n",
    "print(\"=== Raw Data Samples from the MNIST Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_train_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"=== Raw Data Samples from the MNIST Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = MNIST_test_dataset[i]\n",
    "    image = image.squeeze().numpy()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Download Dataset\n",
    "CIFAR10_train_dataset = CIFAR10(root='./Datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR10_test_dataset = CIFAR10(root='./Datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the CIFAR10 train dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_train_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the raw data from the CIFAR10 test dataset\n",
    "print(\"=== Raw Data Samples from the CIFAR10 Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = CIFAR10_test_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EMNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import EMNIST\n",
    "\n",
    "# Download Dataset\n",
    "EMNIST_train_dataset = EMNIST(root='./Datasets', split='byclass', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "EMNIST_test_dataset = EMNIST(root='./Datasets', split='byclass', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "# Load Dataset\n",
    "train_dataset = EMNIST_train_dataset\n",
    "test_dataset = EMNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"EMNIST\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data from the EMNIST train dataset\n",
    "print(\"=== Raw Data Samples from the EMNIST Train Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = EMNIST_train_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the raw data from the EMNIST test dataset\n",
    "print(\"=== Raw Data Samples from the EMNIST Test Dataset ===\")\n",
    "for i in range(3):\n",
    "    image, label = EMNIST_test_dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 0.2.2 Importing Datasets from Downloaded Files ####\n",
    "This will load the dataset downloaded in the local directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**\n",
    "\n",
    "The source of this dataset comes from https://www.kaggle.com/c/GiveMeSomeCredit\n",
    "\n",
    "Note: This dataset is borrowed from the datasets used in the competitive task in FTEC2101 Optimization Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Structure\n",
    "class Give_Me_Some_Credit_Dataset_Class(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for _, row in data_frame.iterrows():\n",
    "            features = row.iloc[:-1].values.astype(np.float32)\n",
    "            label = row.iloc[-1]\n",
    "            data.append(features)\n",
    "            targets.append(float(int(label)))\n",
    "        \n",
    "        data = torch.tensor(data)\n",
    "        targets = torch.tensor(targets)\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Give_Me_Some_Credit_train_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-train.csv', train=True, transform=normalize_tensor)\n",
    "Give_Me_Some_Credit_test_dataset = Give_Me_Some_Credit_Dataset_Class(root='./Datasets/Give_Me_Some_Credit/ftec-cs-full-test.csv', train=False, transform=normalize_tensor)\n",
    "\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**\n",
    "\n",
    "Note: This dataset is best suited for binary classification. The training dataset contains 400000 objects. Each object is described by 2001 columns. The first column contains the label value, all other columns contain numerical features. The validation dataset contains 100000 objects. The structure is identical to the training dataset.\n",
    "\n",
    "Warning: The loading time for this dataset is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Structure\n",
    "class EpsilonDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        line = line.split(' ')\n",
    "        label, values = int(line[0]), line[1:]\n",
    "        value = torch.zeros(line[1:].size())\n",
    "        for item in values:\n",
    "            idx, val = item.split(':')\n",
    "            value[int(idx) - 1] = float(val)\n",
    "        return label, value\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root, nrows=20000)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(self.root, 'r') as fp:\n",
    "            for line in fp:\n",
    "                label, value = self.process_line(line.strip(\"\\n\"))\n",
    "                data.append(value)\n",
    "                targets.append(label)\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "# Load Dataset\n",
    "Epsilon_train_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized', train=True, transform=None)\n",
    "Epsilon_test_dataset = EpsilonDataset(root='./Datasets/epsilon/epsilon_normalized.t', train=False, transform=None)\n",
    "\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Languages Dataset**\n",
    "\n",
    "The dataset is downloaded from here: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('Datasets/Language_dataset/names/*.txt'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "languages_dataset_category_lines = {}\n",
    "languages_dataset_all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('Datasets/Language_dataset/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages_dataset_all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    languages_dataset_category_lines[category] = lines\n",
    "\n",
    "n_categories = len(languages_dataset_all_categories)\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data, self.targets = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_frame = pd.read_csv(self.root, nrows=20000)\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        with open(self.root, 'r') as fp:\n",
    "            for line in fp:\n",
    "                label, value = self.process_line(line.strip(\"\\n\"))\n",
    "                data.append(value)\n",
    "                targets.append(label)\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "        label = self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "Languages_train_dataset = LanguageDataset(root='./Datasets/epsilon/epsilon_normalized', train=True, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Classes, Functions and Algorithms ##\n",
    "All common and helping functions for machine learning tasks are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#     Helping Functions    #\n",
    "############################\n",
    "# Random Seed Function\n",
    "# To ensure a same training result under the random process, you might need to set the random seed via this function.\n",
    "def set_random_seed(custom_random_seed):\n",
    "    torch.manual_seed(custom_random_seed)\n",
    "    random.seed(custom_random_seed)\n",
    "    np.random.seed(custom_random_seed)\n",
    "\n",
    "# Convert anything into a list if input is not a list\n",
    "def convert_to_list(input_list):\n",
    "    if not isinstance(input_list, list):\n",
    "        input_list = [input_list]\n",
    "    return input_list\n",
    "\n",
    "# Graph Plotting Functions\n",
    "def plot_cost_history(cost_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Cost\", title_name=\"Culminative Send Cost History\"):\n",
    "    cost_history_list = convert_to_list(cost_history_list)\n",
    "    for i, cost_history in enumerate(cost_history_list):\n",
    "        plt.plot(cost_history, label=f\"Cost History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(cost_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_culminative_send_cost_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_history(time_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Culminative Time Used\", title_name=\"Time History\"):\n",
    "    time_history_list = convert_to_list(time_history_list)\n",
    "    for i, time_history in enumerate(time_history_list):\n",
    "        plt.plot(time_history, label=f\"Time History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if len(time_history_list) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_time_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(train_loss_history_list=[], test_loss_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Loss\", title_name=\"Loss History\"):\n",
    "    train_loss_history_list = convert_to_list(train_loss_history_list)\n",
    "    test_loss_history_list = convert_to_list(test_loss_history_list)\n",
    "    for i, train_loss_history in enumerate(train_loss_history_list):\n",
    "        plt.plot(train_loss_history, label=f\"Train Loss History {i+1}\")\n",
    "    for i, test_loss_history in enumerate(test_loss_history_list):\n",
    "        plt.plot(test_loss_history, label=f\"Test Loss History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_loss_history_list) + len(test_loss_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_loss_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history(train_accuracy_history_list=[], test_accuracy_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Accuracy\", title_name=\"Accuracy History\"):\n",
    "    train_accuracy_history_list = convert_to_list(train_accuracy_history_list)\n",
    "    test_accuracy_history_list = convert_to_list(test_accuracy_history_list)\n",
    "    for i, train_accuracy_history in enumerate(train_accuracy_history_list):\n",
    "        plt.plot(train_accuracy_history, label=f\"Train Accuracy History {i+1}\")\n",
    "    for i, test_accuracy_history in enumerate(test_accuracy_history_list):\n",
    "        plt.plot(test_accuracy_history, label=f\"Test Accuracy History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_accuracy_history_list) + len(test_accuracy_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_accuracy_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history(train_error_history_list=[], test_error_history_list=[], save=True, x_label_name=\"Epochs\", y_label_name=\"Error\", title_name=\"Error History\"):\n",
    "    train_error_history_list = convert_to_list(train_error_history_list)\n",
    "    test_error_history_list = convert_to_list(test_error_history_list)\n",
    "    for i, train_error_history in enumerate(train_error_history_list):\n",
    "        plt.plot(train_error_history, label=f\"Train Error History {i+1}\")\n",
    "    for i, test_error_history in enumerate(test_error_history_list):\n",
    "        plt.plot(test_error_history, label=f\"Test Error History {i+1}\")\n",
    "    plt.xlabel(x_label_name)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.title(title_name)\n",
    "    if (len(train_error_history_list) + len(test_error_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(f'{current_dataset_name}_error_history_{train_start_time}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Accuracy and Error Rate Calculation\n",
    "def get_accuracy(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "def get_error(outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        if outputs.dim() > 1:\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "        else:\n",
    "            predictions = outputs\n",
    "        return torch.tensor(torch.sum(predictions != labels).item() / len(predictions))\n",
    "\n",
    "def relative_rate_to_client_number(num_client, percentage = 1.00):\n",
    "    return round(num_client * percentage)\n",
    "\n",
    "############################\n",
    "#   Neural Network Model   #\n",
    "############################\n",
    "global Linear_Model_in_features\n",
    "global Linear_Model_out_features\n",
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=Linear_Model_in_features, out_features=Linear_Model_out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "class MNIST_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 7 * 7, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        if len(x.size()) == 1:  # Check if output has a single level of brackets\n",
    "            x = x.unsqueeze(0)  # Add a dimension at the beginning\n",
    "        return x\n",
    "\n",
    "class CIFAR10_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_stack = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "\n",
    "            torch.nn.Flatten(), \n",
    "            torch.nn.Linear(256*4*4, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation_stack(x)\n",
    "        if len(x.size()) == 1:  # Check if output has a single level of brackets\n",
    "            x = x.unsqueeze(0)  # Add a dimension at the beginning\n",
    "        return x\n",
    "\n",
    "class EMNIST_CNN_Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 128)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc2 = torch.nn.Linear(128, 26)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        if len(x.size()) == 1:  # Check if output has a single level of brackets\n",
    "            x = x.unsqueeze(0)  # Add a dimension at the beginning\n",
    "        return x\n",
    "\n",
    "class RNN_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation_stack = torch.nn.Sequential(\n",
    "            torch.nn.RNN()\n",
    "        )\n",
    "\n",
    "############################\n",
    "#    Iterate Algorithm     #\n",
    "############################\n",
    "def evaluate_model_simple(model, dataloader, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "    return loss_average, accuracy_average, error_average\n",
    "\n",
    "def iterate_model_simple(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, test_dataloader=None, include_intial_history=False):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    if include_intial_history is True:\n",
    "        loss, accuracy, error = evaluate_model_simple(model=model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        loss_history.append(loss)\n",
    "        accuracy_history.append(accuracy)\n",
    "        error_history.append(error)\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss_average:.16f}, Train Accuracy: {accuracy_average:.16f}, Train Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "    if test_dataloader is not None:\n",
    "        return loss_history, accuracy_history, error_history, time_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "##################################################################\n",
    "#  Dataset Preprocessing Functions Before Splitting For Clients  #\n",
    "##################################################################\n",
    "# Acknowledge from https://github.com/adap/flower/blob/main/baselines/fedprox/fedprox/dataset_preparation.py\n",
    "# Balance: Trims the dataset so each class contains as many elements as the class that contained the least elements.\n",
    "def dataset_balance_classes(trainset, seed=42):\n",
    "    class_counts = np.bincount(trainset.targets)\n",
    "    smallest = np.min(class_counts)\n",
    "    idxs = trainset.targets.argsort()\n",
    "    tmp = [Subset(trainset, idxs[: int(smallest)])]\n",
    "    tmp_targets = [trainset.targets[idxs[: int(smallest)]]]\n",
    "    for count in np.cumsum(class_counts):\n",
    "        tmp.append(Subset(trainset, idxs[int(count) : int(count + smallest)]))\n",
    "        tmp_targets.append(trainset.targets[idxs[int(count) : int(count + smallest)]])\n",
    "    unshuffled = ConcatDataset(tmp)\n",
    "    unshuffled_targets = torch.cat(tmp_targets)\n",
    "    shuffled_idxs = torch.randperm(\n",
    "        len(unshuffled), generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    shuffled = Subset(unshuffled, shuffled_idxs)\n",
    "    shuffled.targets = unshuffled_targets[shuffled_idxs]\n",
    "    return shuffled\n",
    "\n",
    "def dataset_sort_by_class(trainset: Dataset):\n",
    "    class_counts = np.bincount(trainset.targets)\n",
    "    idxs = trainset.targets.argsort()  # sort targets in ascending order\n",
    "\n",
    "    tmp = []  # create subset of smallest class\n",
    "    tmp_targets = []  # same for targets\n",
    "\n",
    "    start = 0\n",
    "    for count in np.cumsum(class_counts):\n",
    "        tmp.append(\n",
    "            Subset(trainset, idxs[start : int(count + start)])\n",
    "        )  # add rest of classes\n",
    "        tmp_targets.append(trainset.targets[idxs[start : int(count + start)]])\n",
    "        start += count\n",
    "    sorted_dataset = ConcatDataset(tmp)  # concat dataset\n",
    "    sorted_dataset.targets = torch.cat(tmp_targets)  # concat targets\n",
    "    return sorted_dataset\n",
    "\n",
    "# Implemention follow Li et al 2020: https://arxiv.org/abs/1812.06127 with default values set accordingly.\n",
    "global custom_power_law_num_labels_per_partition\n",
    "global custom_power_law_min_data_per_partition\n",
    "global custom_power_law_mean\n",
    "global custom_power_law_sigma\n",
    "custom_power_law_num_labels_per_partition = 2\n",
    "custom_power_law_min_data_per_partition = 10\n",
    "custom_power_law_mean = 0.0\n",
    "custom_power_law_sigma = 2.0\n",
    "def dataset_power_law_split(sorted_trainset, num_partitions):\n",
    "    # Custom Parameters\n",
    "    num_labels_per_partition = custom_power_law_num_labels_per_partition\n",
    "    min_data_per_partition = custom_power_law_min_data_per_partition\n",
    "    mean = custom_power_law_mean\n",
    "    sigma = custom_power_law_sigma\n",
    "\n",
    "    targets = sorted_trainset.targets\n",
    "    full_idx = list(range(len(targets)))\n",
    "\n",
    "    class_counts = np.bincount(sorted_trainset.targets)\n",
    "    labels_cs = np.cumsum(class_counts)\n",
    "    labels_cs = [0] + labels_cs[:-1].tolist()\n",
    "\n",
    "    partitions_idx: List[List[int]] = []\n",
    "    num_classes = len(np.bincount(targets))\n",
    "    hist = np.zeros(num_classes, dtype=np.int32)\n",
    "\n",
    "    # assign min_data_per_partition\n",
    "    min_data_per_class = int(min_data_per_partition / num_labels_per_partition)\n",
    "    for u_id in range(num_partitions):\n",
    "        partitions_idx.append([])\n",
    "        for cls_idx in range(num_labels_per_partition):\n",
    "            # label for the u_id-th client\n",
    "            cls = (u_id + cls_idx) % num_classes\n",
    "            # record minimum data\n",
    "            indices = list(\n",
    "                full_idx[\n",
    "                    labels_cs[cls]\n",
    "                    + hist[cls] : labels_cs[cls]\n",
    "                    + hist[cls]\n",
    "                    + min_data_per_class\n",
    "                ]\n",
    "            )\n",
    "            partitions_idx[-1].extend(indices)\n",
    "            hist[cls] += min_data_per_class\n",
    "\n",
    "    # add remaining images following power-law\n",
    "    probs = np.random.lognormal(\n",
    "        mean,\n",
    "        sigma,\n",
    "        (num_classes, int(num_partitions / num_classes), num_labels_per_partition),\n",
    "    )\n",
    "    remaining_per_class = class_counts - hist\n",
    "    # obtain how many samples each partition should be assigned for each of the\n",
    "    # labels it contains\n",
    "    # pylint: disable=too-many-function-args\n",
    "    probs = (\n",
    "        remaining_per_class.reshape(-1, 1, 1)\n",
    "        * probs\n",
    "        / np.sum(probs, (1, 2), keepdims=True)\n",
    "    )\n",
    "\n",
    "    for u_id in range(num_partitions):\n",
    "        for cls_idx in range(num_labels_per_partition):\n",
    "            cls = (u_id + cls_idx) % num_classes\n",
    "            count = int(probs[cls, u_id // num_classes, cls_idx])\n",
    "\n",
    "            # add count of specific class to partition\n",
    "            indices = full_idx[\n",
    "                labels_cs[cls] + hist[cls] : labels_cs[cls] + hist[cls] + count\n",
    "            ]\n",
    "            partitions_idx[u_id].extend(indices)\n",
    "            hist[cls] += count\n",
    "\n",
    "    # construct subsets\n",
    "    partitions = [Subset(sorted_trainset, p) for p in partitions_idx]\n",
    "    return partitions\n",
    "\n",
    "# Distribute the training datasets to clients, remember it returns an array of datasets\n",
    "def split_datasets_for_clients_random(dataset, num_clients=1):\n",
    "    total_sample_size = len(dataset)\n",
    "    samples_per_clients = total_sample_size // num_clients\n",
    "    client_datasets = random_split(dataset, [min(i + samples_per_clients, total_sample_size) - i for i in range(0, total_sample_size, samples_per_clients)])\n",
    "    return client_datasets\n",
    "\n",
    "global custom_split_dataset_iid\n",
    "global custom_split_dataset_power_law\n",
    "global custom_split_dataset_balance\n",
    "global custom_split_dataset_seed\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "def split_datasets_for_clients_custom(dataset, num_clients=1):\n",
    "    # Custom Parameters\n",
    "    iid=custom_split_dataset_iid\n",
    "    power_law=custom_split_dataset_power_law\n",
    "    balance=custom_split_dataset_balance\n",
    "    seed=custom_split_dataset_seed\n",
    "\n",
    "    trainset = dataset\n",
    "    if balance:\n",
    "        trainset = dataset_balance_classes(trainset, seed)\n",
    "\n",
    "    partition_size = int(len(trainset) / num_clients)\n",
    "    lengths = [partition_size] * num_clients\n",
    "\n",
    "    if iid is True:\n",
    "        client_datasets = random_split(trainset, lengths, torch.Generator().manual_seed(seed))\n",
    "    else:\n",
    "        if power_law is True:\n",
    "            trainset_sorted = dataset_sort_by_class(trainset)\n",
    "            client_datasets = dataset_power_law_split(\n",
    "                trainset_sorted,\n",
    "                num_partitions=num_clients,\n",
    "            )\n",
    "        else:\n",
    "            shard_size = int(partition_size / 2)\n",
    "            idxs = trainset.targets.argsort()\n",
    "            sorted_data = Subset(trainset, idxs)\n",
    "            tmp = []\n",
    "            for idx in range(num_clients * 2):\n",
    "                tmp.append(\n",
    "                    Subset(\n",
    "                        sorted_data, np.arange(shard_size * idx, shard_size * (idx + 1))\n",
    "                    )\n",
    "                )\n",
    "            idxs_list = torch.randperm(\n",
    "                num_clients * 2, generator=torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            client_datasets = [\n",
    "                ConcatDataset((tmp[idxs_list[2 * i]], tmp[idxs_list[2 * i + 1]]))\n",
    "                for i in range(num_clients)\n",
    "            ]\n",
    "\n",
    "    return client_datasets\n",
    "\n",
    "############################\n",
    "#      Client Devices      #\n",
    "############################\n",
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, client_id, model, optimizer, dataset, batch_size, iterate_func, loss_func, accuracy_func=get_accuracy, error_func=get_error):\n",
    "        self.id = client_id\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DeviceDataLoader(torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        self.iterate_func = iterate_func\n",
    "        self.loss_func = loss_func\n",
    "        self.accuracy_func = accuracy_func\n",
    "        self.error_func = error_func\n",
    "\n",
    "        # Framework Specificed Variables\n",
    "        self.straggler = False\n",
    "        self.client_controls = {}\n",
    "        self.full_batch_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False), device)\n",
    "\n",
    "        # Custom Variables\n",
    "        self.adversary = False\n",
    "        self.adversary_attack_func = None\n",
    "        self.adversary_attack_value = 0\n",
    "        self.adversary_attack_Scaffold_all = False\n",
    "\n",
    "    def load_weights(self, weights):\n",
    "        self.model.load_state_dict(weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def get_client_id(self):\n",
    "        return self.id\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def save_local_history(self, num_epochs, loss_history, accuracy_history, error_history, time_history, value=train_start_time):\n",
    "        filename = \"{}_client_{}_with_local_epochs_{}_local_loss_accuracy_error_history_{}.npy\".format(current_dataset_name, self.id, num_epochs, value)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, loss_history=loss_history, accuracy_history=accuracy_history, error_history=error_history, time_history=time_history)\n",
    "\n",
    "    def train(self, num_epochs, show_message=True, plot_history=False):\n",
    "        if self.adversary is True:\n",
    "            if show_message:\n",
    "                print(f\"!-- Client {self.id} is a adversary and start iterations. ---!\")\n",
    "            self.adversary_attack_func(self.model, self.adversary_attack_value, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func)\n",
    "        else:\n",
    "            if self.straggler is True:\n",
    "                if show_message:\n",
    "                    print(f\"!-- Client {self.id} is a straggler and start iterations. ---!\")\n",
    "                loss_history, accuracy_history, error_history, time_history = self.iterate_func(self.model, self.dataloader, random.randint(1, num_epochs), self.optimizer, self.loss_func, self.accuracy_func, self.error_func)\n",
    "            else:\n",
    "                if show_message:\n",
    "                    print(f\"!-- Client {self.id} is normal and start iterations. ---!\")\n",
    "                loss_history, accuracy_history, error_history, time_history = self.iterate_func(self.model, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func)\n",
    "        if plot_history:\n",
    "            plot_time_history(time_history)\n",
    "            plot_loss_history(loss_history)\n",
    "            plot_accuracy_history(accuracy_history)\n",
    "            plot_error_history(error_history)\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    ## == Framework Specificed Functions == ##\n",
    "    def is_straggler(self):\n",
    "        return self.straggler\n",
    "\n",
    "    def get_local_client_controls(self):\n",
    "        return self.client_controls\n",
    "\n",
    "    def train_Scaffold(self, server_controls, num_epochs, Scaffold_update_controls_use_gradient, show_message=True, plot_history=False):\n",
    "        if self.adversary is True:\n",
    "            if show_message:\n",
    "                print(f\"!-- Client {self.id} is a adversary and start iterations. ---!\")\n",
    "            delta_weights, delta_client_controls = adversarial_attack_by_value_Scaffold(self, self.adversary_attack_value, server_controls, self.dataloader, num_epochs, self.adversary_attack_Scaffold_all, self.loss_func, self.accuracy_func, self.error_func, Scaffold_update_controls_use_gradient)\n",
    "        else:\n",
    "            if self.straggler is True:\n",
    "                if show_message:\n",
    "                    print(f\"!-- Client {self.id} is a straggler and start iterations. ---!\")\n",
    "                loss_history, accuracy_history, error_history, time_history, delta_weights, delta_client_controls = iterate_Scaffold_client(self, server_controls, self.dataloader, random.randint(1, num_epochs), self.optimizer, self.loss_func, self.accuracy_func, self.error_func, Scaffold_update_controls_use_gradient)\n",
    "            else:\n",
    "                if show_message:\n",
    "                    print(f\"!-- Client {self.id} is normal and start iterations. ---!\")\n",
    "                loss_history, accuracy_history, error_history, time_history, delta_weights, delta_client_controls = iterate_Scaffold_client(self, server_controls, self.dataloader, num_epochs, self.optimizer, self.loss_func, self.accuracy_func, self.error_func, Scaffold_update_controls_use_gradient)\n",
    "        if plot_history:\n",
    "            plot_time_history(time_history)\n",
    "            plot_loss_history(loss_history)\n",
    "            plot_accuracy_history(accuracy_history)\n",
    "            plot_error_history(error_history)\n",
    "        return delta_weights, delta_client_controls\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, model_list, optimizer_list, dataset_list, batch_size_list, iterate_func_list, loss_func_list, accuracy_func_list, error_func_list):\n",
    "    client_device = [None] * num_clients\n",
    "    for client_id in range(num_clients):\n",
    "        client_device[client_id] = ClientDevice(client_id, model_list[client_id], optimizer_list[client_id], dataset_list[client_id], batch_size_list[client_id], iterate_func_list[client_id], loss_func_list[client_id], accuracy_func_list[client_id], error_func_list[client_id])\n",
    "    return client_device\n",
    "\n",
    "############################\n",
    "#    Adversarial Attack    #\n",
    "############################\n",
    "def adversarial_attack_by_value(model, value, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            m.weight.data.uniform_(-value, value)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.uniform_(-value, value)\n",
    "        elif isinstance(m, torch.nn.Linear):\n",
    "            m.weight.data.uniform_(-value, value)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.uniform_(-value, value)\n",
    "    loss, accuracy, error = evaluate_model_simple(model=model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    print(f'Loss: {loss:.16f}, Accuracy: {accuracy:.16f}, Test Error: {error:.16f}')\n",
    "\n",
    "def adversarial_attack_by_value_dull(model, value, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    for parameters in model.parameters():\n",
    "        parameters.data.fill_(value)\n",
    "    loss, accuracy, error = evaluate_model_simple(model=model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    print(f'Loss: {loss:.16f}, Accuracy: {accuracy:.16f}, Test Error: {error:.16f}')\n",
    "\n",
    "def adversarial_attack_by_train_scaling(model, scale_value, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_average:.16f}, Accuracy: {accuracy_average:.16f}, Error: {error_average:.16f}')\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "            m.weight.data *= scale_value\n",
    "            if m.bias is not None:\n",
    "                m.bias.data *= scale_value\n",
    "    loss, accuracy,error = evaluate_model_simple(model=model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    print(f'After scaling weight with {scale_value}, Loss: {loss:.16f}, Accuracy: {accuracy:.16f}, Test Error: {error:.16f}')\n",
    "\n",
    "def adversarial_attack_by_value_Scaffold(client, value, server_controls, dataloader, num_epochs, all_bool, loss_func, accuracy_func, error_func, Scaffold_update_controls_use_gradient):\n",
    "    local_model = client.model\n",
    "    delta_weights = {}\n",
    "    delta_client_controls = {}\n",
    "    if all_bool is False:\n",
    "        client_controls = client.client_controls\n",
    "        client_controls_update = {}\n",
    "        old_model = copy.deepcopy(local_model)\n",
    "        temp = {}\n",
    "        for name, parameters in local_model.named_parameters():\n",
    "            parameters.data.fill_(value)\n",
    "            temp[name] = parameters.data.clone()\n",
    "        if Scaffold_update_controls_use_gradient is True:\n",
    "            old_model.zero_grad()\n",
    "            features, labels = next(iter(client.full_batch_dataloader))\n",
    "            outputs = old_model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            for name, parameters in old_model.named_parameters():\n",
    "                client_controls_update[name] = parameters.grad\n",
    "                delta_weights[name] = temp[name] - parameters.data\n",
    "                delta_client_controls[name] = client_controls_update[name] - client_controls[name]\n",
    "        else:\n",
    "            lr = client.optimizer.get_step_size()\n",
    "            for name, parameters in old_model.named_parameters():\n",
    "                client_controls_update[name] = client_controls[name] - server_controls[name] + (parameters.data - temp[name]) / (num_epochs * len(dataloader) * lr)\n",
    "                delta_weights[name] = temp[name] - parameters.data\n",
    "                delta_client_controls[name] = client_controls_update[name] - client_controls[name]\n",
    "        client.client_controls = client_controls_update\n",
    "    else:\n",
    "        for name, parameters in local_model.named_parameters():\n",
    "            parameters.data.fill_(value)\n",
    "            delta_weights[name] = parameters.data\n",
    "            delta_client_controls[name] = parameters.data\n",
    "    loss, accuracy,error = evaluate_model_simple(model=local_model, dataloader=dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    print(f'Loss: {loss:.16f}, Accuracy: {accuracy:.16f}, Test Error: {error:.16f}')\n",
    "    return delta_weights, delta_client_controls\n",
    "\n",
    "#########################################\n",
    "#     Federated Learning Algorithms     #\n",
    "#########################################\n",
    "def federated_averaging_old(client_weights_total):\n",
    "    subset_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "    print(client_weights_total)\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / subset_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / subset_clients\n",
    "\n",
    "    print(\"zzz\")\n",
    "    print(aggregate_weights)\n",
    "    return aggregate_weights\n",
    "\n",
    "def federated_averaging(global_model, client_weights_total):\n",
    "    random_sample_client_number = len(client_weights_total)\n",
    "\n",
    "    aggregated_weights = {}\n",
    "    i = 0\n",
    "    for name, parameters in global_model.named_parameters():\n",
    "        aggregated_weights[name] = torch.zeros_like(parameters.data)\n",
    "        if i == 0:\n",
    "            print(aggregated_weights[name])\n",
    "            i += 1\n",
    "\n",
    "    for model_weights in client_weights_total:\n",
    "        i = 0\n",
    "        for name, parameters in global_model.named_parameters():\n",
    "            aggregated_weights[name] += model_weights[name] / random_sample_client_number\n",
    "            if i == 0:\n",
    "                print(\"XXX\")\n",
    "                print(model_weights[name])\n",
    "                print(\"YYY\")\n",
    "                print(aggregated_weights[name])\n",
    "                i += 1\n",
    "\n",
    "    for name, parameters in global_model.named_parameters():\n",
    "        parameters.data = aggregated_weights[name].data\n",
    "\n",
    "def federated_median(client_weights_total):\n",
    "    subset_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights.clone()\n",
    "    print(aggregate_weights[1])\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] = torch.cat((aggregate_weights[layer_name], layer_weights))\n",
    "        print(aggregate_weights[1])\n",
    "\n",
    "    print(\"zzz\")\n",
    "    print(aggregate_weights)\n",
    "\n",
    "    median_weights = {}\n",
    "    for layer_name, layer_weights in aggregate_weights.items():\n",
    "        median_shape = layer_weights.shape\n",
    "        median_weights[layer_name] = torch.median(layer_weights.view(subset_clients, *median_shape), dim=0).values\n",
    "\n",
    "    return median_weights\n",
    "\n",
    "def iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    # Initial Loss, Accuracy, Error\n",
    "    train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    train_error_history.append(train_error)\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_accuracy_history.append(test_accuracy)\n",
    "        test_error_history.append(test_error)\n",
    "\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        client_weights_total = []\n",
    "        random_client_list = random.sample(client_list, random_sample_client_number)\n",
    "        for client in random_client_list:\n",
    "            send_cost += sum(value.numel() for value in global_weights.values())\n",
    "            client.load_weights(global_weights)\n",
    "            client_weights = client.train(num_epochs=local_epochs)\n",
    "            client_weights_total.append(client_weights)\n",
    "            send_cost += sum(value.numel() for value in client_weights.values())\n",
    "        aggregate_func(global_model, client_weights_total)\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(\"!-- Server Model Status --!\")\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "#################################\n",
    "#  FedProx Framework Algorithm  #\n",
    "#################################\n",
    "def iterate_model_FedProx(model, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = model(features)\n",
    "            proximal_term = 0.0\n",
    "            for w, w_t in zip(model.parameters(), FedProx_global_model.parameters()):\n",
    "                proximal_term += torch.square((w - w_t).norm(2))\n",
    "            loss = loss_func(outputs, labels) + (FedProx_mu / 2) * proximal_term\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    return loss_history, accuracy_history, error_history, time_history\n",
    "\n",
    "##################################\n",
    "#  SCAFFOLD Framework Algorithm  #\n",
    "##################################\n",
    "# Inspired by https://github.com/ki-ljl/Scaffold-Federated-Learning/blob/main/ScaffoldOptimizer.py\n",
    "# c: server_controls, ci: client_controls\n",
    "class ScaffoldOptimizer(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=None):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay is not None and weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super(ScaffoldOptimizer, self).__init__(params, defaults)\n",
    "                \n",
    "    def step(self, server_controls, client_controls):\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "            for parameters, c, ci in zip(group['params'], server_controls.values(), client_controls.values()):\n",
    "                if parameters.grad is None:\n",
    "                    continue\n",
    "                parameters_derivative = parameters.grad.data - ci.data + c.data\n",
    "                if weight_decay is not None:\n",
    "                    parameters.data = weight_decay * parameters.data - lr * parameters_derivative.data\n",
    "                else:\n",
    "                    parameters.data = parameters.data - lr * parameters_derivative.data\n",
    "\n",
    "    def get_step_size(self):\n",
    "        return self.param_groups[0]['lr']\n",
    "\n",
    "def iterate_Scaffold_client(client, server_controls, dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, Scaffold_update_controls_use_gradient=True, show_history=True):\n",
    "    local_model = client.model\n",
    "    old_model = copy.deepcopy(local_model)\n",
    "    client_controls = client.client_controls\n",
    "    lr = optimizer.get_step_size()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        for batch in dataloader:\n",
    "            features, labels = batch\n",
    "            outputs = local_model(features)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            accuracy = accuracy_func(outputs, labels)\n",
    "            error = error_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step(server_controls, client_controls)\n",
    "            optimizer.zero_grad()\n",
    "            loss.detach()\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            errors.append(error)\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        time_history.append(time_used)\n",
    "        loss_average = torch.stack(losses).mean().item()\n",
    "        accuracy_average = torch.stack(accuracies).mean().item()\n",
    "        error_average = torch.stack(errors).mean().item()\n",
    "        loss_history.append(loss_average)\n",
    "        accuracy_history.append(accuracy_average)\n",
    "        error_history.append(error_average)\n",
    "        if show_history:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average:.16f}, Average Accuracy: {accuracy_average:.16f}, Average Error: {error_average:.16f}, Culminative Time Used: {time_used}')\n",
    "    \n",
    "    client_controls_update = {}\n",
    "    delta_weights = {}\n",
    "    delta_client_controls = {}\n",
    "    temp = {}\n",
    "    for name, parameters in local_model.named_parameters():\n",
    "        temp[name] = parameters.data.clone()\n",
    "    if Scaffold_update_controls_use_gradient is True:\n",
    "        old_model.zero_grad()\n",
    "        features, labels = next(iter(client.full_batch_dataloader))\n",
    "        outputs = old_model(features)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        for name, parameters in old_model.named_parameters():\n",
    "            client_controls_update[name] = parameters.grad\n",
    "            delta_weights[name] = temp[name] - parameters.data\n",
    "            delta_client_controls[name] = client_controls_update[name] - client_controls[name]\n",
    "    else:\n",
    "        for name, parameters in old_model.named_parameters():\n",
    "            client_controls_update[name] = client_controls[name] - server_controls[name] + (parameters.data - temp[name]) / (num_epochs * len(dataloader) * lr)\n",
    "            delta_weights[name] = temp[name] - parameters.data\n",
    "            delta_client_controls[name] = client_controls_update[name] - client_controls[name]\n",
    "\n",
    "    client.client_controls = client_controls_update\n",
    "\n",
    "    return loss_history, accuracy_history, error_history, time_history, delta_weights, delta_client_controls\n",
    "\n",
    "def iterate_Scaffold_global(train_dataloader, test_dataloader, global_model, client_list, random_sample_client_number, global_epochs, global_step_size, local_epochs, Scaffold_update_controls_use_gradient=False, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True):\n",
    "    cost_history = []\n",
    "    time_history = []\n",
    "    start_time = timeit.default_timer()\n",
    "    send_cost = 0.00\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    train_error_history = []\n",
    "    test_loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    # Initial Loss, Accuracy, Error\n",
    "    train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    train_error_history.append(train_error)\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        test_loss_history.append(test_loss)\n",
    "        test_accuracy_history.append(test_accuracy)\n",
    "        test_error_history.append(test_error)\n",
    "\n",
    "    # Initialize server and client controls\n",
    "    num_clients = len(client_list)\n",
    "    server_controls = {}\n",
    "    for name, parameters in global_model.named_parameters():\n",
    "        server_controls[name] = torch.zeros_like(parameters.data)\n",
    "    for client in client_list:\n",
    "        for name, parameters in client.model.named_parameters():\n",
    "            client.client_controls[name] = torch.zeros_like(parameters.data)\n",
    "\n",
    "    for epoch in range(global_epochs):\n",
    "        global_weights = global_model.state_dict()\n",
    "        delta_weights_total = []\n",
    "        delta_client_controls_total = []\n",
    "        random_client_list = random.sample(client_list, random_sample_client_number)\n",
    "        for client in random_client_list:\n",
    "            client.load_weights(global_weights)\n",
    "            send_cost += sum(value.numel() for value in global_weights.values())\n",
    "            send_cost += sum(value.numel() for value in server_controls.values())\n",
    "            delta_weights, delta_client_controls = client.train_Scaffold(server_controls, local_epochs, Scaffold_update_controls_use_gradient)\n",
    "            delta_weights_total.append(delta_weights)\n",
    "            delta_client_controls_total.append(delta_client_controls)\n",
    "            send_cost += sum(value.numel() for value in delta_weights.values())\n",
    "            send_cost += sum(value.numel() for value in delta_client_controls.values())\n",
    "        \n",
    "        # Aggregation\n",
    "        aggregated_weights = {}\n",
    "        aggregated_client_controls = {}\n",
    "        for name, parameters in global_model.named_parameters():\n",
    "            aggregated_weights[name] = torch.zeros_like(parameters.data)\n",
    "            aggregated_client_controls[name] = torch.zeros_like(parameters.data)\n",
    "\n",
    "        if Scaffold_Aggregate_Median is True:\n",
    "            for name, parameters in global_model.named_parameters():\n",
    "                aggregated_weights[name] = np.median(np.array(aggregated_weights[name]), axis=0)\n",
    "                aggregated_client_controls[name] = np.median(np.array(aggregated_client_controls[name]), axis=0)\n",
    "        else:\n",
    "            for delta_weights, delta_client_controls in zip(delta_weights_total, delta_client_controls_total):\n",
    "                for name, parameters in global_model.named_parameters():\n",
    "                    aggregated_weights[name] += delta_weights[name] / random_sample_client_number\n",
    "                    aggregated_client_controls[name] += delta_client_controls[name] / random_sample_client_number\n",
    "\n",
    "        for name, parameters in global_model.named_parameters():\n",
    "            parameters.data += global_step_size * aggregated_weights[name].data\n",
    "            server_controls[name].data += (random_sample_client_number / num_clients) * aggregated_client_controls[name].data\n",
    "\n",
    "        # Record\n",
    "        time_used = timeit.default_timer() - start_time\n",
    "        cost_history.append(send_cost)\n",
    "        time_history.append(time_used)\n",
    "\n",
    "        train_loss, train_accuracy, train_error = evaluate_model_simple(model=global_model, dataloader=train_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        train_error_history.append(train_error)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy, test_error = evaluate_model_simple(model=global_model, dataloader=test_dataloader, loss_func=loss_func, accuracy_func=accuracy_func, error_func=error_func)\n",
    "            test_loss_history.append(test_loss)\n",
    "            test_accuracy_history.append(test_accuracy)\n",
    "            test_error_history.append(test_error)\n",
    "\n",
    "        if show_history:\n",
    "            print(\"!-- Server Model Status --!\")\n",
    "            print(f'Epoch [{epoch+1}/{global_epochs}], Culminative Send Cost: {send_cost}, Culminative Time Used: {time_used}')\n",
    "            print(f'Train Loss: {train_loss:.16f}, Train Accuracy: {train_accuracy:.16f}, Train Error: {train_error:.16f}')\n",
    "            if test_dataloader is not None:\n",
    "                print(f'Test Loss: {test_loss:.16f}, Test Accuracy: {test_accuracy:.16f}, Test Error: {test_error:.16f}')\n",
    "\n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#    Training Algorithm    #\n",
    "############################\n",
    "global save_file_extra_information\n",
    "save_file_extra_information = \"None.\"\n",
    "def train_neural_network_model(model, train_dataloader, test_dataloader, num_epochs, optimizer, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True, save_path_str=\"Centralized\"):\n",
    "    if test_dataloader is not None:\n",
    "        train_loss_history, train_accuracy_history, train_error_history, time_history, test_loss_history, test_accuracy_history, test_error_history = iterate_model_simple(model, train_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, test_dataloader, True)\n",
    "    else:\n",
    "        train_loss_history, train_accuracy_history, train_error_history, time_history = iterate_model_simple(model, train_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, True)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_{}.npy\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history, extra_information=save_file_extra_information)\n",
    "        torch.save(model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "        logname = \"{}_{}_{}.txt\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(logname, 'wb') as f:\n",
    "            f.write(save_file_extra_information.encode('utf-8'))\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func=federated_averaging, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True, save_path_str=\"FederatedLearning\"):\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_{}.npy\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history, extra_information=save_file_extra_information)\n",
    "        torch.save(global_model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "        logname = \"{}_{}_{}.txt\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(logname, 'wb') as f:\n",
    "            f.write(save_file_extra_information.encode('utf-8'))\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "def train_Scaffold_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, global_step_size, Scaffold_update_controls_use_gradient, loss_func=torch.nn.functional.cross_entropy, accuracy_func=get_accuracy, error_func=get_error, show_history=True, save_result=True, save_path_str=\"Scaffold\"):\n",
    "    cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = iterate_Scaffold_global(train_dataloader, test_dataloader, global_model, client_list, random_sample_client_number, global_epochs, global_step_size, local_epochs, Scaffold_update_controls_use_gradient, loss_func, accuracy_func, error_func, show_history)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in global_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Save Results\n",
    "    if save_result:\n",
    "        filename = \"{}_{}_{}.npy\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(filename, 'wb') as f:\n",
    "            np.savez(f, cost_history=cost_history, time_history=time_history, train_loss_history=train_loss_history, train_accuracy_history=train_accuracy_history, train_error_history=train_error_history, test_loss_history=test_loss_history, test_accuracy_history=test_accuracy_history, test_error_history=test_error_history, extra_information=save_file_extra_information)\n",
    "        torch.save(global_model.state_dict(), filename + \"_model_state_dict.pth\")\n",
    "        logname = \"{}_{}_{}.txt\".format(save_path_str, train_start_time, experiment_id)\n",
    "        with open(logname, 'wb') as f:\n",
    "            f.write(save_file_extra_information.encode('utf-8'))\n",
    "\n",
    "    # Graph\n",
    "    if show_history:\n",
    "        plot_cost_history([cost_history])\n",
    "        plot_time_history([time_history])\n",
    "        plot_loss_history([train_loss_history], [test_loss_history])\n",
    "        plot_accuracy_history([train_accuracy_history], [test_accuracy_history])\n",
    "        plot_error_history([train_error_history], [test_error_history])\n",
    "    \n",
    "    return cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history\n",
    "\n",
    "############################\n",
    "#   Experiment Functions   #\n",
    "############################\n",
    "def experiment_neural_network_model(train_dataset, test_dataset, modelClass, optimizerClass, train_func, epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, experiment_rounds = 1, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    epochs_list = convert_to_list(epochs_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "    \n",
    "    for n in range(experiment_rounds):\n",
    "        experiment_id = experiment_id + 1\n",
    "        print(f'=== Training Experiment {experiment_id} ===')\n",
    "        print(f'number of epochs is {epochs_list[min(n, len(epochs_list) - 1)]}')\n",
    "        num_epochs = epochs_list[min(n, len(epochs_list) - 1)]\n",
    "        print(f'learning rate is {learning_rate_list[min(n, len(learning_rate_list) - 1)]}')\n",
    "        learning_rate = learning_rate_list[min(n, len(learning_rate_list) - 1)]\n",
    "        print(f'batch size is {batch_size_list[min(n, len(batch_size_list) - 1)]}')\n",
    "        batch_size = batch_size_list[min(n, len(batch_size_list) - 1)]\n",
    "        print(f'loss function is {loss_func_list[min(n, len(loss_func_list) - 1)].__name__}')\n",
    "        loss_func = loss_func_list[min(n, len(loss_func_list) - 1)]\n",
    "        print(f'accuracy function is {accuracy_func_list[min(n, len(accuracy_func_list) - 1)].__name__}')\n",
    "        accuracy_func = accuracy_func_list[min(n, len(accuracy_func_list) - 1)]\n",
    "        print(f'error function is {error_func_list[min(n, len(error_func_list) - 1)].__name__}')\n",
    "        error_func = error_func_list[min(n, len(error_func_list) - 1)]\n",
    "        \n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        optimizer = optimizerClass(global_model.parameters(), learning_rate)\n",
    "\n",
    "        save_path_str = f\"{current_dataset_name}_Centralized_E_{num_epochs}_lr_{learning_rate}_B_{batch_size}\"\n",
    "\n",
    "        global save_file_extra_information\n",
    "        save_file_extra_information = f\"\"\"\n",
    "        === {save_path_str} ===\n",
    "        The experiment ID is: {experiment_id}\n",
    "        The train start time is: {train_start_time}\n",
    "        The dataset is: {current_dataset_name}\n",
    "        [should be Centralized Training]\n",
    "\n",
    "        experiment_rounds = {experiment_rounds}\n",
    "\n",
    "        modelType = {modelClass.__name__}\n",
    "        optimizerType = {optimizerClass.__name__}\n",
    "\n",
    "        num_epochs_list = {epochs_list}\n",
    "        learning_rate_list = {learning_rate_list}\n",
    "        batch_size_list = {batch_size_list}\n",
    "        loss_func_list = {loss_func_list}\n",
    "        accuracy_func_list = {accuracy_func_list}\n",
    "        error_func_list = {error_func_list}\n",
    "\n",
    "        !-- Current Status --!\n",
    "        num_epochs = {num_epochs}\n",
    "        learning_rate = {learning_rate}\n",
    "        batch_size = {batch_size}\n",
    "        loss_func = {loss_func.__name__}\n",
    "        accuracy_func = {accuracy_func.__name__}\n",
    "        error_func = {error_func.__name__}\n",
    "        \"\"\"\n",
    "\n",
    "        time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_func(global_model, train_dataloader, test_dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, save_result, save_path_str)\n",
    "\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list = [0], adversary_attack_func_list = None, adversary_attack_value_list = 0, experiment_rounds = 1, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "    adversary_list = convert_to_list(adversary_list)\n",
    "    adversary_attack_func_list = convert_to_list(adversary_attack_func_list)\n",
    "    adversary_attack_value_list = convert_to_list(adversary_attack_value_list)\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "\n",
    "    for n in range(experiment_rounds):\n",
    "        experiment_id = experiment_id + 1\n",
    "        print(f'=== Training Experiment {experiment_id} ===')\n",
    "        print(f'global epochs is {global_epochs_list[min(n, len(global_epochs_list) - 1)]}')\n",
    "        global_epochs = global_epochs_list[min(n, len(global_epochs_list) - 1)]\n",
    "        print(f'local epochs is {local_epochs_list[min(n, len(local_epochs_list) - 1)]}')\n",
    "        local_epochs = local_epochs_list[min(n, len(local_epochs_list) - 1)]\n",
    "        print(f'num clients is {num_clients_list[min(n, len(num_clients_list) - 1)]}')\n",
    "        num_clients = num_clients_list[min(n, len(num_clients_list) - 1)]\n",
    "        print(f'random sample client number is {random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]}')\n",
    "        random_sample_client_number = random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]\n",
    "        print(f'learning rate is {learning_rate_list[min(n, len(learning_rate_list) - 1)]}')\n",
    "        learning_rate = learning_rate_list[min(n, len(learning_rate_list) - 1)]\n",
    "        print(f'batch size is {batch_size_list[min(n, len(batch_size_list) - 1)]}')\n",
    "        batch_size = batch_size_list[min(n, len(batch_size_list) - 1)]\n",
    "        print(f'aggregate function is {aggregate_func_list[min(n, len(aggregate_func_list) - 1)].__name__}')\n",
    "        aggregate_func = aggregate_func_list[min(n, len(aggregate_func_list) - 1)]\n",
    "        print(f'loss function is {loss_func_list[min(n, len(loss_func_list) - 1)].__name__}')\n",
    "        loss_func = loss_func_list[min(n, len(loss_func_list) - 1)]\n",
    "        print(f'accuracy function is {accuracy_func_list[min(n, len(accuracy_func_list) - 1)].__name__}')\n",
    "        accuracy_func = accuracy_func_list[min(n, len(accuracy_func_list) - 1)]\n",
    "        print(f'error function is {error_func_list[min(n, len(error_func_list) - 1)].__name__}')\n",
    "        error_func = error_func_list[min(n, len(error_func_list) - 1)]\n",
    "        print(f'adversary is {adversary_list[min(n, len(adversary_list) - 1)]}')\n",
    "        adversary = adversary_list[min(n, len(adversary_list) - 1)]\n",
    "        print(f'adversary attack function is {adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]}')\n",
    "        adversary_attack_func = adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]\n",
    "        print(f'adversary attack value is {adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]}')\n",
    "        adversary_attack_value = adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]\n",
    "        \n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [to_device(modelClass(), device)] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        for i in range(adversary):\n",
    "            client_list[i].adversary = True\n",
    "            client_list[i].adversary_attack_func = adversary_attack_func\n",
    "            client_list[i].adversary_attack_value = adversary_attack_value\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        save_path_str = f\"{current_dataset_name}_{current_method_name}_GE_{global_epochs}_LE_{local_epochs}_C_{num_clients}_RC_{random_sample_client_number}_lr_{learning_rate}_B_{batch_size}\"\n",
    "\n",
    "        global save_file_extra_information\n",
    "        save_file_extra_information = f\"\"\"\n",
    "        === {save_path_str} ===\n",
    "        The experiment ID is: {experiment_id}\n",
    "        The train start time is: {train_start_time}\n",
    "        The dataset is: {current_dataset_name}\n",
    "        current_method_name = {current_method_name}\n",
    "        [should be FedAvg]\n",
    "\n",
    "        experiment_rounds = {experiment_rounds}\n",
    "\n",
    "        modelType = {modelClass.__name__}\n",
    "        itera_func = {itera_func.__name__}\n",
    "        optimizerType = {optimizerClass.__name__}\n",
    "        dataset_distributing_func = {dataset_distributing_func.__name__}\n",
    "\n",
    "        custom_split_dataset_iid = {custom_split_dataset_iid}\n",
    "        custom_split_dataset_power_law = {custom_split_dataset_power_law}\n",
    "        custom_split_dataset_balance = {custom_split_dataset_balance}\n",
    "        custom_split_dataset_seed = {custom_split_dataset_seed}\n",
    "\n",
    "        global_epochs_list = {global_epochs_list}\n",
    "        local_epochs_list = {local_epochs_list}\n",
    "        num_clients_list = {num_clients_list}\n",
    "        random_sample_client_number_list = {random_sample_client_number_list}\n",
    "        learning_rate_list = {learning_rate_list}\n",
    "        batch_size_list = {batch_size_list}\n",
    "        aggregate_func_list = {aggregate_func_list}\n",
    "        loss_func_list = {loss_func_list}\n",
    "        accuracy_func_list = {accuracy_func_list}\n",
    "        error_func_list = {error_func_list}\n",
    "        adversary_list = {adversary_list}\n",
    "        adversary_attack_func_list = {adversary_attack_func_list}\n",
    "        adversary_attack_value_list = {adversary_attack_value_list}\n",
    "\n",
    "        !-- Current Status --!\n",
    "        global_epochs = {global_epochs}\n",
    "        local_epochs = {local_epochs}\n",
    "        num_clients = {num_clients}\n",
    "        random_sample_client_number = {random_sample_client_number}\n",
    "        learning_rate = {learning_rate}\n",
    "        batch_size = {batch_size}\n",
    "        aggregate_func = {aggregate_func.__name__}\n",
    "        loss_func = {loss_func.__name__}\n",
    "        accuracy_func = {accuracy_func.__name__}\n",
    "        error_func = {error_func.__name__}\n",
    "        adversary = {adversary}\n",
    "        adversary_attack_func = {adversary_attack_func}\n",
    "        adversary_attack_value = {adversary_attack_value}\n",
    "        \"\"\"\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result, save_path_str)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, adversary_list = [0], adversary_attack_func_list = None, adversary_attack_value_list = 0, experiment_rounds = 1, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    aggregate_func_list = convert_to_list(aggregate_func_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "    straggler_list = convert_to_list(straggler_list)\n",
    "    mu_list = convert_to_list(mu_list)\n",
    "    adversary_list = convert_to_list(adversary_list)\n",
    "    adversary_attack_func_list = convert_to_list(adversary_attack_func_list)\n",
    "    adversary_attack_value_list = convert_to_list(adversary_attack_value_list)\n",
    "\n",
    "    global FedProx_global_model\n",
    "    global FedProx_mu\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "    \n",
    "    for n in range(experiment_rounds):\n",
    "        experiment_id = experiment_id + 1\n",
    "        print(f'=== Training Experiment {experiment_id} ===')\n",
    "        print(f'global epochs is {global_epochs_list[min(n, len(global_epochs_list) - 1)]}')\n",
    "        global_epochs = global_epochs_list[min(n, len(global_epochs_list) - 1)]\n",
    "        print(f'local epochs is {local_epochs_list[min(n, len(local_epochs_list) - 1)]}')\n",
    "        local_epochs = local_epochs_list[min(n, len(local_epochs_list) - 1)]\n",
    "        print(f'num clients is {num_clients_list[min(n, len(num_clients_list) - 1)]}')\n",
    "        num_clients = num_clients_list[min(n, len(num_clients_list) - 1)]\n",
    "        print(f'random sample client number is {random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]}')\n",
    "        random_sample_client_number = random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]\n",
    "        print(f'learning rate list is {learning_rate_list[min(n, len(learning_rate_list) - 1)]}')\n",
    "        learning_rate = learning_rate_list[min(n, len(learning_rate_list) - 1)]\n",
    "        print(f'batch size list is {batch_size_list[min(n, len(batch_size_list) - 1)]}')\n",
    "        batch_size = batch_size_list[min(n, len(batch_size_list) - 1)]\n",
    "        print(f'aggregate function is {aggregate_func_list[min(n, len(aggregate_func_list) - 1)].__name__}')\n",
    "        aggregate_func = aggregate_func_list[min(n, len(aggregate_func_list) - 1)]\n",
    "        print(f'loss function is {loss_func_list[min(n, len(loss_func_list) - 1)].__name__}')\n",
    "        loss_func = loss_func_list[min(n, len(loss_func_list) - 1)]\n",
    "        print(f'accuracy function is {accuracy_func_list[min(n, len(accuracy_func_list) - 1)].__name__}')\n",
    "        accuracy_func = accuracy_func_list[min(n, len(accuracy_func_list) - 1)]\n",
    "        print(f'error function is {error_func_list[min(n, len(error_func_list) - 1)].__name__}')\n",
    "        error_func = error_func_list[min(n, len(error_func_list) - 1)]\n",
    "        print(f'straggler is {straggler_list[min(n, len(straggler_list) - 1)]}')\n",
    "        straggler = straggler_list[min(n, len(straggler_list) - 1)]\n",
    "        print(f'mu is {mu_list[min(n, len(mu_list) - 1)]}')\n",
    "        FedProx_mu = mu_list[min(n, len(mu_list) - 1)]\n",
    "        print(f'adversary is {adversary_list[min(n, len(adversary_list) - 1)]}')\n",
    "        adversary = adversary_list[min(n, len(adversary_list) - 1)]\n",
    "        print(f'adversary attack function is {adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]}')\n",
    "        adversary_attack_func = adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]\n",
    "        print(f'adversary attack value is {adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]}')\n",
    "        adversary_attack_value = adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]\n",
    "        \n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "        FedProx_global_model = global_model\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [to_device(modelClass(), device)] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        for i in range(straggler):\n",
    "            client_list[i].straggler = True\n",
    "\n",
    "        for i in range(adversary):\n",
    "            if client_list[i].straggler is True:\n",
    "                client_list[i+straggler].adversary = True\n",
    "                client_list[i+straggler].adversary_attack_func = adversary_attack_func\n",
    "                client_list[i+straggler].adversary_attack_value = 0\n",
    "            else:\n",
    "                client_list[i].adversary = True\n",
    "                client_list[i].adversary_attack_func = adversary_attack_func\n",
    "                client_list[i].adversary_attack_value = 0\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        save_path_str = f\"{current_dataset_name}_{current_method_name}_GE_{global_epochs}_LE_{local_epochs}_C_{num_clients}_RC_{random_sample_client_number}_lr_{learning_rate}_B_{batch_size}\"\n",
    "\n",
    "        global save_file_extra_information\n",
    "        save_file_extra_information = f\"\"\"\n",
    "        === {save_path_str} ===\n",
    "        The experiment ID is: {experiment_id}\n",
    "        The train start time is: {train_start_time}\n",
    "        The dataset is: {current_dataset_name}\n",
    "        current_method_name = {current_method_name}\n",
    "        [should be FedProx]\n",
    "\n",
    "        experiment_rounds = {experiment_rounds}\n",
    "\n",
    "        modelType = {modelClass.__name__}\n",
    "        itera_func = {itera_func.__name__}\n",
    "        optimizerType = {optimizerClass.__name__}\n",
    "        dataset_distributing_func = {dataset_distributing_func.__name__}\n",
    "\n",
    "        custom_split_dataset_iid = {custom_split_dataset_iid}\n",
    "        custom_split_dataset_power_law = {custom_split_dataset_power_law}\n",
    "        custom_split_dataset_balance = {custom_split_dataset_balance}\n",
    "        custom_split_dataset_seed = {custom_split_dataset_seed}\n",
    "\n",
    "        global_epochs_list = {global_epochs_list}\n",
    "        local_epochs_list = {local_epochs_list}\n",
    "        num_clients_list = {num_clients_list}\n",
    "        random_sample_client_number_list = {random_sample_client_number_list}\n",
    "        learning_rate_list = {learning_rate_list}\n",
    "        batch_size_list = {batch_size_list}\n",
    "        aggregate_func_list = {aggregate_func_list}\n",
    "        loss_func_list = {loss_func_list}\n",
    "        accuracy_func_list = {accuracy_func_list}\n",
    "        error_func_list = {error_func_list}\n",
    "        straggler_list = {straggler_list}\n",
    "        mu_list = {mu_list}\n",
    "        adversary_list = {adversary_list}\n",
    "        adversary_attack_func_list = {adversary_attack_func_list}\n",
    "        adversary_attack_value_list = {adversary_attack_value_list}\n",
    "\n",
    "        !-- Current Status --!\n",
    "        global_epochs = {global_epochs}\n",
    "        local_epochs = {local_epochs}\n",
    "        num_clients = {num_clients}\n",
    "        random_sample_client_number = {random_sample_client_number}\n",
    "        learning_rate = {learning_rate}\n",
    "        batch_size = {batch_size}\n",
    "        aggregate_func = {aggregate_func.__name__}\n",
    "        loss_func = {loss_func.__name__}\n",
    "        accuracy_func = {accuracy_func.__name__}\n",
    "        error_func = {error_func.__name__}\n",
    "        straggler = {straggler}\n",
    "        mu = {FedProx_mu}\n",
    "        adversary = {adversary}\n",
    "        adversary_attack_func = {adversary_attack_func}\n",
    "        adversary_attack_value = {adversary_attack_value}\n",
    "        \"\"\"\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_federated_learning_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result, save_path_str)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)\n",
    "\n",
    "def experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list = False, straggler_list = [0], adversary_list = [0], adversary_attack_func_list = None, adversary_attack_value_list = 0, adversary_attack_Scaffold_all_list = False, Scaffold_Aggregate_Median_list = False, experiment_rounds = 1, show_history=True, save_result=True):\n",
    "    global experiment_id\n",
    "    experiment_id = 0\n",
    "\n",
    "    global_epochs_list = convert_to_list(global_epochs_list)\n",
    "    local_epochs_list = convert_to_list(local_epochs_list)\n",
    "    num_clients_list = convert_to_list(num_clients_list)\n",
    "    random_sample_client_number_list = convert_to_list(random_sample_client_number_list)\n",
    "    learning_rate_list = convert_to_list(learning_rate_list)\n",
    "    global_step_size_list = convert_to_list(global_step_size_list)\n",
    "    batch_size_list = convert_to_list(batch_size_list)\n",
    "    loss_func_list = convert_to_list(loss_func_list)\n",
    "    accuracy_func_list = convert_to_list(accuracy_func_list)\n",
    "    error_func_list = convert_to_list(error_func_list)\n",
    "    Scaffold_update_controls_use_gradient_list = convert_to_list(Scaffold_update_controls_use_gradient_list)\n",
    "    straggler_list = convert_to_list(straggler_list)\n",
    "    adversary_list = convert_to_list(adversary_list)\n",
    "    adversary_attack_func_list = convert_to_list(adversary_attack_func_list)\n",
    "    adversary_attack_value_list = convert_to_list(adversary_attack_value_list)\n",
    "    adversary_attack_Scaffold_all_list = convert_to_list(adversary_attack_Scaffold_all_list)\n",
    "    Scaffold_Aggregate_Median_list = convert_to_list(Scaffold_Aggregate_Median_list)\n",
    "\n",
    "    global Scaffold_Aggregate_Median\n",
    "\n",
    "    cost_history_total = []\n",
    "    time_history_total = []\n",
    "    train_loss_history_total = []\n",
    "    train_accuracy_history_total = []\n",
    "    train_error_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_accuracy_history_total = []\n",
    "    test_error_history_total = []\n",
    "    \n",
    "    for n in range(experiment_rounds):\n",
    "        experiment_id = experiment_id + 1\n",
    "        print(f'=== Training Experiment {experiment_id} ===')\n",
    "        print(f'global epochs is {global_epochs_list[min(n, len(global_epochs_list) - 1)]}')\n",
    "        global_epochs = global_epochs_list[min(n, len(global_epochs_list) - 1)]\n",
    "        print(f'local epochs is {local_epochs_list[min(n, len(local_epochs_list) - 1)]}')\n",
    "        local_epochs = local_epochs_list[min(n, len(local_epochs_list) - 1)]\n",
    "        print(f'num clients is {num_clients_list[min(n, len(num_clients_list) - 1)]}')\n",
    "        num_clients = num_clients_list[min(n, len(num_clients_list) - 1)]\n",
    "        print(f'random sample client number is {random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]}')\n",
    "        random_sample_client_number = random_sample_client_number_list[min(n, len(random_sample_client_number_list) - 1)]\n",
    "        print(f'learning rate list is {learning_rate_list[min(n, len(learning_rate_list) - 1)]}')\n",
    "        learning_rate = learning_rate_list[min(n, len(learning_rate_list) - 1)]\n",
    "        print(f'global step size is {global_step_size_list[min(n, len(global_step_size_list) - 1)]}')\n",
    "        global_step_size = global_step_size_list[min(n, len(global_step_size_list) - 1)]\n",
    "        print(f'batch size list is {batch_size_list[min(n, len(batch_size_list) - 1)]}')\n",
    "        batch_size = batch_size_list[min(n, len(batch_size_list) - 1)]\n",
    "        print(f'loss function is {loss_func_list[min(n, len(loss_func_list) - 1)].__name__}')\n",
    "        loss_func = loss_func_list[min(n, len(loss_func_list) - 1)]\n",
    "        print(f'accuracy function is {accuracy_func_list[min(n, len(accuracy_func_list) - 1)].__name__}')\n",
    "        accuracy_func = accuracy_func_list[min(n, len(accuracy_func_list) - 1)]\n",
    "        print(f'error function is {error_func_list[min(n, len(error_func_list) - 1)].__name__}')\n",
    "        error_func = error_func_list[min(n, len(error_func_list) - 1)]\n",
    "        print(f'use gradient to update control variable is {Scaffold_update_controls_use_gradient_list[min(n, len(Scaffold_update_controls_use_gradient_list) - 1)]}')\n",
    "        Scaffold_update_controls_use_gradient = Scaffold_update_controls_use_gradient_list[min(n, len(Scaffold_update_controls_use_gradient_list) - 1)]\n",
    "        print(f'straggler is {straggler_list[min(n, len(straggler_list) - 1)]}')\n",
    "        straggler = straggler_list[min(n, len(straggler_list) - 1)]\n",
    "        print(f'adversary is {adversary_list[min(n, len(adversary_list) - 1)]}')\n",
    "        adversary = adversary_list[min(n, len(adversary_list) - 1)]\n",
    "        print(f'adversary attack function is {adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]}')\n",
    "        adversary_attack_func = adversary_attack_func_list[min(n, len(adversary_attack_func_list) - 1)]\n",
    "        print(f'adversary attack value is {adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]}')\n",
    "        adversary_attack_value = adversary_attack_value_list[min(n, len(adversary_attack_value_list) - 1)]\n",
    "        print(f'adversary attack include control variables is {adversary_attack_Scaffold_all_list[min(n, len(adversary_attack_Scaffold_all_list) - 1)]}')\n",
    "        adversary_attack_Scaffold_all = adversary_attack_Scaffold_all_list[min(n, len(adversary_attack_Scaffold_all_list) - 1)]\n",
    "        print(f'scaffold aggregate median is {Scaffold_Aggregate_Median_list[min(n, len(adversary_attack_Scaffold_all_list) - 1)]}')\n",
    "        Scaffold_Aggregate_Median = Scaffold_Aggregate_Median_list[min(n, len(adversary_attack_Scaffold_all_list) - 1)]\n",
    "\n",
    "        global_model = to_device(modelClass(), device)\n",
    "        print(global_model)\n",
    "        print(global_model.state_dict())\n",
    "\n",
    "        train_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True), device)\n",
    "        test_dataloader = DeviceDataLoader(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False), device)\n",
    "\n",
    "        print(\"Establishing client devices...\")\n",
    "\n",
    "        client_model_list = [to_device(modelClass(), device)] * num_clients\n",
    "        client_optimizer_list = [optimizerClass(model.parameters(), learning_rate) for model in client_model_list]\n",
    "        client_dataset_list = dataset_distributing_func(train_dataset, num_clients)\n",
    "        print(f'Training dataset has been distributed into {len(client_dataset_list)} pieces.')\n",
    "        client_batch_size_list = [batch_size] * num_clients\n",
    "        client_itera_func_list = [itera_func] * num_clients\n",
    "        client_loss_func_list = [loss_func] * num_clients\n",
    "        client_accuracy_func_list = [accuracy_func] * num_clients\n",
    "        client_error_func_list = [error_func] * num_clients\n",
    "        client_list = establish_client_devices(num_clients, client_model_list, client_optimizer_list, client_dataset_list, client_batch_size_list, client_itera_func_list, client_loss_func_list, client_accuracy_func_list, client_error_func_list)\n",
    "\n",
    "        for i in range(straggler):\n",
    "            client_list[i].straggler = True\n",
    "\n",
    "        for i in range(adversary):\n",
    "            if client_list[i].straggler is True:\n",
    "                client_list[i+straggler].adversary = True\n",
    "                client_list[i+straggler].adversary_attack_func = adversary_attack_func\n",
    "                client_list[i+straggler].adversary_attack_value = 0\n",
    "                client_list[i+straggler].adversary_attack_Scaffold_all = adversary_attack_Scaffold_all\n",
    "            else:\n",
    "                client_list[i].adversary = True\n",
    "                client_list[i].adversary_attack_func = adversary_attack_func\n",
    "                client_list[i].adversary_attack_value = 0\n",
    "                client_list[i].adversary_attack_Scaffold_all = adversary_attack_Scaffold_all\n",
    "\n",
    "        # Visualize the client dataset. Please comment these codes to avoid a long code output if necessary.\n",
    "        #for i in range(len(client_list)):\n",
    "        #    print(client_dataset_list[i])\n",
    "\n",
    "        print(f'Established {len(client_list)} client devices.')\n",
    "\n",
    "        save_path_str = f\"{current_dataset_name}_{current_method_name}_GE_{global_epochs}_LE_{local_epochs}_C_{num_clients}_RC_{random_sample_client_number}_lr_{learning_rate}_B_{batch_size}\"\n",
    "\n",
    "        global save_file_extra_information\n",
    "        save_file_extra_information = f\"\"\"\n",
    "        === {save_path_str} ===\n",
    "        The experiment ID is: {experiment_id}\n",
    "        The train start time is: {train_start_time}\n",
    "        The dataset is: {current_dataset_name}\n",
    "        current_method_name = {current_method_name}\n",
    "        [should be Scaffold]\n",
    "\n",
    "        experiment_rounds = {experiment_rounds}\n",
    "\n",
    "        modelType = {modelClass.__name__}\n",
    "        itera_func = {itera_func.__name__}\n",
    "        optimizerType = {optimizerClass.__name__}\n",
    "        dataset_distributing_func = {dataset_distributing_func.__name__}\n",
    "\n",
    "        custom_split_dataset_iid = {custom_split_dataset_iid}\n",
    "        custom_split_dataset_power_law = {custom_split_dataset_power_law}\n",
    "        custom_split_dataset_balance = {custom_split_dataset_balance}\n",
    "        custom_split_dataset_seed = {custom_split_dataset_seed}\n",
    "\n",
    "        global_epochs_list = {global_epochs_list}\n",
    "        local_epochs_list = {local_epochs_list}\n",
    "        num_clients_list = {num_clients_list}\n",
    "        random_sample_client_number_list = {random_sample_client_number_list}\n",
    "        learning_rate_list = {learning_rate_list}\n",
    "        global_step_size_list = {global_step_size_list}\n",
    "        batch_size_list = {batch_size_list}\n",
    "        loss_func_list = {loss_func_list}\n",
    "        accuracy_func_list = {accuracy_func_list}\n",
    "        error_func_list = {error_func_list}\n",
    "        Scaffold_update_controls_use_gradient_list = {Scaffold_update_controls_use_gradient_list}\n",
    "        straggler_list = {straggler_list}\n",
    "        adversary_list = {adversary_list}\n",
    "        adversary_attack_func_list = {adversary_attack_func_list}\n",
    "        adversary_attack_value_list = {adversary_attack_value_list}\n",
    "        adversary_attack_Scaffold_all_list = {adversary_attack_Scaffold_all_list}\n",
    "\n",
    "        !-- Current Status --!\n",
    "        global_epochs = {global_epochs}\n",
    "        local_epochs = {local_epochs}\n",
    "        num_clients = {num_clients}\n",
    "        random_sample_client_number = {random_sample_client_number}\n",
    "        learning_rate = {learning_rate}\n",
    "        global_step_size = {global_step_size}\n",
    "        batch_size = {batch_size}\n",
    "        loss_func = {loss_func.__name__}\n",
    "        accuracy_func = {accuracy_func.__name__}\n",
    "        error_func = {error_func.__name__}\n",
    "        Scaffold_update_controls_use_gradient = {Scaffold_update_controls_use_gradient}\n",
    "        straggler = {straggler}\n",
    "        adversary = {adversary}\n",
    "        adversary_attack_func = {adversary_attack_func}\n",
    "        adversary_attack_value = {adversary_attack_value}\n",
    "        adversary_attack_Scaffold_all = {adversary_attack_Scaffold_all}\n",
    "        \"\"\"\n",
    "\n",
    "        cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history = train_Scaffold_model(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, global_step_size, Scaffold_update_controls_use_gradient, loss_func, accuracy_func, error_func, show_history, save_result, save_path_str)\n",
    "\n",
    "        cost_history_total.append(cost_history)\n",
    "        time_history_total.append(time_history)\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_accuracy_history_total.append(train_accuracy_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_accuracy_history_total.append(test_accuracy_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    print(f'Name of current dataset: {current_dataset_name}')\n",
    "    plot_cost_history(cost_history_total)\n",
    "    plot_time_history(time_history_total)\n",
    "    plot_loss_history(train_loss_history_total, test_loss_history_total)\n",
    "    plot_accuracy_history(train_accuracy_history_total, test_accuracy_history_total)\n",
    "    plot_error_history(train_error_history_total, test_error_history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Experiment ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Choosing Dataset ###\n",
    "\n",
    "In this section, execute a cell only to choose a dataset you want do experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "Number of samples in the train dataset: 60000\n",
      "Number of samples in the test dataset: 10000\n",
      "Number of samples in the train dataset after random split: 3000\n",
      "Number of samples in the test dataset after random split: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "train_dataset = MNIST_train_dataset\n",
    "test_dataset = MNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"MNIST\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "split_ratio = 0.05\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "split_ratio = 0.1\n",
    "test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 784\n",
    "model_type_preset = MNIST_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = CIFAR10_train_dataset\n",
    "test_dataset = CIFAR10_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"CIFAR10\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.50\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.007\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 10\n",
    "input_dim = 1024\n",
    "model_type_preset = CIFAR10_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EMNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = EMNIST_train_dataset\n",
    "test_dataset = EMNIST_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"EMNIST\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.50\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.007\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 1\n",
    "num_classes_preset = 26\n",
    "input_dim = 784\n",
    "model_type_preset = EMNIST_CNN_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give Me Some Credit Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Give_Me_Some_Credit_train_dataset\n",
    "test_dataset = Give_Me_Some_Credit_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Give Me Some Credit Dataset\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.01\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.01\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "Linear_Model_in_features = 7\n",
    "Linear_Model_out_features = 1\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_dataset = Epsilon_train_dataset\n",
    "test_dataset = Epsilon_test_dataset\n",
    "\n",
    "# Show Dataset Status\n",
    "current_dataset_name = \"Epsilon\"\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "print(f'Number of samples in the train dataset: {len(train_dataset)}')\n",
    "print(f'Number of samples in the test dataset: {len(test_dataset)}')\n",
    "\n",
    "# Validation Dataset\n",
    "#split_ratio = 0.50\n",
    "#train_dataset, validation_dataset = random_split(train_dataset, [int(len(train_dataset) * split_ratio), int(len(train_dataset) - int(len(train_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the train dataset after random split: {len(train_dataset)}')\n",
    "#split_ratio = 0.007\n",
    "#test_dataset, _ = random_split(test_dataset, [int(len(test_dataset) * split_ratio), int(len(test_dataset) - int(len(test_dataset) * split_ratio))])\n",
    "print(f'Number of samples in the test dataset after random split: {len(test_dataset)}')\n",
    "\n",
    "# Define variables associate with the dataset\n",
    "learning_rate_preset = 0.5\n",
    "batch_size_preset = 100\n",
    "num_features_preset = 7\n",
    "num_classes_preset = 2\n",
    "\n",
    "model_type_preset = Linear_Model\n",
    "optimizer_type_preset = torch.optim.SGD\n",
    "loss_func_preset = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Neural Network Experiments ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "num_epochs_list = 100\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "experiment_rounds = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "train_func = train_neural_network_model\n",
    "\n",
    "num_epochs_list = 100\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "\n",
    "experiment_rounds = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "experiment_neural_network_model(train_dataset, test_dataset, modelType, optimizerType, train_func, num_epochs_list, learning_rate_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3 FedAvg Federated Learning Experiments ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 20\n",
    "local_epochs_list = 5\n",
    "num_clients_list = [100]\n",
    "random_sample_client_number_list = [20]\n",
    "learning_rate_list = 0.01\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "adversary_list = [20]\n",
    "adversary_attack_func_list = [adversarial_attack_by_train_scaling]\n",
    "adversary_attack_value_list = [-0.01]\n",
    "\n",
    "experiment_rounds = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 20\n",
    "local_epochs_list = 5\n",
    "num_clients_list = [100]\n",
    "random_sample_client_number_list = [20]\n",
    "learning_rate_list = 0.01\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "adversary_list = [20]\n",
    "adversary_attack_func_list = [adversarial_attack_by_train_scaling]\n",
    "adversary_attack_value_list = [-0.01]\n",
    "\n",
    "experiment_rounds = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000]\n",
    "random_sample_client_number_list = [10, 500]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "adversary_list = [1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value]\n",
    "adversary_attack_value_list = [10000]\n",
    "\n",
    "experiment_rounds = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Number of Clients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Random Client Number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Batch Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4 FedProx Federated Learning Experiments ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Update Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000]\n",
    "random_sample_client_number_list = [10, 500]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.5, 0.5]\n",
    "adversary_list = [1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value]\n",
    "adversary_attack_value_list = [10000]\n",
    "\n",
    "experiment_rounds = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [100]\n",
    "random_sample_client_number_list = [20]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0]\n",
    "mu_list = [0.5]\n",
    "adversary_list = [1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value]\n",
    "adversary_attack_value_list = [10000]\n",
    "\n",
    "experiment_rounds = 2\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "straggler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10]\n",
    "random_sample_client_number_list = [10]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [5, 9]\n",
    "mu_list = [0.0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value]\n",
    "adversary_attack_value_list = [10000]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_FedProx\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [100]\n",
    "random_sample_client_number_list = [20]\n",
    "learning_rate_list = 0.03\n",
    "batch_size_list = 128\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "straggler_list = [0, 500, 900]\n",
    "mu_list = [0.0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value]\n",
    "adversary_attack_value_list = [10000]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedProx\"\n",
    "experiment_FedProx_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, straggler_list, mu_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.5 Scaffold Federated Learning Experiments ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [10, 128, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [10, 128, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [10, 128, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [1000]\n",
    "random_sample_client_number_list = [500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [10, 128, 1000]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [0]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False]\n",
    "\n",
    "experiment_rounds = 3\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment: Compare Local Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [0]\n",
    "adversary_list = [1, 100, 1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [0]\n",
    "adversary_list = [1, 100, 1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [1, 100, 1, 100]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = True\n",
    "custom_split_dataset_power_law = False\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [1, 100, 1, 100]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [0]\n",
    "adversary_list = [1, 100, 1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [0]\n",
    "adversary_list = [1, 100, 1, 100]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = True\n",
    "straggler_list = [1, 100, 1, 100]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = ScaffoldOptimizer\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_custom\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 100\n",
    "local_epochs_list = 10\n",
    "num_clients_list = [10, 1000, 10, 1000]\n",
    "random_sample_client_number_list = [10, 500, 10, 500]\n",
    "learning_rate_list = [0.03]\n",
    "global_step_size_list = 1.0\n",
    "batch_size_list = [128]\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "Scaffold_update_controls_use_gradient_list = False\n",
    "straggler_list = [1, 100, 1, 100]\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_value_Scaffold]\n",
    "adversary_attack_value_list = [10000]\n",
    "adversary_attack_Scaffold_all_list = [False, False, True, True]\n",
    "\n",
    "experiment_rounds = 4\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"Scaffold\"\n",
    "experiment_Scaffold_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, global_step_size_list, batch_size_list, loss_func_list, accuracy_func_list, error_func_list, Scaffold_update_controls_use_gradient_list, straggler_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, adversary_attack_Scaffold_all_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.6 Federated Median Learning Experiments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset is MNIST.\n",
      "The current train start time is 2024-04-02 11.46.05.\n",
      "=== Training Experiment 1 ===\n",
      "global epochs is 20\n",
      "local epochs is 2\n",
      "num clients is 3\n",
      "random sample client number is 3\n",
      "learning rate is 0.3\n",
      "batch size is 10\n",
      "aggregate function is federated_averaging\n",
      "loss function is cross_entropy\n",
      "accuracy function is get_accuracy\n",
      "error function is get_error\n",
      "adversary is 0\n",
      "adversary attack function is <function adversarial_attack_by_train_scaling at 0x00000170AFC3B6D0>\n",
      "adversary attack value is -0.01\n",
      "MNIST_CNN_Model(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "OrderedDict([('conv1.weight', tensor([[[[-0.1390,  0.1241,  0.0422,  0.1667, -0.1243],\n",
      "          [ 0.1350,  0.1772, -0.1845,  0.0195,  0.1916],\n",
      "          [ 0.0748,  0.0385, -0.0899, -0.0017, -0.1976],\n",
      "          [-0.0119,  0.0115,  0.1571, -0.1291, -0.1886],\n",
      "          [ 0.0149, -0.0461,  0.0553, -0.0201,  0.1402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1639, -0.1597, -0.1675,  0.1713,  0.0090],\n",
      "          [-0.1793, -0.0956, -0.0864,  0.1047,  0.1848],\n",
      "          [ 0.0311, -0.0984, -0.0191,  0.0760, -0.0390],\n",
      "          [-0.1442, -0.0365,  0.0172,  0.0580,  0.0893],\n",
      "          [-0.1722, -0.0130,  0.0388, -0.1819, -0.1569]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1723, -0.0061,  0.1861,  0.1098,  0.1047],\n",
      "          [-0.0547, -0.0728, -0.1872, -0.1697,  0.1151],\n",
      "          [-0.1130, -0.1132,  0.0359, -0.0151, -0.1451],\n",
      "          [-0.1434,  0.1138, -0.1136,  0.0070, -0.0025],\n",
      "          [-0.1902,  0.0066, -0.1873,  0.1108,  0.0416]]],\n",
      "\n",
      "\n",
      "        [[[-0.0021, -0.1438,  0.1164, -0.1970,  0.0104],\n",
      "          [ 0.1192, -0.0703, -0.0826, -0.0200, -0.0390],\n",
      "          [ 0.1211,  0.0018,  0.0132, -0.1811,  0.1391],\n",
      "          [-0.0986,  0.1645, -0.1271, -0.1933,  0.1785],\n",
      "          [-0.1094, -0.1632, -0.0885,  0.1672,  0.0637]]],\n",
      "\n",
      "\n",
      "        [[[-0.0567,  0.1829,  0.1416, -0.0513, -0.1832],\n",
      "          [-0.1196,  0.0208,  0.0248,  0.0519,  0.1936],\n",
      "          [-0.1461,  0.0340, -0.0936,  0.1726, -0.0200],\n",
      "          [-0.0817, -0.1673,  0.0914,  0.0707,  0.1254],\n",
      "          [-0.1521,  0.0605,  0.1044,  0.0896, -0.0496]]],\n",
      "\n",
      "\n",
      "        [[[-0.1432,  0.1446,  0.1035,  0.1068,  0.1166],\n",
      "          [ 0.0624,  0.0851,  0.1043, -0.0581,  0.1163],\n",
      "          [ 0.0040,  0.0329, -0.0766,  0.1647,  0.1237],\n",
      "          [ 0.0263,  0.1603,  0.1094,  0.1843, -0.0658],\n",
      "          [ 0.0427,  0.1700,  0.0572, -0.1532, -0.1818]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0914, -0.1014, -0.0153, -0.0998,  0.1990],\n",
      "          [ 0.1722, -0.0214,  0.1319, -0.0881,  0.0662],\n",
      "          [-0.1317, -0.1731, -0.0421,  0.0999,  0.1628],\n",
      "          [-0.1737,  0.1807,  0.0716, -0.1981, -0.0596],\n",
      "          [ 0.1348, -0.0768,  0.0830,  0.0486, -0.1034]]],\n",
      "\n",
      "\n",
      "        [[[-0.0039,  0.0029,  0.1196,  0.0643,  0.1633],\n",
      "          [-0.1378, -0.1488,  0.0400, -0.0348,  0.1523],\n",
      "          [-0.0964, -0.0906,  0.0476, -0.0687,  0.0128],\n",
      "          [-0.1620, -0.1451,  0.1299, -0.0472, -0.1714],\n",
      "          [ 0.0238, -0.0084,  0.1974,  0.1231,  0.0765]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1837,  0.1516,  0.1606, -0.1323, -0.0908],\n",
      "          [-0.0110,  0.1891, -0.0329, -0.0759, -0.1222],\n",
      "          [ 0.1903, -0.1995,  0.1882,  0.1728,  0.1246],\n",
      "          [-0.0468,  0.0336, -0.1717,  0.0314, -0.0364],\n",
      "          [ 0.0765,  0.0197,  0.1948, -0.1961,  0.1092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0087,  0.1199, -0.1426, -0.0487,  0.1665],\n",
      "          [-0.0409, -0.1039, -0.1688, -0.0437,  0.1317],\n",
      "          [-0.1638,  0.0464,  0.1199, -0.0545,  0.0248],\n",
      "          [ 0.0747,  0.0179,  0.0138, -0.0385, -0.1158],\n",
      "          [ 0.1526,  0.1260, -0.0141,  0.0245,  0.0023]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0769,  0.0600,  0.0055,  0.0725, -0.1790],\n",
      "          [-0.1878,  0.1306, -0.1970,  0.0651, -0.0952],\n",
      "          [ 0.0546, -0.0993, -0.0494,  0.1622,  0.1426],\n",
      "          [-0.1392,  0.0596, -0.0780, -0.0764, -0.0494],\n",
      "          [ 0.1831, -0.0733, -0.0896,  0.0085,  0.0410]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0395,  0.1751, -0.1119,  0.1227, -0.0943],\n",
      "          [-0.0939, -0.1000, -0.1165, -0.1907, -0.0842],\n",
      "          [-0.0386, -0.0064,  0.1670,  0.1314, -0.0479],\n",
      "          [-0.0897,  0.0575, -0.1290, -0.1620, -0.0040],\n",
      "          [-0.0039, -0.0420,  0.1420, -0.1020, -0.1964]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1004,  0.1846, -0.1331, -0.0164,  0.0922],\n",
      "          [-0.1325, -0.0137, -0.0307, -0.0476,  0.1216],\n",
      "          [-0.1786,  0.1090, -0.0614, -0.0933,  0.0293],\n",
      "          [ 0.0201, -0.0238,  0.1390,  0.1484, -0.0010],\n",
      "          [-0.1577,  0.1443,  0.0178, -0.0036,  0.1953]]],\n",
      "\n",
      "\n",
      "        [[[-0.0332,  0.0492,  0.1165, -0.0883, -0.1516],\n",
      "          [ 0.1865, -0.0466,  0.1481, -0.1875,  0.0097],\n",
      "          [-0.0811, -0.1817,  0.0084, -0.1395, -0.1343],\n",
      "          [-0.1036,  0.1974,  0.1454, -0.0831, -0.0350],\n",
      "          [-0.0780,  0.1427,  0.1908, -0.1375,  0.1315]]],\n",
      "\n",
      "\n",
      "        [[[-0.0350,  0.0267,  0.1261,  0.1434,  0.1755],\n",
      "          [-0.1029,  0.0152, -0.0859, -0.0327, -0.1391],\n",
      "          [ 0.1078,  0.1602,  0.0040,  0.0204, -0.0633],\n",
      "          [ 0.0435, -0.0460,  0.1107, -0.0714,  0.0591],\n",
      "          [-0.0559, -0.1260, -0.0010, -0.1506,  0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0556,  0.1761,  0.0955, -0.0730, -0.1994],\n",
      "          [-0.1202,  0.0539, -0.1939,  0.1902,  0.0757],\n",
      "          [ 0.1964, -0.0179, -0.0293, -0.0535, -0.0146],\n",
      "          [-0.1219, -0.0222,  0.0980, -0.0373,  0.1813],\n",
      "          [ 0.0811,  0.0560,  0.1428, -0.0669,  0.1285]]]])), ('conv1.bias', tensor([ 0.0237,  0.1451, -0.0231, -0.0321, -0.0833,  0.0408,  0.0345,  0.0351,\n",
      "        -0.0632, -0.1144, -0.1510,  0.0116,  0.1103,  0.0519,  0.0897, -0.0202])), ('conv2.weight', tensor([[[[ 3.8291e-02,  4.8438e-02,  2.5587e-02,  1.6260e-03, -3.5189e-02],\n",
      "          [-5.7698e-03, -3.4182e-02,  4.5124e-02, -1.5413e-02,  8.6124e-03],\n",
      "          [ 2.5847e-02, -2.1359e-02, -3.2417e-02,  1.4674e-02,  1.9370e-02],\n",
      "          [-4.2085e-02, -2.7679e-02, -1.5610e-02,  1.1952e-02,  2.0244e-02],\n",
      "          [ 2.9812e-02,  2.6504e-02,  4.6127e-02, -1.1974e-02,  2.7978e-02]],\n",
      "\n",
      "         [[ 3.4528e-02, -1.3969e-02, -3.6804e-02, -2.5674e-02,  4.0126e-02],\n",
      "          [ 1.8715e-03,  1.1560e-04, -3.8798e-02,  1.5780e-02,  3.9448e-02],\n",
      "          [-2.8846e-02,  3.6924e-02,  3.2220e-02, -3.1057e-02,  4.3246e-02],\n",
      "          [-4.5654e-02, -3.6459e-02, -3.3473e-03, -3.0396e-02,  2.2179e-02],\n",
      "          [ 2.4353e-02, -3.9661e-03, -3.5831e-02, -3.0865e-02,  2.9739e-02]],\n",
      "\n",
      "         [[-2.3943e-02,  1.7504e-02, -4.9540e-02,  7.6898e-03, -2.8139e-02],\n",
      "          [-4.7877e-02,  3.4630e-02,  3.4841e-02, -2.5195e-02, -4.7023e-02],\n",
      "          [ 2.5877e-02, -1.5945e-02,  1.1321e-02,  8.3452e-03, -4.7608e-02],\n",
      "          [-9.0827e-03, -2.0157e-02, -3.7747e-02, -4.5148e-02,  4.0848e-02],\n",
      "          [ 3.6130e-02, -3.0657e-02, -7.0840e-03,  2.5469e-02, -2.2885e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9617e-02,  3.0415e-02,  4.4007e-02,  2.5648e-02,  3.3053e-02],\n",
      "          [-4.0898e-02,  4.4382e-02, -4.1308e-02, -4.2249e-02, -1.5990e-02],\n",
      "          [-2.3080e-02,  2.6100e-02,  3.6006e-02,  4.3826e-02, -2.4295e-02],\n",
      "          [ 3.9480e-02,  2.6450e-02,  3.4130e-02, -2.2857e-02,  3.0930e-02],\n",
      "          [-4.2822e-02,  4.5047e-02, -1.6263e-02,  1.9267e-02, -1.6294e-02]],\n",
      "\n",
      "         [[ 4.9543e-02, -4.8064e-02, -4.6444e-02, -3.5293e-02,  3.9531e-02],\n",
      "          [ 2.7857e-02, -2.8445e-02,  1.2533e-02,  2.0034e-02, -3.8960e-02],\n",
      "          [ 3.8351e-02, -2.1417e-02,  2.3045e-03,  1.6014e-02,  8.8367e-03],\n",
      "          [-4.6684e-03,  3.0546e-02, -8.4638e-03, -3.7940e-02,  4.6685e-02],\n",
      "          [ 2.5415e-02,  4.8793e-02, -2.8895e-02,  1.5845e-02,  3.6501e-02]],\n",
      "\n",
      "         [[-4.9453e-02, -3.2241e-02,  1.2178e-02,  3.0094e-02,  1.7412e-03],\n",
      "          [-1.3840e-02,  2.4905e-02,  7.0294e-03, -3.2567e-02, -1.3772e-02],\n",
      "          [-2.3167e-02, -3.0344e-02,  4.0640e-02, -3.4775e-02, -4.3705e-02],\n",
      "          [-3.5833e-03, -4.5111e-02, -3.9034e-02,  8.3836e-03,  2.9099e-03],\n",
      "          [-8.9463e-03,  1.9671e-02,  3.9773e-02, -1.9639e-02,  3.6334e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1238e-02,  4.0813e-02, -4.6751e-02, -3.7031e-02,  2.4127e-02],\n",
      "          [-2.9110e-02,  4.1023e-02, -4.0154e-02, -3.8716e-02,  4.9854e-02],\n",
      "          [ 4.8452e-02,  3.5135e-02, -1.0232e-02,  4.3235e-02, -2.5110e-02],\n",
      "          [-2.7921e-02, -1.2594e-02, -3.3252e-02,  8.9640e-03, -4.5346e-02],\n",
      "          [ 4.2808e-02, -3.9377e-02, -3.8000e-02,  1.0401e-02, -1.2805e-02]],\n",
      "\n",
      "         [[-3.2482e-02,  2.7836e-02, -4.2324e-02, -1.4237e-02,  7.8726e-03],\n",
      "          [ 2.3153e-02,  3.8704e-02,  4.2865e-02,  2.6868e-02, -1.2175e-02],\n",
      "          [ 6.2615e-03, -3.7063e-02, -2.6040e-02,  4.1898e-02,  2.8352e-02],\n",
      "          [-2.3265e-02, -1.5303e-02, -3.6434e-02, -4.9462e-02, -2.2938e-02],\n",
      "          [ 2.5311e-02, -2.4297e-02,  2.9067e-02,  3.7331e-02,  3.6115e-02]],\n",
      "\n",
      "         [[ 2.8162e-02, -1.9500e-02,  3.1025e-03,  4.2720e-03, -3.0954e-02],\n",
      "          [-7.3380e-03, -3.1816e-02,  4.7280e-02, -3.3852e-02, -4.0168e-02],\n",
      "          [ 1.9843e-02,  4.9507e-04, -1.4607e-02, -8.2879e-03,  4.2957e-02],\n",
      "          [ 4.7546e-02, -2.6462e-02, -2.4232e-02,  1.4294e-02, -3.7717e-02],\n",
      "          [ 4.4676e-02,  1.6066e-02, -4.2428e-03,  3.4236e-02,  4.1690e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6447e-02, -2.4040e-03, -4.5425e-02,  4.2648e-02, -6.3528e-03],\n",
      "          [-4.5461e-02,  9.2852e-03,  1.6291e-03, -4.3453e-03,  3.7995e-02],\n",
      "          [ 1.2606e-02, -4.0405e-02, -1.4528e-02,  2.9360e-02,  4.1798e-02],\n",
      "          [-1.6600e-02, -3.1354e-02, -3.7928e-02, -3.8552e-02,  3.0495e-02],\n",
      "          [-2.5486e-02,  4.2332e-02,  5.7314e-03,  4.8501e-02,  3.9117e-02]],\n",
      "\n",
      "         [[-3.7631e-02, -2.7097e-02,  1.5369e-02,  7.8455e-03, -2.9434e-02],\n",
      "          [ 4.1393e-03, -1.3173e-02, -3.5556e-02,  1.2044e-03,  1.0399e-02],\n",
      "          [ 3.9013e-02,  3.9269e-02,  4.6857e-02, -2.5063e-02, -2.7753e-02],\n",
      "          [ 2.5236e-02,  1.6421e-02, -3.1622e-02, -4.6886e-02,  3.8023e-02],\n",
      "          [-1.0191e-02,  3.5253e-02, -3.4528e-02,  1.7630e-03, -3.5367e-02]],\n",
      "\n",
      "         [[-3.9047e-02,  1.1939e-02,  2.2202e-02,  1.5796e-04,  1.9665e-03],\n",
      "          [-2.2258e-02,  4.4145e-02,  8.6887e-03, -1.8474e-02, -3.3091e-02],\n",
      "          [-1.5056e-02,  4.9264e-02, -2.6461e-02,  1.8994e-02, -4.4615e-02],\n",
      "          [ 2.0600e-02, -1.1711e-02, -7.9916e-03,  2.5012e-02, -9.0971e-03],\n",
      "          [-1.7868e-02, -2.6955e-04, -4.4445e-03, -9.0808e-04, -3.2018e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1108e-02, -1.5087e-02,  2.0692e-02, -1.8760e-02, -2.7286e-02],\n",
      "          [ 4.3983e-02,  2.7179e-02, -4.5421e-02, -2.5363e-02, -4.0920e-02],\n",
      "          [-1.1493e-02, -1.5808e-02,  1.0130e-02,  3.2616e-02,  2.8830e-02],\n",
      "          [ 4.8173e-02, -2.8786e-02, -4.9174e-03, -9.7227e-03,  9.4452e-03],\n",
      "          [ 1.8393e-02, -4.5798e-02,  4.6060e-02, -4.1783e-02, -1.2252e-02]],\n",
      "\n",
      "         [[-1.7112e-04, -2.6721e-02, -1.7776e-02, -8.0860e-03, -1.4206e-03],\n",
      "          [ 4.8927e-02, -6.7047e-03, -2.5096e-02, -1.8784e-02,  3.6754e-02],\n",
      "          [ 1.4583e-02,  2.9577e-02,  3.7715e-02, -2.2187e-02,  5.4687e-03],\n",
      "          [-9.4650e-03, -3.6813e-03,  3.5474e-02, -1.9587e-02,  2.4661e-02],\n",
      "          [ 1.9503e-02,  4.8432e-02,  1.4403e-02, -1.5503e-03, -4.8785e-02]],\n",
      "\n",
      "         [[ 3.5396e-02,  3.6857e-02, -3.6156e-03,  2.8097e-04,  4.4818e-02],\n",
      "          [ 3.4694e-02,  4.9917e-02, -2.4963e-02, -2.8436e-02, -4.4628e-02],\n",
      "          [ 2.4180e-02, -2.6993e-02,  8.1985e-03, -4.2497e-02, -1.6871e-03],\n",
      "          [ 3.2828e-03, -4.6432e-02,  3.6347e-02, -2.6627e-02,  4.6759e-02],\n",
      "          [-7.9015e-03,  2.2067e-02,  1.9978e-02, -2.0753e-02, -2.1144e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5033e-02, -2.0951e-02,  2.2390e-02, -1.8609e-02,  3.8581e-02],\n",
      "          [ 2.6470e-02, -3.1624e-02, -4.3421e-02, -5.2048e-03, -1.5297e-02],\n",
      "          [-6.6310e-03, -4.3590e-02, -4.7145e-02, -4.8639e-02, -3.2277e-02],\n",
      "          [-2.9268e-04, -4.8820e-02, -2.0478e-02,  4.4209e-02,  6.4462e-03],\n",
      "          [-4.1772e-02, -5.1344e-03, -3.3830e-02,  2.0056e-02, -1.2903e-02]],\n",
      "\n",
      "         [[-2.2618e-02,  1.0915e-02, -2.5120e-02,  3.7832e-02,  4.9061e-02],\n",
      "          [ 2.0036e-02, -4.2426e-02, -1.6692e-03, -4.3690e-02, -2.9771e-02],\n",
      "          [-4.7017e-02, -2.3739e-02,  2.7530e-02, -3.1986e-02, -4.2667e-02],\n",
      "          [ 7.1012e-03, -4.7821e-03,  4.3262e-02, -3.9690e-02,  4.7885e-02],\n",
      "          [-8.1102e-04, -1.7898e-02, -1.2833e-03,  4.4951e-03,  7.1468e-03]],\n",
      "\n",
      "         [[ 1.8449e-02, -2.2603e-02,  1.0486e-02, -4.7518e-02, -4.1678e-02],\n",
      "          [-2.3900e-02, -3.1301e-02, -4.7617e-02, -4.5449e-02, -4.4066e-02],\n",
      "          [-1.8024e-02, -4.4081e-02, -3.0147e-02, -3.1891e-02,  1.8049e-02],\n",
      "          [-3.8336e-02,  4.5972e-02, -4.9006e-02, -1.4716e-02, -4.4477e-02],\n",
      "          [ 4.5890e-03, -2.5547e-02,  3.0133e-02,  2.4687e-02,  4.4926e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.7002e-03, -3.3051e-02, -1.7232e-02,  9.2653e-03,  1.9192e-03],\n",
      "          [-4.3208e-02, -1.4330e-02, -7.3801e-03, -2.8910e-02,  2.5046e-02],\n",
      "          [ 8.9609e-03, -3.7945e-02, -2.9334e-03, -3.2330e-02,  3.9732e-02],\n",
      "          [ 4.5658e-02,  4.8428e-02,  4.4469e-02, -1.6705e-02,  4.7388e-02],\n",
      "          [ 3.4542e-02,  4.3896e-03, -1.5540e-02,  3.1677e-03,  4.6292e-02]],\n",
      "\n",
      "         [[-2.6978e-02, -1.2549e-02, -1.0320e-02, -3.7440e-02, -1.7161e-02],\n",
      "          [-1.0647e-02,  8.2668e-03, -3.4752e-02,  1.9262e-02,  2.5834e-03],\n",
      "          [ 3.3475e-02,  3.2575e-03, -8.4353e-04,  3.7230e-03,  2.6845e-02],\n",
      "          [-3.7331e-02,  3.3538e-02, -3.0674e-02,  5.0032e-03,  1.9811e-02],\n",
      "          [-1.8179e-02,  4.1532e-02, -3.1875e-02,  2.1940e-02, -1.2358e-04]],\n",
      "\n",
      "         [[-4.6767e-02, -3.9625e-02, -1.8852e-03,  1.9525e-02,  1.0846e-03],\n",
      "          [-1.7913e-02, -1.1678e-03, -2.4248e-02,  4.4486e-02, -5.4372e-03],\n",
      "          [ 3.8232e-02,  3.6077e-02,  1.6273e-02,  3.8056e-02, -8.6034e-03],\n",
      "          [ 4.1202e-02, -1.7147e-02,  1.8478e-02, -4.8954e-02,  2.4316e-02],\n",
      "          [-4.4139e-02, -2.0227e-02, -4.7297e-02, -1.3449e-02, -3.1743e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8041e-03, -1.5854e-04, -4.6667e-02, -4.1823e-02,  3.8202e-02],\n",
      "          [ 4.6668e-02, -2.6221e-02, -3.8264e-02,  2.3253e-02,  2.2794e-02],\n",
      "          [-2.4470e-02, -2.9039e-03,  2.5965e-02,  3.3588e-02, -3.8732e-02],\n",
      "          [ 1.0103e-02,  1.8496e-02,  2.6533e-02,  2.7266e-02, -7.8189e-03],\n",
      "          [ 2.5037e-02,  3.0829e-02,  1.1061e-02,  1.0824e-02,  3.5539e-03]],\n",
      "\n",
      "         [[-5.5946e-03, -4.2657e-02,  2.5169e-02, -1.6140e-02, -4.4170e-03],\n",
      "          [-4.9443e-02,  1.6870e-03,  2.2075e-02, -9.2491e-03, -1.1001e-02],\n",
      "          [-4.2247e-02,  3.9101e-02,  1.9890e-02,  3.6777e-02,  1.3158e-03],\n",
      "          [-1.6075e-02,  1.8132e-02,  2.3061e-02,  4.4875e-02,  1.7571e-02],\n",
      "          [-2.8236e-02,  4.7425e-02,  3.9351e-02,  4.1477e-02, -2.1579e-02]],\n",
      "\n",
      "         [[ 9.1032e-03, -4.0536e-02, -2.7178e-02, -2.0937e-02,  2.6969e-02],\n",
      "          [ 4.9945e-02, -4.6775e-02,  3.9418e-02, -3.1401e-02,  1.7959e-02],\n",
      "          [-5.2428e-03,  4.2706e-02, -4.8915e-02, -2.8814e-02, -4.6059e-02],\n",
      "          [ 7.7509e-03, -3.7212e-02, -7.8347e-03, -2.1385e-02, -4.7304e-02],\n",
      "          [ 6.4067e-03, -1.8575e-02,  2.7351e-03, -2.0907e-02, -1.8379e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2798e-02, -1.1419e-02, -8.9743e-03, -3.3818e-03,  3.5928e-02],\n",
      "          [ 2.9698e-02, -2.1536e-02,  3.5015e-02, -2.8172e-02, -8.6412e-03],\n",
      "          [-4.0023e-02,  9.8872e-03, -5.5102e-03, -7.4170e-03, -4.4846e-02],\n",
      "          [-3.2702e-02, -3.6702e-02,  1.5848e-02,  1.5562e-02, -1.4578e-02],\n",
      "          [-1.4827e-02, -2.7020e-02,  1.5403e-02,  1.5775e-02,  3.6539e-02]],\n",
      "\n",
      "         [[-3.4638e-02, -3.4350e-02, -1.6380e-02, -4.2397e-02,  3.7547e-02],\n",
      "          [-1.7579e-02,  2.9055e-02, -1.3205e-02,  1.3699e-02,  1.3385e-02],\n",
      "          [-4.0517e-02,  3.9617e-03, -5.9333e-03, -1.6725e-02, -3.6646e-02],\n",
      "          [ 8.8213e-03, -1.9620e-02,  4.9243e-02,  3.2658e-02, -3.7438e-03],\n",
      "          [ 3.1270e-03, -4.8235e-02, -3.7929e-02, -4.4305e-03, -3.9181e-02]],\n",
      "\n",
      "         [[ 1.4594e-02, -8.4175e-03,  4.0091e-03, -1.0225e-03,  4.0828e-02],\n",
      "          [ 1.4715e-02, -1.9904e-02, -4.8048e-02,  2.3451e-02,  2.8953e-02],\n",
      "          [-4.6233e-02, -1.3085e-02, -4.3738e-02,  7.1743e-03, -3.4417e-02],\n",
      "          [ 1.3599e-02, -3.3360e-02, -9.5195e-03, -3.8468e-02, -1.7404e-03],\n",
      "          [-3.1478e-02,  4.3487e-02, -4.4233e-02, -5.9967e-03, -2.4757e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6402e-03,  3.9844e-02, -2.6103e-02,  4.7476e-02,  2.5109e-02],\n",
      "          [ 2.9156e-02, -3.7572e-02, -4.1164e-02,  7.1113e-03,  2.7514e-02],\n",
      "          [-5.5878e-03,  4.9367e-02, -3.6786e-02,  8.3731e-03,  9.2155e-03],\n",
      "          [ 6.2146e-03, -2.2548e-02, -1.5724e-02, -2.3313e-02, -3.9454e-02],\n",
      "          [ 1.6823e-02, -3.2644e-02,  2.9877e-02,  2.5681e-02,  9.4839e-03]],\n",
      "\n",
      "         [[-2.9321e-02, -1.0392e-02, -2.1852e-02,  3.9548e-02,  4.9254e-02],\n",
      "          [-2.9349e-02,  1.7568e-02, -4.2921e-02,  2.7152e-02,  2.4276e-02],\n",
      "          [-2.9805e-03, -4.6216e-02,  1.2556e-03,  3.0472e-02, -4.6747e-02],\n",
      "          [-4.8876e-02, -2.1980e-02,  3.9305e-02, -3.0425e-02,  2.5106e-02],\n",
      "          [-4.6185e-02, -1.1344e-02, -1.2940e-02, -1.9077e-02, -6.9353e-04]],\n",
      "\n",
      "         [[-8.3027e-03, -2.4352e-02,  1.0615e-02, -1.9258e-02, -3.6096e-02],\n",
      "          [ 4.0168e-02, -2.2954e-02,  3.9840e-02,  3.5643e-02,  1.1487e-02],\n",
      "          [-4.5012e-02,  3.1134e-02,  2.4030e-02, -1.6509e-02,  2.9503e-02],\n",
      "          [-4.7987e-02,  4.8546e-02, -2.0944e-02,  1.9593e-02, -3.2254e-02],\n",
      "          [-4.2521e-02, -4.7807e-02,  3.2021e-02, -3.2561e-02, -8.6281e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.4838e-02, -2.4185e-02,  9.6389e-03,  4.6201e-02,  4.4275e-02],\n",
      "          [-2.4615e-02,  3.4543e-02, -3.2299e-02, -1.8806e-02,  3.9071e-02],\n",
      "          [-3.6036e-02, -2.5228e-02,  2.1181e-02,  1.1009e-02,  2.8248e-02],\n",
      "          [ 1.7336e-02, -3.7785e-02, -1.4166e-02,  5.4557e-03,  1.1758e-02],\n",
      "          [-2.7704e-02, -3.1257e-02,  1.3790e-03, -1.2437e-02,  1.2998e-02]],\n",
      "\n",
      "         [[ 2.6265e-02, -2.6065e-02, -4.0896e-02,  1.1692e-02, -3.1374e-02],\n",
      "          [ 2.8033e-02, -3.7718e-03,  2.6187e-02, -4.3865e-02,  2.1388e-02],\n",
      "          [ 2.0751e-03, -2.7462e-02, -4.5477e-02,  4.1714e-02,  4.3692e-03],\n",
      "          [ 8.6314e-03, -2.0456e-02, -1.9053e-02, -2.4676e-02, -1.5213e-02],\n",
      "          [ 2.3108e-02, -1.4828e-02,  4.0209e-03, -3.6080e-02,  4.2858e-02]],\n",
      "\n",
      "         [[-4.0245e-02,  4.6752e-02,  1.7118e-05, -1.4165e-02,  7.0845e-04],\n",
      "          [ 3.8839e-02,  3.5254e-02, -4.8200e-02, -3.7539e-02, -6.3040e-03],\n",
      "          [-4.8243e-03, -3.8271e-02, -1.0680e-02,  4.9799e-02, -1.3852e-02],\n",
      "          [-1.4980e-02,  2.7315e-02,  3.6978e-02, -1.4568e-02, -3.0154e-03],\n",
      "          [-4.6811e-02, -4.2413e-02, -3.1797e-02, -1.7581e-02, -3.3740e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9958e-02, -1.5411e-02, -1.2081e-02,  1.2924e-02, -4.2298e-02],\n",
      "          [ 1.0173e-02, -6.2731e-03,  2.3234e-02,  3.8453e-02, -3.0057e-02],\n",
      "          [-1.9575e-02, -3.4408e-03,  1.7851e-02,  9.6166e-03, -2.6493e-02],\n",
      "          [-1.8384e-02, -2.1104e-02, -4.1645e-02,  6.6411e-03,  2.2199e-02],\n",
      "          [-5.7619e-03,  2.7090e-03, -2.4894e-02, -2.8245e-02, -4.1705e-02]],\n",
      "\n",
      "         [[-3.3047e-02,  2.9025e-02, -8.1869e-04,  2.8711e-03,  6.0536e-03],\n",
      "          [-3.3577e-02, -4.4899e-02,  3.9642e-02, -8.7377e-04,  4.0603e-02],\n",
      "          [-1.9521e-02,  5.9897e-04,  2.3151e-02, -3.9023e-02, -2.5845e-02],\n",
      "          [ 2.5026e-02,  6.2019e-03,  2.7251e-02, -1.5553e-02,  1.8291e-02],\n",
      "          [ 4.6223e-02,  2.9872e-02,  2.9269e-02,  4.5827e-03,  1.3872e-02]],\n",
      "\n",
      "         [[ 1.2193e-02,  1.7444e-02,  4.9416e-02,  3.4372e-02,  2.6414e-02],\n",
      "          [-3.0374e-02,  3.5201e-02, -1.0977e-02, -6.9667e-03,  1.0544e-02],\n",
      "          [ 3.3997e-02, -9.7400e-03,  2.0858e-02, -1.6117e-02,  6.1452e-03],\n",
      "          [-3.7668e-02,  6.4159e-03,  9.1491e-03, -4.6204e-02, -3.9065e-02],\n",
      "          [ 3.7865e-02, -1.3862e-03,  1.6208e-02,  3.3890e-03,  1.9372e-02]]]])), ('conv2.bias', tensor([-0.0447,  0.0163, -0.0073, -0.0348,  0.0413,  0.0055,  0.0460,  0.0007,\n",
      "         0.0146, -0.0194, -0.0278, -0.0033,  0.0466,  0.0481,  0.0447,  0.0394,\n",
      "         0.0332,  0.0445, -0.0059, -0.0492,  0.0054, -0.0423, -0.0326,  0.0472,\n",
      "         0.0443, -0.0434,  0.0463,  0.0292, -0.0300,  0.0026,  0.0177, -0.0454])), ('fc1.weight', tensor([[ 0.0209,  0.0166,  0.0066,  ..., -0.0107, -0.0229,  0.0168],\n",
      "        [-0.0028, -0.0025, -0.0123,  ..., -0.0179, -0.0100,  0.0090],\n",
      "        [ 0.0239,  0.0148, -0.0247,  ..., -0.0091, -0.0164, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0219,  0.0051,  0.0166,  ...,  0.0096, -0.0242, -0.0167],\n",
      "        [-0.0125, -0.0169,  0.0161,  ...,  0.0070, -0.0205, -0.0091],\n",
      "        [-0.0136, -0.0006,  0.0123,  ..., -0.0086, -0.0211, -0.0003]])), ('fc1.bias', tensor([ 0.0154,  0.0087,  0.0089,  0.0017, -0.0144,  0.0034,  0.0191, -0.0178,\n",
      "        -0.0060, -0.0162]))])\n",
      "Establishing client devices...\n",
      "Training dataset has been distributed into 3 pieces.\n",
      "Established 3 client devices.\n",
      "!-- Client 1 is normal and start iterations. ---!\n",
      "Epoch [1/2], Train Loss: 1.8918178081512451, Train Accuracy: 0.3680000007152557, Train Error: 0.6320000290870667, Culminative Time Used: 0.8842336013913155\n",
      "Epoch [2/2], Train Loss: 0.6320134401321411, Train Accuracy: 0.8020000457763672, Train Error: 0.1980000138282776, Culminative Time Used: 1.7295829020440578\n",
      "!-- Client 0 is normal and start iterations. ---!\n",
      "Epoch [1/2], Train Loss: 1.8412593603134155, Train Accuracy: 0.3620000183582306, Train Error: 0.6380000114440918, Culminative Time Used: 0.8550206981599331\n",
      "Epoch [2/2], Train Loss: 0.4743083715438843, Train Accuracy: 0.8570000529289246, Train Error: 0.1430000066757202, Culminative Time Used: 1.722629301249981\n",
      "!-- Client 2 is normal and start iterations. ---!\n",
      "Epoch [1/2], Train Loss: 1.4824527502059937, Train Accuracy: 0.5270000100135803, Train Error: 0.4729999601840973, Culminative Time Used: 0.8585234992206097\n",
      "Epoch [2/2], Train Loss: 0.4662519693374634, Train Accuracy: 0.8600000143051147, Train Error: 0.1400000154972076, Culminative Time Used: 1.693496696650982\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "XXX\n",
      "tensor([[[[-0.3534, -0.0775, -0.2170, -0.0648, -0.2574],\n",
      "          [-0.1285, -0.0360, -0.3809, -0.1319,  0.0790],\n",
      "          [-0.2367, -0.1869, -0.3021, -0.1457, -0.3289],\n",
      "          [-0.2963, -0.2122, -0.0709, -0.2881, -0.3228],\n",
      "          [-0.2129, -0.1912, -0.0545, -0.1265, -0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0398, -0.2739, -0.2488,  0.2464,  0.0336],\n",
      "          [-0.2380, -0.1718, -0.2428,  0.1572,  0.2017],\n",
      "          [-0.0045, -0.1489, -0.1361,  0.1023, -0.0329],\n",
      "          [-0.1831, -0.0865, -0.0721,  0.0356,  0.1351],\n",
      "          [-0.1985, -0.0354, -0.0151, -0.1055,  0.0277]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4558,  0.4269,  0.7044,  0.7871,  0.7144],\n",
      "          [-0.0865, -0.0756, -0.2062, -0.0510,  0.3433],\n",
      "          [-0.3529, -0.2855, -0.1159, -0.2611, -0.3799],\n",
      "          [-0.3167, -0.0966, -0.3094, -0.2901, -0.2128],\n",
      "          [-0.3107, -0.1675, -0.3478, -0.0828,  0.0798]]],\n",
      "\n",
      "\n",
      "        [[[-0.1642, -0.3771, -0.0997, -0.2634,  0.0537],\n",
      "          [-0.0345, -0.2864, -0.2782, -0.1327,  0.1041],\n",
      "          [-0.0799, -0.2151, -0.1907, -0.3254,  0.3906],\n",
      "          [-0.2169,  0.0289, -0.3304, -0.3207,  0.5507],\n",
      "          [-0.1190, -0.2564, -0.3080,  0.3290,  0.4491]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447,  0.2039,  0.2707, -0.0486, -0.4627],\n",
      "          [-0.3153, -0.0855,  0.1059,  0.1711,  0.0786],\n",
      "          [-0.3332, -0.1144, -0.1058,  0.4250, -0.0332],\n",
      "          [-0.2380, -0.3317,  0.0606,  0.2288,  0.0492],\n",
      "          [-0.2828, -0.0523,  0.0828,  0.1692, -0.1281]]],\n",
      "\n",
      "\n",
      "        [[[-0.5169, -0.0660,  0.1490,  0.1869,  0.1826],\n",
      "          [-0.2383,  0.0344,  0.1925, -0.1137,  0.0868],\n",
      "          [-0.1636,  0.1577,  0.0651,  0.2105, -0.0157],\n",
      "          [-0.0109,  0.2631,  0.1229,  0.1464, -0.3188],\n",
      "          [ 0.0702,  0.2072, -0.0254, -0.2533, -0.3859]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1159, -0.0875, -0.0219, -0.1258,  0.1663],\n",
      "          [ 0.2344, -0.0094,  0.1087, -0.1233,  0.0373],\n",
      "          [-0.1252, -0.1948, -0.0902,  0.0399,  0.1324],\n",
      "          [-0.1910,  0.1339,  0.0158, -0.2385, -0.0878],\n",
      "          [ 0.0913, -0.1149,  0.0429,  0.0236, -0.1270]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0704,  0.0991,  0.2622,  0.2554,  0.3195],\n",
      "          [-0.1178, -0.1685, -0.0320, -0.0562,  0.2582],\n",
      "          [-0.1303, -0.1477, -0.0807, -0.2319, -0.0218],\n",
      "          [-0.1969, -0.2316, -0.0174, -0.2237, -0.2560],\n",
      "          [-0.0306, -0.1192,  0.0485, -0.0051,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134, -0.0545, -0.1768, -0.4554, -0.3599],\n",
      "          [-0.2390, -0.0500, -0.3412, -0.3016, -0.2818],\n",
      "          [-0.0811, -0.4019,  0.0009, -0.0029, -0.0376],\n",
      "          [-0.3476, -0.1263, -0.3397, -0.1711, -0.2363],\n",
      "          [-0.2572, -0.1527, -0.0478, -0.4132, -0.0643]]],\n",
      "\n",
      "\n",
      "        [[[-0.1372, -0.0975, -0.3247, -0.1888, -0.0162],\n",
      "          [-0.2035, -0.2968, -0.3310, -0.1734, -0.0552],\n",
      "          [-0.2458, -0.1255,  0.0037, -0.1607, -0.1359],\n",
      "          [ 0.1788,  0.0091,  0.0151, -0.0618, -0.1518],\n",
      "          [ 0.4057,  0.3699,  0.1382,  0.0994,  0.0575]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0819,  0.0695,  0.0160,  0.0799, -0.1755],\n",
      "          [-0.1915,  0.1297, -0.1919,  0.0709, -0.0906],\n",
      "          [ 0.0516, -0.1001, -0.0520,  0.1612,  0.1428],\n",
      "          [-0.1415,  0.0579, -0.0806, -0.0740, -0.0465],\n",
      "          [ 0.1887, -0.0732, -0.0911,  0.0091,  0.0500]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702,  0.2573, -0.0742,  0.1404, -0.1008],\n",
      "          [-0.0595, -0.1277, -0.1377, -0.1938, -0.0871],\n",
      "          [-0.0655, -0.0650,  0.1230,  0.1066, -0.0605],\n",
      "          [-0.1230,  0.0035, -0.1713, -0.1727, -0.0073],\n",
      "          [-0.0261, -0.0847,  0.1144, -0.1058, -0.1982]]],\n",
      "\n",
      "\n",
      "        [[[-0.2073, -0.1550, -0.4371, -0.1172,  0.2604],\n",
      "          [-0.3073, -0.2608, -0.2520, -0.0908,  0.4761],\n",
      "          [-0.3519, -0.1031, -0.3146, -0.1106,  0.4048],\n",
      "          [-0.1314, -0.2409, -0.1149,  0.2292,  0.2726],\n",
      "          [-0.2173, -0.0267, -0.0272,  0.2367,  0.4805]]],\n",
      "\n",
      "\n",
      "        [[[-0.0086,  0.0956,  0.1224, -0.1360, -0.1918],\n",
      "          [ 0.1716, -0.0269,  0.1740, -0.2093, -0.0027],\n",
      "          [-0.1335, -0.1691,  0.0543, -0.1465, -0.1489],\n",
      "          [-0.1750,  0.1845,  0.1581, -0.1298, -0.0885],\n",
      "          [-0.1216,  0.1228,  0.1647, -0.2338,  0.0471]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664,  0.1222,  0.2296,  0.3579,  0.3757],\n",
      "          [-0.0805, -0.0474, -0.1900, -0.0737, -0.1342],\n",
      "          [ 0.0191,  0.0691, -0.0890, -0.0732, -0.1790],\n",
      "          [-0.0756, -0.1280,  0.0194, -0.1446, -0.0484],\n",
      "          [-0.1837, -0.2152, -0.0821, -0.1972,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165,  0.3216,  0.2215, -0.1209, -0.3748],\n",
      "          [-0.0152,  0.2338, -0.0607,  0.0736, -0.1489],\n",
      "          [ 0.2349,  0.1609,  0.0908, -0.1540, -0.2327],\n",
      "          [-0.1526,  0.1049,  0.1905, -0.1133,  0.0734],\n",
      "          [-0.0465,  0.0447,  0.1887, -0.1350,  0.0966]]]])\n",
      "YYY\n",
      "tensor([[[[-0.1178, -0.0258, -0.0723, -0.0216, -0.0858],\n",
      "          [-0.0428, -0.0120, -0.1270, -0.0440,  0.0263],\n",
      "          [-0.0789, -0.0623, -0.1007, -0.0486, -0.1096],\n",
      "          [-0.0988, -0.0707, -0.0236, -0.0960, -0.1076],\n",
      "          [-0.0710, -0.0637, -0.0182, -0.0422, -0.0025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0133, -0.0913, -0.0829,  0.0821,  0.0112],\n",
      "          [-0.0793, -0.0573, -0.0809,  0.0524,  0.0672],\n",
      "          [-0.0015, -0.0496, -0.0454,  0.0341, -0.0110],\n",
      "          [-0.0610, -0.0288, -0.0240,  0.0119,  0.0450],\n",
      "          [-0.0662, -0.0118, -0.0050, -0.0352,  0.0092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1519,  0.1423,  0.2348,  0.2624,  0.2381],\n",
      "          [-0.0288, -0.0252, -0.0687, -0.0170,  0.1144],\n",
      "          [-0.1176, -0.0952, -0.0386, -0.0870, -0.1266],\n",
      "          [-0.1056, -0.0322, -0.1031, -0.0967, -0.0709],\n",
      "          [-0.1036, -0.0558, -0.1159, -0.0276,  0.0266]]],\n",
      "\n",
      "\n",
      "        [[[-0.0547, -0.1257, -0.0332, -0.0878,  0.0179],\n",
      "          [-0.0115, -0.0955, -0.0927, -0.0442,  0.0347],\n",
      "          [-0.0266, -0.0717, -0.0636, -0.1085,  0.1302],\n",
      "          [-0.0723,  0.0096, -0.1101, -0.1069,  0.1836],\n",
      "          [-0.0397, -0.0855, -0.1027,  0.1097,  0.1497]]],\n",
      "\n",
      "\n",
      "        [[[-0.0482,  0.0680,  0.0902, -0.0162, -0.1542],\n",
      "          [-0.1051, -0.0285,  0.0353,  0.0570,  0.0262],\n",
      "          [-0.1111, -0.0381, -0.0353,  0.1417, -0.0111],\n",
      "          [-0.0793, -0.1106,  0.0202,  0.0763,  0.0164],\n",
      "          [-0.0943, -0.0174,  0.0276,  0.0564, -0.0427]]],\n",
      "\n",
      "\n",
      "        [[[-0.1723, -0.0220,  0.0497,  0.0623,  0.0609],\n",
      "          [-0.0794,  0.0115,  0.0642, -0.0379,  0.0289],\n",
      "          [-0.0545,  0.0526,  0.0217,  0.0702, -0.0052],\n",
      "          [-0.0036,  0.0877,  0.0410,  0.0488, -0.1063],\n",
      "          [ 0.0234,  0.0691, -0.0085, -0.0844, -0.1286]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0386, -0.0292, -0.0073, -0.0419,  0.0554],\n",
      "          [ 0.0781, -0.0031,  0.0362, -0.0411,  0.0124],\n",
      "          [-0.0417, -0.0649, -0.0301,  0.0133,  0.0441],\n",
      "          [-0.0637,  0.0446,  0.0053, -0.0795, -0.0293],\n",
      "          [ 0.0304, -0.0383,  0.0143,  0.0079, -0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0235,  0.0330,  0.0874,  0.0851,  0.1065],\n",
      "          [-0.0393, -0.0562, -0.0107, -0.0187,  0.0861],\n",
      "          [-0.0434, -0.0492, -0.0269, -0.0773, -0.0073],\n",
      "          [-0.0656, -0.0772, -0.0058, -0.0746, -0.0853],\n",
      "          [-0.0102, -0.0397,  0.0162, -0.0017,  0.0161]]],\n",
      "\n",
      "\n",
      "        [[[-0.0045, -0.0182, -0.0589, -0.1518, -0.1200],\n",
      "          [-0.0797, -0.0167, -0.1137, -0.1005, -0.0939],\n",
      "          [-0.0270, -0.1340,  0.0003, -0.0010, -0.0125],\n",
      "          [-0.1159, -0.0421, -0.1132, -0.0570, -0.0788],\n",
      "          [-0.0857, -0.0509, -0.0159, -0.1377, -0.0214]]],\n",
      "\n",
      "\n",
      "        [[[-0.0457, -0.0325, -0.1082, -0.0629, -0.0054],\n",
      "          [-0.0678, -0.0989, -0.1103, -0.0578, -0.0184],\n",
      "          [-0.0819, -0.0418,  0.0012, -0.0536, -0.0453],\n",
      "          [ 0.0596,  0.0030,  0.0050, -0.0206, -0.0506],\n",
      "          [ 0.1352,  0.1233,  0.0461,  0.0331,  0.0192]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0273,  0.0232,  0.0053,  0.0266, -0.0585],\n",
      "          [-0.0638,  0.0432, -0.0640,  0.0236, -0.0302],\n",
      "          [ 0.0172, -0.0334, -0.0173,  0.0537,  0.0476],\n",
      "          [-0.0472,  0.0193, -0.0269, -0.0247, -0.0155],\n",
      "          [ 0.0629, -0.0244, -0.0304,  0.0030,  0.0167]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0567,  0.0858, -0.0247,  0.0468, -0.0336],\n",
      "          [-0.0198, -0.0426, -0.0459, -0.0646, -0.0290],\n",
      "          [-0.0218, -0.0217,  0.0410,  0.0355, -0.0202],\n",
      "          [-0.0410,  0.0012, -0.0571, -0.0576, -0.0024],\n",
      "          [-0.0087, -0.0282,  0.0381, -0.0353, -0.0661]]],\n",
      "\n",
      "\n",
      "        [[[-0.0691, -0.0517, -0.1457, -0.0391,  0.0868],\n",
      "          [-0.1024, -0.0869, -0.0840, -0.0303,  0.1587],\n",
      "          [-0.1173, -0.0344, -0.1049, -0.0369,  0.1349],\n",
      "          [-0.0438, -0.0803, -0.0383,  0.0764,  0.0909],\n",
      "          [-0.0724, -0.0089, -0.0091,  0.0789,  0.1602]]],\n",
      "\n",
      "\n",
      "        [[[-0.0029,  0.0319,  0.0408, -0.0453, -0.0639],\n",
      "          [ 0.0572, -0.0090,  0.0580, -0.0698, -0.0009],\n",
      "          [-0.0445, -0.0564,  0.0181, -0.0488, -0.0496],\n",
      "          [-0.0583,  0.0615,  0.0527, -0.0433, -0.0295],\n",
      "          [-0.0405,  0.0409,  0.0549, -0.0779,  0.0157]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0221,  0.0407,  0.0765,  0.1193,  0.1252],\n",
      "          [-0.0268, -0.0158, -0.0633, -0.0246, -0.0447],\n",
      "          [ 0.0064,  0.0230, -0.0297, -0.0244, -0.0597],\n",
      "          [-0.0252, -0.0427,  0.0065, -0.0482, -0.0161],\n",
      "          [-0.0612, -0.0717, -0.0274, -0.0657,  0.0007]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0055,  0.1072,  0.0738, -0.0403, -0.1249],\n",
      "          [-0.0051,  0.0779, -0.0202,  0.0245, -0.0496],\n",
      "          [ 0.0783,  0.0536,  0.0303, -0.0513, -0.0776],\n",
      "          [-0.0509,  0.0350,  0.0635, -0.0378,  0.0245],\n",
      "          [-0.0155,  0.0149,  0.0629, -0.0450,  0.0322]]]])\n",
      "XXX\n",
      "tensor([[[[-0.3534, -0.0775, -0.2170, -0.0648, -0.2574],\n",
      "          [-0.1285, -0.0360, -0.3809, -0.1319,  0.0790],\n",
      "          [-0.2367, -0.1869, -0.3021, -0.1457, -0.3289],\n",
      "          [-0.2963, -0.2122, -0.0709, -0.2881, -0.3228],\n",
      "          [-0.2129, -0.1912, -0.0545, -0.1265, -0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0398, -0.2739, -0.2488,  0.2464,  0.0336],\n",
      "          [-0.2380, -0.1718, -0.2428,  0.1572,  0.2017],\n",
      "          [-0.0045, -0.1489, -0.1361,  0.1023, -0.0329],\n",
      "          [-0.1831, -0.0865, -0.0721,  0.0356,  0.1351],\n",
      "          [-0.1985, -0.0354, -0.0151, -0.1055,  0.0277]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4558,  0.4269,  0.7044,  0.7871,  0.7144],\n",
      "          [-0.0865, -0.0756, -0.2062, -0.0510,  0.3433],\n",
      "          [-0.3529, -0.2855, -0.1159, -0.2611, -0.3799],\n",
      "          [-0.3167, -0.0966, -0.3094, -0.2901, -0.2128],\n",
      "          [-0.3107, -0.1675, -0.3478, -0.0828,  0.0798]]],\n",
      "\n",
      "\n",
      "        [[[-0.1642, -0.3771, -0.0997, -0.2634,  0.0537],\n",
      "          [-0.0345, -0.2864, -0.2782, -0.1327,  0.1041],\n",
      "          [-0.0799, -0.2151, -0.1907, -0.3254,  0.3906],\n",
      "          [-0.2169,  0.0289, -0.3304, -0.3207,  0.5507],\n",
      "          [-0.1190, -0.2564, -0.3080,  0.3290,  0.4491]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447,  0.2039,  0.2707, -0.0486, -0.4627],\n",
      "          [-0.3153, -0.0855,  0.1059,  0.1711,  0.0786],\n",
      "          [-0.3332, -0.1144, -0.1058,  0.4250, -0.0332],\n",
      "          [-0.2380, -0.3317,  0.0606,  0.2288,  0.0492],\n",
      "          [-0.2828, -0.0523,  0.0828,  0.1692, -0.1281]]],\n",
      "\n",
      "\n",
      "        [[[-0.5169, -0.0660,  0.1490,  0.1869,  0.1826],\n",
      "          [-0.2383,  0.0344,  0.1925, -0.1137,  0.0868],\n",
      "          [-0.1636,  0.1577,  0.0651,  0.2105, -0.0157],\n",
      "          [-0.0109,  0.2631,  0.1229,  0.1464, -0.3188],\n",
      "          [ 0.0702,  0.2072, -0.0254, -0.2533, -0.3859]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1159, -0.0875, -0.0219, -0.1258,  0.1663],\n",
      "          [ 0.2344, -0.0094,  0.1087, -0.1233,  0.0373],\n",
      "          [-0.1252, -0.1948, -0.0902,  0.0399,  0.1324],\n",
      "          [-0.1910,  0.1339,  0.0158, -0.2385, -0.0878],\n",
      "          [ 0.0913, -0.1149,  0.0429,  0.0236, -0.1270]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0704,  0.0991,  0.2622,  0.2554,  0.3195],\n",
      "          [-0.1178, -0.1685, -0.0320, -0.0562,  0.2582],\n",
      "          [-0.1303, -0.1477, -0.0807, -0.2319, -0.0218],\n",
      "          [-0.1969, -0.2316, -0.0174, -0.2237, -0.2560],\n",
      "          [-0.0306, -0.1192,  0.0485, -0.0051,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134, -0.0545, -0.1768, -0.4554, -0.3599],\n",
      "          [-0.2390, -0.0500, -0.3412, -0.3016, -0.2818],\n",
      "          [-0.0811, -0.4019,  0.0009, -0.0029, -0.0376],\n",
      "          [-0.3476, -0.1263, -0.3397, -0.1711, -0.2363],\n",
      "          [-0.2572, -0.1527, -0.0478, -0.4132, -0.0643]]],\n",
      "\n",
      "\n",
      "        [[[-0.1372, -0.0975, -0.3247, -0.1888, -0.0162],\n",
      "          [-0.2035, -0.2968, -0.3310, -0.1734, -0.0552],\n",
      "          [-0.2458, -0.1255,  0.0037, -0.1607, -0.1359],\n",
      "          [ 0.1788,  0.0091,  0.0151, -0.0618, -0.1518],\n",
      "          [ 0.4057,  0.3699,  0.1382,  0.0994,  0.0575]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0819,  0.0695,  0.0160,  0.0799, -0.1755],\n",
      "          [-0.1915,  0.1297, -0.1919,  0.0709, -0.0906],\n",
      "          [ 0.0516, -0.1001, -0.0520,  0.1612,  0.1428],\n",
      "          [-0.1415,  0.0579, -0.0806, -0.0740, -0.0465],\n",
      "          [ 0.1887, -0.0732, -0.0911,  0.0091,  0.0500]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702,  0.2573, -0.0742,  0.1404, -0.1008],\n",
      "          [-0.0595, -0.1277, -0.1377, -0.1938, -0.0871],\n",
      "          [-0.0655, -0.0650,  0.1230,  0.1066, -0.0605],\n",
      "          [-0.1230,  0.0035, -0.1713, -0.1727, -0.0073],\n",
      "          [-0.0261, -0.0847,  0.1144, -0.1058, -0.1982]]],\n",
      "\n",
      "\n",
      "        [[[-0.2073, -0.1550, -0.4371, -0.1172,  0.2604],\n",
      "          [-0.3073, -0.2608, -0.2520, -0.0908,  0.4761],\n",
      "          [-0.3519, -0.1031, -0.3146, -0.1106,  0.4048],\n",
      "          [-0.1314, -0.2409, -0.1149,  0.2292,  0.2726],\n",
      "          [-0.2173, -0.0267, -0.0272,  0.2367,  0.4805]]],\n",
      "\n",
      "\n",
      "        [[[-0.0086,  0.0956,  0.1224, -0.1360, -0.1918],\n",
      "          [ 0.1716, -0.0269,  0.1740, -0.2093, -0.0027],\n",
      "          [-0.1335, -0.1691,  0.0543, -0.1465, -0.1489],\n",
      "          [-0.1750,  0.1845,  0.1581, -0.1298, -0.0885],\n",
      "          [-0.1216,  0.1228,  0.1647, -0.2338,  0.0471]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664,  0.1222,  0.2296,  0.3579,  0.3757],\n",
      "          [-0.0805, -0.0474, -0.1900, -0.0737, -0.1342],\n",
      "          [ 0.0191,  0.0691, -0.0890, -0.0732, -0.1790],\n",
      "          [-0.0756, -0.1280,  0.0194, -0.1446, -0.0484],\n",
      "          [-0.1837, -0.2152, -0.0821, -0.1972,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165,  0.3216,  0.2215, -0.1209, -0.3748],\n",
      "          [-0.0152,  0.2338, -0.0607,  0.0736, -0.1489],\n",
      "          [ 0.2349,  0.1609,  0.0908, -0.1540, -0.2327],\n",
      "          [-0.1526,  0.1049,  0.1905, -0.1133,  0.0734],\n",
      "          [-0.0465,  0.0447,  0.1887, -0.1350,  0.0966]]]])\n",
      "YYY\n",
      "tensor([[[[-0.2356, -0.0516, -0.1447, -0.0432, -0.1716],\n",
      "          [-0.0857, -0.0240, -0.2539, -0.0879,  0.0527],\n",
      "          [-0.1578, -0.1246, -0.2014, -0.0971, -0.2193],\n",
      "          [-0.1975, -0.1415, -0.0472, -0.1920, -0.2152],\n",
      "          [-0.1420, -0.1275, -0.0363, -0.0844, -0.0050]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0265, -0.1826, -0.1659,  0.1643,  0.0224],\n",
      "          [-0.1587, -0.1146, -0.1619,  0.1048,  0.1345],\n",
      "          [-0.0030, -0.0992, -0.0907,  0.0682, -0.0219],\n",
      "          [-0.1220, -0.0577, -0.0481,  0.0237,  0.0901],\n",
      "          [-0.1323, -0.0236, -0.0101, -0.0704,  0.0185]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3038,  0.2846,  0.4696,  0.5247,  0.4763],\n",
      "          [-0.0576, -0.0504, -0.1374, -0.0340,  0.2288],\n",
      "          [-0.2352, -0.1903, -0.0773, -0.1741, -0.2533],\n",
      "          [-0.2111, -0.0644, -0.2063, -0.1934, -0.1419],\n",
      "          [-0.2071, -0.1117, -0.2319, -0.0552,  0.0532]]],\n",
      "\n",
      "\n",
      "        [[[-0.1094, -0.2514, -0.0664, -0.1756,  0.0358],\n",
      "          [-0.0230, -0.1910, -0.1855, -0.0885,  0.0694],\n",
      "          [-0.0532, -0.1434, -0.1271, -0.2169,  0.2604],\n",
      "          [-0.1446,  0.0193, -0.2203, -0.2138,  0.3671],\n",
      "          [-0.0793, -0.1709, -0.2053,  0.2193,  0.2994]]],\n",
      "\n",
      "\n",
      "        [[[-0.0965,  0.1360,  0.1805, -0.0324, -0.3085],\n",
      "          [-0.2102, -0.0570,  0.0706,  0.1140,  0.0524],\n",
      "          [-0.2221, -0.0763, -0.0706,  0.2834, -0.0221],\n",
      "          [-0.1587, -0.2211,  0.0404,  0.1526,  0.0328],\n",
      "          [-0.1886, -0.0349,  0.0552,  0.1128, -0.0854]]],\n",
      "\n",
      "\n",
      "        [[[-0.3446, -0.0440,  0.0993,  0.1246,  0.1217],\n",
      "          [-0.1588,  0.0230,  0.1284, -0.0758,  0.0579],\n",
      "          [-0.1091,  0.1052,  0.0434,  0.1403, -0.0105],\n",
      "          [-0.0073,  0.1754,  0.0820,  0.0976, -0.2125],\n",
      "          [ 0.0468,  0.1381, -0.0170, -0.1689, -0.2573]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0773, -0.0583, -0.0146, -0.0839,  0.1109],\n",
      "          [ 0.1562, -0.0063,  0.0725, -0.0822,  0.0248],\n",
      "          [-0.0835, -0.1299, -0.0601,  0.0266,  0.0883],\n",
      "          [-0.1273,  0.0892,  0.0105, -0.1590, -0.0585],\n",
      "          [ 0.0609, -0.0766,  0.0286,  0.0157, -0.0847]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0470,  0.0660,  0.1748,  0.1702,  0.2130],\n",
      "          [-0.0785, -0.1123, -0.0213, -0.0374,  0.1722],\n",
      "          [-0.0869, -0.0985, -0.0538, -0.1546, -0.0145],\n",
      "          [-0.1312, -0.1544, -0.0116, -0.1491, -0.1707],\n",
      "          [-0.0204, -0.0795,  0.0323, -0.0034,  0.0321]]],\n",
      "\n",
      "\n",
      "        [[[-0.0089, -0.0363, -0.1178, -0.3036, -0.2399],\n",
      "          [-0.1593, -0.0333, -0.2274, -0.2011, -0.1878],\n",
      "          [-0.0541, -0.2680,  0.0006, -0.0020, -0.0250],\n",
      "          [-0.2317, -0.0842, -0.2265, -0.1141, -0.1575],\n",
      "          [-0.1715, -0.1018, -0.0319, -0.2755, -0.0428]]],\n",
      "\n",
      "\n",
      "        [[[-0.0914, -0.0650, -0.2165, -0.1258, -0.0108],\n",
      "          [-0.1357, -0.1978, -0.2206, -0.1156, -0.0368],\n",
      "          [-0.1639, -0.0837,  0.0025, -0.1072, -0.0906],\n",
      "          [ 0.1192,  0.0061,  0.0101, -0.0412, -0.1012],\n",
      "          [ 0.2705,  0.2466,  0.0921,  0.0663,  0.0384]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0546,  0.0463,  0.0107,  0.0533, -0.1170],\n",
      "          [-0.1276,  0.0864, -0.1279,  0.0473, -0.0604],\n",
      "          [ 0.0344, -0.0667, -0.0346,  0.1075,  0.0952],\n",
      "          [-0.0943,  0.0386, -0.0537, -0.0494, -0.0310],\n",
      "          [ 0.1258, -0.0488, -0.0607,  0.0061,  0.0334]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1134,  0.1715, -0.0495,  0.0936, -0.0672],\n",
      "          [-0.0396, -0.0851, -0.0918, -0.1292, -0.0581],\n",
      "          [-0.0437, -0.0434,  0.0820,  0.0710, -0.0403],\n",
      "          [-0.0820,  0.0023, -0.1142, -0.1151, -0.0048],\n",
      "          [-0.0174, -0.0564,  0.0763, -0.0705, -0.1322]]],\n",
      "\n",
      "\n",
      "        [[[-0.1382, -0.1034, -0.2914, -0.0781,  0.1736],\n",
      "          [-0.2048, -0.1739, -0.1680, -0.0605,  0.3174],\n",
      "          [-0.2346, -0.0687, -0.2097, -0.0737,  0.2699],\n",
      "          [-0.0876, -0.1606, -0.0766,  0.1528,  0.1817],\n",
      "          [-0.1449, -0.0178, -0.0181,  0.1578,  0.3203]]],\n",
      "\n",
      "\n",
      "        [[[-0.0057,  0.0638,  0.0816, -0.0907, -0.1279],\n",
      "          [ 0.1144, -0.0179,  0.1160, -0.1395, -0.0018],\n",
      "          [-0.0890, -0.1127,  0.0362, -0.0977, -0.0993],\n",
      "          [-0.1166,  0.1230,  0.1054, -0.0866, -0.0590],\n",
      "          [-0.0810,  0.0819,  0.1098, -0.1559,  0.0314]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0443,  0.0815,  0.1530,  0.2386,  0.2505],\n",
      "          [-0.0537, -0.0316, -0.1267, -0.0491, -0.0895],\n",
      "          [ 0.0127,  0.0460, -0.0593, -0.0488, -0.1193],\n",
      "          [-0.0504, -0.0854,  0.0129, -0.0964, -0.0323],\n",
      "          [-0.1225, -0.1434, -0.0547, -0.1315,  0.0013]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0110,  0.2144,  0.1477, -0.0806, -0.2498],\n",
      "          [-0.0102,  0.1559, -0.0404,  0.0491, -0.0993],\n",
      "          [ 0.1566,  0.1073,  0.0605, -0.1026, -0.1551],\n",
      "          [-0.1018,  0.0699,  0.1270, -0.0755,  0.0490],\n",
      "          [-0.0310,  0.0298,  0.1258, -0.0900,  0.0644]]]])\n",
      "XXX\n",
      "tensor([[[[-0.3534, -0.0775, -0.2170, -0.0648, -0.2574],\n",
      "          [-0.1285, -0.0360, -0.3809, -0.1319,  0.0790],\n",
      "          [-0.2367, -0.1869, -0.3021, -0.1457, -0.3289],\n",
      "          [-0.2963, -0.2122, -0.0709, -0.2881, -0.3228],\n",
      "          [-0.2129, -0.1912, -0.0545, -0.1265, -0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0398, -0.2739, -0.2488,  0.2464,  0.0336],\n",
      "          [-0.2380, -0.1718, -0.2428,  0.1572,  0.2017],\n",
      "          [-0.0045, -0.1489, -0.1361,  0.1023, -0.0329],\n",
      "          [-0.1831, -0.0865, -0.0721,  0.0356,  0.1351],\n",
      "          [-0.1985, -0.0354, -0.0151, -0.1055,  0.0277]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4558,  0.4269,  0.7044,  0.7871,  0.7144],\n",
      "          [-0.0865, -0.0756, -0.2062, -0.0510,  0.3433],\n",
      "          [-0.3529, -0.2855, -0.1159, -0.2611, -0.3799],\n",
      "          [-0.3167, -0.0966, -0.3094, -0.2901, -0.2128],\n",
      "          [-0.3107, -0.1675, -0.3478, -0.0828,  0.0798]]],\n",
      "\n",
      "\n",
      "        [[[-0.1642, -0.3771, -0.0997, -0.2634,  0.0537],\n",
      "          [-0.0345, -0.2864, -0.2782, -0.1327,  0.1041],\n",
      "          [-0.0799, -0.2151, -0.1907, -0.3254,  0.3906],\n",
      "          [-0.2169,  0.0289, -0.3304, -0.3207,  0.5507],\n",
      "          [-0.1190, -0.2564, -0.3080,  0.3290,  0.4491]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447,  0.2039,  0.2707, -0.0486, -0.4627],\n",
      "          [-0.3153, -0.0855,  0.1059,  0.1711,  0.0786],\n",
      "          [-0.3332, -0.1144, -0.1058,  0.4250, -0.0332],\n",
      "          [-0.2380, -0.3317,  0.0606,  0.2288,  0.0492],\n",
      "          [-0.2828, -0.0523,  0.0828,  0.1692, -0.1281]]],\n",
      "\n",
      "\n",
      "        [[[-0.5169, -0.0660,  0.1490,  0.1869,  0.1826],\n",
      "          [-0.2383,  0.0344,  0.1925, -0.1137,  0.0868],\n",
      "          [-0.1636,  0.1577,  0.0651,  0.2105, -0.0157],\n",
      "          [-0.0109,  0.2631,  0.1229,  0.1464, -0.3188],\n",
      "          [ 0.0702,  0.2072, -0.0254, -0.2533, -0.3859]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1159, -0.0875, -0.0219, -0.1258,  0.1663],\n",
      "          [ 0.2344, -0.0094,  0.1087, -0.1233,  0.0373],\n",
      "          [-0.1252, -0.1948, -0.0902,  0.0399,  0.1324],\n",
      "          [-0.1910,  0.1339,  0.0158, -0.2385, -0.0878],\n",
      "          [ 0.0913, -0.1149,  0.0429,  0.0236, -0.1270]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0704,  0.0991,  0.2622,  0.2554,  0.3195],\n",
      "          [-0.1178, -0.1685, -0.0320, -0.0562,  0.2582],\n",
      "          [-0.1303, -0.1477, -0.0807, -0.2319, -0.0218],\n",
      "          [-0.1969, -0.2316, -0.0174, -0.2237, -0.2560],\n",
      "          [-0.0306, -0.1192,  0.0485, -0.0051,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134, -0.0545, -0.1768, -0.4554, -0.3599],\n",
      "          [-0.2390, -0.0500, -0.3412, -0.3016, -0.2818],\n",
      "          [-0.0811, -0.4019,  0.0009, -0.0029, -0.0376],\n",
      "          [-0.3476, -0.1263, -0.3397, -0.1711, -0.2363],\n",
      "          [-0.2572, -0.1527, -0.0478, -0.4132, -0.0643]]],\n",
      "\n",
      "\n",
      "        [[[-0.1372, -0.0975, -0.3247, -0.1888, -0.0162],\n",
      "          [-0.2035, -0.2968, -0.3310, -0.1734, -0.0552],\n",
      "          [-0.2458, -0.1255,  0.0037, -0.1607, -0.1359],\n",
      "          [ 0.1788,  0.0091,  0.0151, -0.0618, -0.1518],\n",
      "          [ 0.4057,  0.3699,  0.1382,  0.0994,  0.0575]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0819,  0.0695,  0.0160,  0.0799, -0.1755],\n",
      "          [-0.1915,  0.1297, -0.1919,  0.0709, -0.0906],\n",
      "          [ 0.0516, -0.1001, -0.0520,  0.1612,  0.1428],\n",
      "          [-0.1415,  0.0579, -0.0806, -0.0740, -0.0465],\n",
      "          [ 0.1887, -0.0732, -0.0911,  0.0091,  0.0500]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702,  0.2573, -0.0742,  0.1404, -0.1008],\n",
      "          [-0.0595, -0.1277, -0.1377, -0.1938, -0.0871],\n",
      "          [-0.0655, -0.0650,  0.1230,  0.1066, -0.0605],\n",
      "          [-0.1230,  0.0035, -0.1713, -0.1727, -0.0073],\n",
      "          [-0.0261, -0.0847,  0.1144, -0.1058, -0.1982]]],\n",
      "\n",
      "\n",
      "        [[[-0.2073, -0.1550, -0.4371, -0.1172,  0.2604],\n",
      "          [-0.3073, -0.2608, -0.2520, -0.0908,  0.4761],\n",
      "          [-0.3519, -0.1031, -0.3146, -0.1106,  0.4048],\n",
      "          [-0.1314, -0.2409, -0.1149,  0.2292,  0.2726],\n",
      "          [-0.2173, -0.0267, -0.0272,  0.2367,  0.4805]]],\n",
      "\n",
      "\n",
      "        [[[-0.0086,  0.0956,  0.1224, -0.1360, -0.1918],\n",
      "          [ 0.1716, -0.0269,  0.1740, -0.2093, -0.0027],\n",
      "          [-0.1335, -0.1691,  0.0543, -0.1465, -0.1489],\n",
      "          [-0.1750,  0.1845,  0.1581, -0.1298, -0.0885],\n",
      "          [-0.1216,  0.1228,  0.1647, -0.2338,  0.0471]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664,  0.1222,  0.2296,  0.3579,  0.3757],\n",
      "          [-0.0805, -0.0474, -0.1900, -0.0737, -0.1342],\n",
      "          [ 0.0191,  0.0691, -0.0890, -0.0732, -0.1790],\n",
      "          [-0.0756, -0.1280,  0.0194, -0.1446, -0.0484],\n",
      "          [-0.1837, -0.2152, -0.0821, -0.1972,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165,  0.3216,  0.2215, -0.1209, -0.3748],\n",
      "          [-0.0152,  0.2338, -0.0607,  0.0736, -0.1489],\n",
      "          [ 0.2349,  0.1609,  0.0908, -0.1540, -0.2327],\n",
      "          [-0.1526,  0.1049,  0.1905, -0.1133,  0.0734],\n",
      "          [-0.0465,  0.0447,  0.1887, -0.1350,  0.0966]]]])\n",
      "YYY\n",
      "tensor([[[[-0.3534, -0.0775, -0.2170, -0.0648, -0.2574],\n",
      "          [-0.1285, -0.0360, -0.3809, -0.1319,  0.0790],\n",
      "          [-0.2367, -0.1869, -0.3021, -0.1457, -0.3289],\n",
      "          [-0.2963, -0.2122, -0.0709, -0.2881, -0.3228],\n",
      "          [-0.2129, -0.1912, -0.0545, -0.1265, -0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0398, -0.2739, -0.2488,  0.2464,  0.0336],\n",
      "          [-0.2380, -0.1718, -0.2428,  0.1572,  0.2017],\n",
      "          [-0.0045, -0.1489, -0.1361,  0.1023, -0.0329],\n",
      "          [-0.1831, -0.0865, -0.0721,  0.0356,  0.1351],\n",
      "          [-0.1985, -0.0354, -0.0151, -0.1055,  0.0277]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4558,  0.4269,  0.7044,  0.7871,  0.7144],\n",
      "          [-0.0865, -0.0756, -0.2062, -0.0510,  0.3433],\n",
      "          [-0.3529, -0.2855, -0.1159, -0.2611, -0.3799],\n",
      "          [-0.3167, -0.0966, -0.3094, -0.2901, -0.2128],\n",
      "          [-0.3107, -0.1675, -0.3478, -0.0828,  0.0798]]],\n",
      "\n",
      "\n",
      "        [[[-0.1642, -0.3771, -0.0997, -0.2634,  0.0537],\n",
      "          [-0.0345, -0.2864, -0.2782, -0.1327,  0.1041],\n",
      "          [-0.0799, -0.2151, -0.1907, -0.3254,  0.3906],\n",
      "          [-0.2169,  0.0289, -0.3304, -0.3207,  0.5507],\n",
      "          [-0.1190, -0.2564, -0.3080,  0.3290,  0.4491]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447,  0.2039,  0.2707, -0.0486, -0.4627],\n",
      "          [-0.3153, -0.0855,  0.1059,  0.1711,  0.0786],\n",
      "          [-0.3332, -0.1144, -0.1058,  0.4250, -0.0332],\n",
      "          [-0.2380, -0.3317,  0.0606,  0.2288,  0.0492],\n",
      "          [-0.2828, -0.0523,  0.0828,  0.1692, -0.1281]]],\n",
      "\n",
      "\n",
      "        [[[-0.5169, -0.0660,  0.1490,  0.1869,  0.1826],\n",
      "          [-0.2383,  0.0344,  0.1925, -0.1137,  0.0868],\n",
      "          [-0.1636,  0.1577,  0.0651,  0.2105, -0.0157],\n",
      "          [-0.0109,  0.2631,  0.1229,  0.1464, -0.3188],\n",
      "          [ 0.0702,  0.2072, -0.0254, -0.2533, -0.3859]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1159, -0.0875, -0.0219, -0.1258,  0.1663],\n",
      "          [ 0.2344, -0.0094,  0.1087, -0.1233,  0.0373],\n",
      "          [-0.1252, -0.1948, -0.0902,  0.0399,  0.1324],\n",
      "          [-0.1910,  0.1339,  0.0158, -0.2385, -0.0878],\n",
      "          [ 0.0913, -0.1149,  0.0429,  0.0236, -0.1270]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0704,  0.0991,  0.2622,  0.2554,  0.3195],\n",
      "          [-0.1178, -0.1685, -0.0320, -0.0562,  0.2582],\n",
      "          [-0.1303, -0.1477, -0.0807, -0.2319, -0.0218],\n",
      "          [-0.1969, -0.2316, -0.0174, -0.2237, -0.2560],\n",
      "          [-0.0306, -0.1192,  0.0485, -0.0051,  0.0482]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134, -0.0545, -0.1768, -0.4554, -0.3599],\n",
      "          [-0.2390, -0.0500, -0.3412, -0.3016, -0.2818],\n",
      "          [-0.0811, -0.4019,  0.0009, -0.0029, -0.0376],\n",
      "          [-0.3476, -0.1263, -0.3397, -0.1711, -0.2363],\n",
      "          [-0.2572, -0.1527, -0.0478, -0.4132, -0.0643]]],\n",
      "\n",
      "\n",
      "        [[[-0.1372, -0.0975, -0.3247, -0.1888, -0.0162],\n",
      "          [-0.2035, -0.2968, -0.3310, -0.1734, -0.0552],\n",
      "          [-0.2458, -0.1255,  0.0037, -0.1607, -0.1359],\n",
      "          [ 0.1788,  0.0091,  0.0151, -0.0618, -0.1518],\n",
      "          [ 0.4057,  0.3699,  0.1382,  0.0994,  0.0575]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0819,  0.0695,  0.0160,  0.0799, -0.1755],\n",
      "          [-0.1915,  0.1297, -0.1919,  0.0709, -0.0906],\n",
      "          [ 0.0516, -0.1001, -0.0520,  0.1612,  0.1428],\n",
      "          [-0.1415,  0.0579, -0.0806, -0.0740, -0.0465],\n",
      "          [ 0.1887, -0.0732, -0.0911,  0.0091,  0.0500]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702,  0.2573, -0.0742,  0.1404, -0.1008],\n",
      "          [-0.0595, -0.1277, -0.1377, -0.1938, -0.0871],\n",
      "          [-0.0655, -0.0650,  0.1230,  0.1066, -0.0605],\n",
      "          [-0.1230,  0.0035, -0.1713, -0.1727, -0.0073],\n",
      "          [-0.0261, -0.0847,  0.1144, -0.1058, -0.1982]]],\n",
      "\n",
      "\n",
      "        [[[-0.2073, -0.1550, -0.4371, -0.1172,  0.2604],\n",
      "          [-0.3073, -0.2608, -0.2520, -0.0908,  0.4761],\n",
      "          [-0.3519, -0.1031, -0.3146, -0.1106,  0.4048],\n",
      "          [-0.1314, -0.2409, -0.1149,  0.2292,  0.2726],\n",
      "          [-0.2173, -0.0267, -0.0272,  0.2367,  0.4805]]],\n",
      "\n",
      "\n",
      "        [[[-0.0086,  0.0956,  0.1224, -0.1360, -0.1918],\n",
      "          [ 0.1716, -0.0269,  0.1740, -0.2093, -0.0027],\n",
      "          [-0.1335, -0.1691,  0.0543, -0.1465, -0.1489],\n",
      "          [-0.1750,  0.1845,  0.1581, -0.1298, -0.0885],\n",
      "          [-0.1216,  0.1228,  0.1647, -0.2338,  0.0471]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664,  0.1222,  0.2296,  0.3579,  0.3757],\n",
      "          [-0.0805, -0.0474, -0.1900, -0.0737, -0.1342],\n",
      "          [ 0.0191,  0.0691, -0.0890, -0.0732, -0.1790],\n",
      "          [-0.0756, -0.1280,  0.0194, -0.1446, -0.0484],\n",
      "          [-0.1837, -0.2152, -0.0821, -0.1972,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165,  0.3216,  0.2215, -0.1209, -0.3748],\n",
      "          [-0.0152,  0.2338, -0.0607,  0.0736, -0.1489],\n",
      "          [ 0.2349,  0.1609,  0.0908, -0.1540, -0.2327],\n",
      "          [-0.1526,  0.1049,  0.1905, -0.1133,  0.0734],\n",
      "          [-0.0465,  0.0447,  0.1887, -0.1350,  0.0966]]]])\n",
      "!-- Server Model Status --!\n",
      "Epoch [1/20], Culminative Send Cost: 173628.0, Culminative Time Used: 7.325578100979328\n",
      "Train Loss: 0.2456807643175125, Train Accuracy: 0.9230000972747803, Train Error: 0.0769999995827675\n",
      "Test Loss: 0.2103652805089951, Test Accuracy: 0.9399999976158142, Test Error: 0.0599999986588955\n",
      "!-- Client 1 is normal and start iterations. ---!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe current train start time is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m current_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFedAvg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mexperiment_FedAvg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_distributing_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitera_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madversary_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madversary_attack_func_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madversary_attack_value_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_rounds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 1373\u001b[0m, in \u001b[0;36mexperiment_FedAvg_model\u001b[1;34m(train_dataset, test_dataset, dataset_distributing_func, modelClass, optimizerClass, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds, show_history, save_result)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m save_file_extra_information\n\u001b[0;32m   1323\u001b[0m save_file_extra_information \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124mThe experiment ID is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_id\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;124madversary_attack_value = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madversary_attack_value\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m-> 1373\u001b[0m cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_federated_learning_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m cost_history_total\u001b[38;5;241m.\u001b[39mappend(cost_history)\n\u001b[0;32m   1376\u001b[0m time_history_total\u001b[38;5;241m.\u001b[39mappend(time_history)\n",
      "Cell \u001b[1;32mIn[41], line 1082\u001b[0m, in \u001b[0;36mtrain_federated_learning_model\u001b[1;34m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history, save_result, save_path_str)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_federated_learning_model\u001b[39m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func\u001b[38;5;241m=\u001b[39mfederated_averaging, loss_func\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy, accuracy_func\u001b[38;5;241m=\u001b[39mget_accuracy, error_func\u001b[38;5;241m=\u001b[39mget_error, show_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_path_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFederatedLearning\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1082\u001b[0m     cost_history, time_history, train_loss_history, train_accuracy_history, train_error_history, test_loss_history, test_accuracy_history, test_error_history \u001b[38;5;241m=\u001b[39m \u001b[43miterate_federated_learning_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_sample_client_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m# Print learned parameters\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m global_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "Cell \u001b[1;32mIn[41], line 786\u001b[0m, in \u001b[0;36miterate_federated_learning_model\u001b[1;34m(global_model, train_dataloader, test_dataloader, client_list, random_sample_client_number, global_epochs, local_epochs, aggregate_func, loss_func, accuracy_func, error_func, show_history)\u001b[0m\n\u001b[0;32m    784\u001b[0m send_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(value\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m global_weights\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    785\u001b[0m client\u001b[38;5;241m.\u001b[39mload_weights(global_weights)\n\u001b[1;32m--> 786\u001b[0m client_weights \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    787\u001b[0m client_weights_total\u001b[38;5;241m.\u001b[39mappend(client_weights)\n\u001b[0;32m    788\u001b[0m send_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(value\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m client_weights\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[1;32mIn[41], line 544\u001b[0m, in \u001b[0;36mClientDevice.train\u001b[1;34m(self, num_epochs, show_message, plot_history)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_message:\n\u001b[0;32m    543\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!-- Client \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is normal and start iterations. ---!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 544\u001b[0m         loss_history, accuracy_history, error_history, time_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_history:\n\u001b[0;32m    546\u001b[0m     plot_time_history(time_history)\n",
      "Cell \u001b[1;32mIn[41], line 281\u001b[0m, in \u001b[0;36miterate_model_simple\u001b[1;34m(model, dataloader, num_epochs, optimizer, loss_func, accuracy_func, error_func, show_history, test_dataloader, include_intial_history)\u001b[0m\n\u001b[0;32m    279\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_func(outputs, labels)\n\u001b[0;32m    280\u001b[0m error \u001b[38;5;241m=\u001b[39m error_func(outputs, labels)\n\u001b[1;32m--> 281\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    283\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelType = model_type_preset\n",
    "optimizerType = optimizer_type_preset\n",
    "itera_func = iterate_model_simple\n",
    "dataset_distributing_func = split_datasets_for_clients_random\n",
    "\n",
    "custom_split_dataset_iid = False\n",
    "custom_split_dataset_power_law = True\n",
    "custom_split_dataset_balance = False\n",
    "custom_split_dataset_seed = 42\n",
    "\n",
    "global_epochs_list = 20\n",
    "local_epochs_list = 2\n",
    "num_clients_list = [3]\n",
    "random_sample_client_number_list = [3]\n",
    "learning_rate_list = 0.3\n",
    "batch_size_list = 10\n",
    "aggregate_func_list = federated_averaging\n",
    "loss_func_list = loss_func_preset\n",
    "accuracy_func_list = get_accuracy\n",
    "error_func_list = get_error\n",
    "adversary_list = [0]\n",
    "adversary_attack_func_list = [adversarial_attack_by_train_scaling]\n",
    "adversary_attack_value_list = [-0.01]\n",
    "\n",
    "experiment_rounds = 1\n",
    "\n",
    "print(f'The current dataset is {current_dataset_name}.')\n",
    "train_start_time = datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")\n",
    "print(f'The current train start time is {train_start_time}.')\n",
    "\n",
    "current_method_name = \"FedAvg\"\n",
    "experiment_FedAvg_model(train_dataset, test_dataset, dataset_distributing_func, modelType, optimizerType, itera_func, global_epochs_list, local_epochs_list, num_clients_list, random_sample_client_number_list, learning_rate_list, batch_size_list, aggregate_func_list, loss_func_list, accuracy_func_list, error_func_list, adversary_list, adversary_attack_func_list, adversary_attack_value_list, experiment_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.0 Loading Data ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clear and Initialize Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cost_history_total = []\n",
    "data_time_history_total = []\n",
    "data_train_loss_history_total = []\n",
    "data_train_accuracy_history_total = []\n",
    "data_train_error_history_total = []\n",
    "data_test_loss_history_total = []\n",
    "data_test_accuracy_history_total = []\n",
    "data_test_error_history_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data Manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the filename_load path manually here!!\n",
    "filename_load_list = [\"MNIST_FedProx_GE_100_LE_10_C_1000_RC_200_lr_0.03_B_128_2024-03-11 20.28.13_1.npy\",\n",
    "                      \"MNIST_FedProx_GE_100_LE_10_C_1000_RC_200_lr_0.03_B_128_2024-03-11 20.28.13_2.npy\",\n",
    "                      \"MNIST_FedProx_GE_100_LE_10_C_1000_RC_200_lr_0.03_B_128_2024-03-11 20.28.13_3.npy\",]\n",
    "data_append_load = True\n",
    "\n",
    "for filename_load in filename_load_list:\n",
    "    print(\"============================================\")\n",
    "    print(\"*** Loading file...                     ***\")\n",
    "    print(\"============================================\")\n",
    "    try:\n",
    "        # Load the file\n",
    "        load_result = np.load(filename_load)\n",
    "        print('Result has been loaded from the file: ', filename_load)\n",
    "\n",
    "        # Load the attributes from the file\n",
    "        data_cost_history = load_result['cost_history']\n",
    "        data_time_history = load_result['time_history']\n",
    "        data_train_loss_history = load_result['train_loss_history']\n",
    "        data_train_accuracy_history = load_result['train_accuracy_history']\n",
    "        data_train_error_history = load_result['train_error_history']\n",
    "        data_test_loss_history = load_result['test_loss_history']\n",
    "        data_test_accuracy_history = load_result['test_accuracy_history']\n",
    "        data_test_error_history = load_result['test_error_history']\n",
    "\n",
    "        print(\"=======Content of the File=======\")\n",
    "        print(load_result.files)\n",
    "\n",
    "        print(\"=======VISUALIZATION RESULT=======\")\n",
    "        #plot_cost_history([data_cost_history], save=False)\n",
    "        #plot_time_history([data_time_history], save=False)\n",
    "        plot_loss_history([data_train_loss_history], [data_test_loss_history], save=False)\n",
    "        plot_accuracy_history([data_train_accuracy_history], [data_test_accuracy_history], save=False)\n",
    "        plot_error_history([data_train_error_history], [data_test_error_history], save=False)\n",
    "\n",
    "        print(\"=======STATUS RESULT=======\")\n",
    "        print(\"Cost History: \", data_cost_history)\n",
    "        print(\"Time History: \", data_time_history)\n",
    "\n",
    "        print(\"=======TRAIN RESULT=======\")\n",
    "        print(\"Train Loss History: \", data_train_loss_history)\n",
    "        print(\"Train Accuracy History: \", data_train_accuracy_history)\n",
    "        print(\"Train Error History: \", data_train_error_history)\n",
    "\n",
    "        print(\"=======TEST RESULT=======\")\n",
    "        print(\"Test Loss History: \", data_test_loss_history)\n",
    "        print(\"Test Accuracy History: \", data_test_accuracy_history)\n",
    "        print(\"Test Error History: \", data_test_error_history)\n",
    "\n",
    "        # Append the data\n",
    "        if data_append_load:\n",
    "            data_cost_history_total.append(data_cost_history)\n",
    "            data_time_history_total.append(data_time_history)\n",
    "            data_train_loss_history_total.append(data_train_loss_history)\n",
    "            data_train_accuracy_history_total.append(data_train_accuracy_history)\n",
    "            data_train_error_history_total.append(data_train_error_history)\n",
    "            data_test_loss_history_total.append(data_test_loss_history)\n",
    "            data_test_accuracy_history_total.append(data_test_accuracy_history)\n",
    "            data_test_error_history_total.append(data_test_error_history)\n",
    "    except (FileNotFoundError, IOError):\n",
    "        print(\"Failed to load the file: \", filename_load)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.1 Data Visualization ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize All Data Directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_history(data_cost_history_total, save=False)\n",
    "plot_time_history(data_time_history_total, save=False)\n",
    "plot_loss_history(data_train_loss_history_total, data_test_loss_history_total, save=False)\n",
    "plot_accuracy_history(data_train_accuracy_history_total, data_test_accuracy_history_total, save=False)\n",
    "plot_error_history(data_train_error_history_total, data_test_error_history_total, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Graph in Centralized Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_list = [1, 2, 3, 4, 5, 10]\n",
    "plot_save_fig_bool = False\n",
    "plot_show_train_bool = True\n",
    "plot_show_test_bool = False\n",
    "plot_log_scale = False\n",
    "\n",
    "plot_different_parameter_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_parameter_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_loss_history in enumerate(plot_different_parameter_train_loss_history):\n",
    "        plt.plot(plot_train_loss_history, label=f\"Train Loss History\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_loss_history in enumerate(plot_different_parameter_test_loss_history):\n",
    "        plt.plot(plot_test_loss_history, label=f\"Test Loss History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History in Centralized Training\")\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_loss_history_centralized_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_parameter_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_accuracy_history in enumerate(plot_different_parameter_train_accuracy_history):\n",
    "        plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_accuracy_history in enumerate(plot_different_parameter_test_accuracy_history):\n",
    "        plt.plot(plot_test_accuracy_history, label=f\"Test Accuracy History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Accuracy\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History in Centralized Training\")\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_accuracy_history_centralized_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_parameter_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_error_history in enumerate(plot_different_parameter_train_error_history):\n",
    "        plt.plot(plot_train_error_history, label=f\"Train Error History\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_error_history in enumerate(plot_different_parameter_test_error_history):\n",
    "        plt.plot(plot_test_error_history, label=f\"Test Error History\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Error\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History in Centralized Training\")\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_error_history_centralized_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Graph between different parameters in Federated Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameters_list = [0.00, 0.50, 1.00]\n",
    "plot_title_strings = \"different mu\"\n",
    "plot_legend_strings = \"mu\"\n",
    "plot_save_fig_bool = False\n",
    "plot_show_train_bool = True\n",
    "plot_show_test_bool = False\n",
    "plot_log_scale = False\n",
    "\n",
    "plot_different_parameter_train_loss_history = convert_to_list(data_train_loss_history_total)\n",
    "plot_different_parameter_test_loss_history = convert_to_list(data_test_loss_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_loss_history in enumerate(plot_different_parameter_train_loss_history):\n",
    "        plt.plot(plot_train_loss_history, label=f\"Train Loss History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_loss_history in enumerate(plot_different_parameter_test_loss_history):\n",
    "        plt.plot(plot_test_loss_history, label=f\"Test Loss History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History with \" + plot_title_strings)\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_loss_history_{plot_title_strings}_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_accuracy_history = convert_to_list(data_train_accuracy_history_total)\n",
    "plot_different_parameter_test_accuracy_history = convert_to_list(data_test_accuracy_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_accuracy_history in enumerate(plot_different_parameter_train_accuracy_history):\n",
    "        plt.plot(plot_train_accuracy_history, label=f\"Train Accuracy History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_accuracy_history in enumerate(plot_different_parameter_test_accuracy_history):\n",
    "        plt.plot(plot_test_accuracy_history, label=f\"Test Aacuracy History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Accuracy\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy History with \" + plot_title_strings)\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_accuracy_history_{plot_title_strings}_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_train_error_history = convert_to_list(data_train_error_history_total)\n",
    "plot_different_parameter_test_error_history = convert_to_list(data_test_error_history_total)\n",
    "if plot_show_train_bool is True:\n",
    "    for i, plot_train_error_history in enumerate(plot_different_parameter_train_error_history):\n",
    "        plt.plot(plot_train_error_history, label=f\"Train Error History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "if plot_show_test_bool is True:\n",
    "    for i, plot_test_error_history in enumerate(plot_different_parameter_test_error_history):\n",
    "        plt.plot(plot_test_error_history, label=f\"Test Error History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Error\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Error\")\n",
    "plt.title(\"Error History with \" + plot_title_strings)\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_error_history_{plot_title_strings}_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_time_history = convert_to_list(data_time_history_total)\n",
    "for i, plot_time_history in enumerate(plot_different_parameter_time_history):\n",
    "    plt.plot(plot_time_history, label=f\"Time History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Culminative Time Used\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Culminative Time Used\")\n",
    "plt.title(\"Time History with \" + plot_title_strings)\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_time_history_{plot_title_strings}_{train_start_time}.png')\n",
    "plt.show()\n",
    "\n",
    "plot_different_parameter_cost_history = convert_to_list(data_cost_history_total)\n",
    "for i, plot_cost_history in enumerate(plot_different_parameter_cost_history):\n",
    "    plt.plot(plot_cost_history, label=f\"Time History with {plot_legend_strings} = {plot_parameters_list[i]}\")\n",
    "plt.xlabel(\"Communication Rounds\")\n",
    "if plot_log_scale is True:\n",
    "    plt.ylabel(\"Log Cost\")\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.ylabel(\"Cost\")\n",
    "plt.title(\"Culminative Send Cost History with \" + plot_title_strings)\n",
    "plt.legend()\n",
    "if plot_save_fig_bool is True:\n",
    "    plt.savefig(f'Analysis_{current_dataset_name}_cost_history_{plot_title_strings}_{train_start_time}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment between different Federated Learning Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Graph Averaging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3.2 Analysing ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Analysis**\n",
    "\n",
    "We analysis the variance of a particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance = statistics.variance(data_cost_history)\n",
    "time_variance = statistics.variance(data_time_history)\n",
    "train_loss_variance = statistics.variance(data_train_loss_history)\n",
    "train_accuracy_variance = statistics.variance(data_train_accuracy_history)\n",
    "train_error_variance = statistics.variance(data_train_error_history)\n",
    "test_loss_variance = statistics.variance(data_test_loss_history)\n",
    "test_accuracy_variance = statistics.variance(data_test_accuracy_history)\n",
    "test_error_variance = statistics.variance(data_test_error_history)\n",
    "print(\"=======VARIANCE RESULT=======\")\n",
    "print(\"Cost Variance: \", cost_variance)\n",
    "print(\"Time Variance: \", time_variance)\n",
    "print(\"Train Loss Variance: \", train_loss_variance)\n",
    "print(\"Train Accuracy Variance: \", train_accuracy_variance)\n",
    "print(\"Train Error Variance: \", train_error_variance)\n",
    "print(\"Test Loss Variance: \", test_loss_variance)\n",
    "print(\"Test Accuracy Variance: \", test_accuracy_variance)\n",
    "print(\"Test Error Variance: \", test_error_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_variance_subset_min = 0\n",
    "analysis_variance_subset_max = len(data_cost_history) // 2\n",
    "\n",
    "cost_variance_subset = statistics.variance(data_cost_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "time_variance_subset = statistics.variance(data_time_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[analysis_variance_subset_min:analysis_variance_subset_max])\n",
    "print(f'=======VARIANCE RESULT IN SUBSET BETWEEN {analysis_variance_subset_min} and {analysis_variance_subset_max}=======')\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[:len(data_cost_history) // 2])\n",
    "time_variance_subset = statistics.variance(data_time_history[:len(data_time_history) // 2])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[:len(data_train_loss_history) // 2])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[:len(data_train_accuracy_history) // 2])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[:len(data_train_error_history) // 2])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[:len(data_test_loss_history) // 2])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[:len(data_test_accuracy_history) // 2])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[:len(data_test_error_history) // 2])\n",
    "print(\"=======VARIANCE FIRST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_variance_subset = statistics.variance(data_cost_history[len(data_cost_history) // 2:])\n",
    "time_variance_subset = statistics.variance(data_time_history[len(data_time_history) // 2:])\n",
    "train_loss_variance_subset = statistics.variance(data_train_loss_history[len(data_train_loss_history) // 2:])\n",
    "train_accuracy_variance_subset = statistics.variance(data_train_accuracy_history[len(data_train_accuracy_history) // 2:])\n",
    "train_error_variance_subset = statistics.variance(data_train_error_history[len(data_train_error_history) // 2:])\n",
    "test_loss_variance_subset = statistics.variance(data_test_loss_history[len(data_test_loss_history) // 2:])\n",
    "test_accuracy_variance_subset = statistics.variance(data_test_accuracy_history[len(data_test_accuracy_history) // 2:])\n",
    "test_error_variance_subset = statistics.variance(data_test_error_history[len(data_test_error_history) // 2:])\n",
    "print(\"=======VARIANCE LAST SUBSET RESULT=======\")\n",
    "print(\"Cost Variance in Subset: \", cost_variance_subset)\n",
    "print(\"Time Variance in Subset: \", time_variance_subset)\n",
    "print(\"Train Loss Variance in Subset: \", train_loss_variance_subset)\n",
    "print(\"Train Accuracy Variance in Subset: \", train_accuracy_variance_subset)\n",
    "print(\"Train Error Variance in Subset: \", train_error_variance_subset)\n",
    "print(\"Test Loss Variance in Subset: \", test_loss_variance_subset)\n",
    "print(\"Test Accuracy Variance in Subset: \", test_accuracy_variance_subset)\n",
    "print(\"Test Error Variance in Subset: \", test_error_variance_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Image Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"Test.png\"\n",
    "test_model_type = MNIST_CNN_Model\n",
    "test_model_path = \"MNIST_FedAvg_with_global_epochs_100_local_epochs_3_num_clients_1_batch_size_128_2024-03-01 16.14.36_model_state_dict.pth\"\n",
    "test_image_resize = 28\n",
    "test_image_togrey = True\n",
    "\n",
    "test_global_model = test_model_type()\n",
    "test_global_model.load_state_dict(torch.load(test_model_path, map_location=device))\n",
    "\n",
    "if test_image_togrey is True:\n",
    "    test_image = 1.0 - cv2.cvtColor(cv2.resize(load_image(test_image_path), (test_image_resize, test_image_resize)), cv2.COLOR_RGB2GRAY)\n",
    "else:\n",
    "    test_image = cv2.resize(load_image(test_image_path), (test_image_resize, test_image_resize))\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "test_image_torch = torch.Tensor(test_image[np.newaxis, np.newaxis])\n",
    "test_predicted_label_array = test_global_model(test_image_torch).detach().numpy()[0, ...]\n",
    "print('The chances for predicted labels are: ', test_predicted_label_array)\n",
    "\n",
    "test_predicted_label = np.argmax(test_predicted_label_array)\n",
    "\n",
    "print('The predicted label is: ', test_predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
